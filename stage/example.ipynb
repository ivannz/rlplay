{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "830df33b",
   "metadata": {},
   "source": [
    "# Patchy overview of `rlplay` with REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718fe01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f6bf1",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb18c007",
   "metadata": {},
   "source": [
    "## Rollout collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc6e6c8",
   "metadata": {},
   "source": [
    "Rollout collection is designed to be as much `plug-n-play` as possible, i.e. it\n",
    "supports **arbitrarily structured nested containers** of arrays or tensors for\n",
    "environment observations and actions. The actor, however, should **expose**\n",
    "certain API (described below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec2a863",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from rlplay.engine import core\n",
    "\n",
    "# help(core.collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f7cdf2",
   "metadata": {},
   "source": [
    "It's role is to serve as a *middle-man* between the **actor-environment** pair\n",
    "and the **training loop**: to track the trajectory of the actor in the environment,\n",
    "and properly record it into the data buffer.\n",
    "\n",
    "For example, it is not responsible for seeding or randomization of environments\n",
    "(i'm looking at you, `AtariEnv`), and datatype casting (except for rewards,\n",
    "which are cast to `fp32` automatically). In theory, there is **no need** for\n",
    "special data preporcessing, except for, perhaps, casting data to proper dtypes,\n",
    "like from `numpy.float64` observations to `float32` in `CartPole`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e813f733",
   "metadata": {},
   "source": [
    "#### Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb5c99",
   "metadata": {},
   "source": [
    "The collector just carefully records the trajectory by alternating between\n",
    "the **REACT** and **STEP+EMIT** phases in the following fashion:\n",
    "\n",
    "$$\n",
    "    \\cdots\n",
    "        \\longrightarrow t\n",
    "        \\overset{\\mathrm{REACT}}{\\longrightarrow} t + \\tfrac12\n",
    "        \\overset{\\mathrm{STEP+EMIT}}{\\longrightarrow} t + 1\n",
    "        \\longrightarrow \\cdots\n",
    "    \\,, $$\n",
    "\n",
    "where the half-times $t + \\tfrac12$ are commonly reffered to as the `afterstates`:\n",
    "the actor has chosen an action in response to the current observation, yet has\n",
    "not interacted with the environment.\n",
    "\n",
    "So the `time` advances in halves, and the proper names for the half times\n",
    "in the diagram above are the `state`, the `afterstate` and the `next state`,\n",
    "respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fdf5ec",
   "metadata": {},
   "source": [
    "The collected `fragment` data has the following structure:\n",
    "* `.state` $z_t$ **the current \"extended\" observation**\n",
    "  * `.stepno` $n_t$ the step counter\n",
    "  * `.obs` $x_t$ **the current observation** emitted by transitioning to $s_t$\n",
    "  * `.act` $a_{t-1}$ **the last action** which caused $s_{t-1} \\longrightarrow s_t$ in the env\n",
    "  * `.rew` $r_t$ **the previous reward** received by getting to $s_t$\n",
    "  * `.fin` $d_t$ **the termination flag** indicating if $s_t$ is terminal in the env\n",
    "\n",
    "* `.actor` $A_t$ auxiliary data from the actor due to **REACT**\n",
    "\n",
    "* `.env` $E_{t+1}$ auxiliary data from the environment due to **STEP+EMIT**\n",
    "\n",
    "* `.hx` $h_0$ the starting recurrent state of the actor\n",
    "\n",
    "Here $s_t$ denotes **the unobserved true full state** of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482a59a",
   "metadata": {},
   "source": [
    "The actor $\\theta$ interacts with the environment and generates the following\n",
    "<span style=\"color:orange\">**tracked**</span> data during the rollout,\n",
    "unobserved/non-tracked data <span style=\"color:red\">**in red**</span>\n",
    "and $t = 0..T-1$:\n",
    "\n",
    "*  ${\\color{orange}{h_0}}$, the starting recurrent state, is recorded in $\\,.\\!\\mathtt{hx}$\n",
    "\n",
    "* **REACT**: the actor performs the following update ($t \\to t + \\frac12$)\n",
    "\n",
    "$$ \n",
    "    \\bigl(\n",
    "        \\underbrace{\n",
    "            .\\!\\mathtt{state}[\\mathtt{t}]\n",
    "        }_{{\\color{orange}{z_t}}},\\,\n",
    "        {\\color{red}{h_t}}\n",
    "    \\bigr)\n",
    "        \\overset{\\text{Actor}_{\\theta_{\\text{old}}}}{\\longrightarrow}\n",
    "        \\bigl(\n",
    "            \\underbrace{\n",
    "                .\\!\\mathtt{state}.\\!\\mathtt{act}[\\mathtt{t+1}]\n",
    "            }_{a_t \\leadsto {\\color{orange}{z_{t+1}}}},\\,\n",
    "            \\underbrace{\n",
    "                .\\!\\mathtt{actor}[\\mathtt{t}]\n",
    "            }_{{\\color{orange}{A_t}}},\\,\n",
    "            {\\color{red}{h_{t+1}}}\n",
    "        \\bigr)\n",
    "    \\,, $$\n",
    "\n",
    "* **STEP+EMIT**: the environment updates it's unobserved state and emits\n",
    "the observed data ($t + \\frac12 \\to t+1_-$)\n",
    "\n",
    "$$ \n",
    "    \\bigl(\n",
    "        {\\color{red}{s_t}},\\,\n",
    "        \\underbrace{\n",
    "            .\\!\\mathtt{state}.\\!\\mathtt{act}[\\mathtt{t+1}]\n",
    "        }_{a_t \\leadsto {\\color{orange}{z_{t+1}}}}\n",
    "    \\bigr)\n",
    "        \\overset{\\text{Env}}{\\longrightarrow}\n",
    "        \\bigl(\n",
    "            {\\color{red}{s_{t+1}}},\\,\n",
    "            \\underbrace{\n",
    "                .\\!\\mathtt{state}.\\!\\mathtt{obs}[\\mathtt{t+1}]\n",
    "            }_{x_{t+1} \\leadsto {\\color{orange}{z_{t+1}}}},\\,\n",
    "            \\underbrace{\n",
    "                .\\!\\mathtt{state}.\\!\\mathtt{rew}[\\mathtt{t+1}]\n",
    "            }_{r_{t+1} \\leadsto {\\color{orange}{z_{t+1}}}},\\,\n",
    "            \\underbrace{\n",
    "                .\\!\\mathtt{state}.\\!\\mathtt{fin}[\\mathtt{t+1}]\n",
    "            }_{d_{t+1} \\leadsto {\\color{orange}{z_{t+1}}}},\\,\n",
    "            \\underbrace{\n",
    "                .\\!\\mathtt{env}[\\mathtt{t}]\n",
    "            }_{{\\color{orange}{E_{t+1}}}}\n",
    "        \\bigr)\n",
    "    \\,, $$\n",
    "\n",
    "* collect loop ($t + 1_- \\to t+1$)\n",
    "\n",
    "$$ \n",
    "    \\bigl(\n",
    "        {\\color{orange}{n_t}},\\,\n",
    "        {\\color{orange}{d_{t+1}}}\n",
    "    \\bigr)\n",
    "        \\longrightarrow\n",
    "            \\underbrace{\n",
    "                .\\!\\mathtt{state}.\\!\\mathtt{stepno}[\\mathtt{t+1}]\n",
    "            }_{n_{t+1} \\leadsto {\\color{orange}{z_{t+1}}}}\n",
    "    \\,. $$\n",
    "\n",
    "Here $r_t$ is a scalar reward, $d_t = \\top$ if $s_t$ is terminal, or $\\bot$\n",
    "otherwise, $n_{t+1} = 0$ if $d_t = \\top$, else $1 + n_t$, and $a \\leadsto b$\n",
    "means $a$ being recored into $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab719f2",
   "metadata": {},
   "source": [
    "In general, we may treat $z_t$, the extended observation, as an ordinary\n",
    "observation, by **suitably modifying** the environment: we can make it\n",
    "recall the most recent action $a_{t-1}$ and compute the termination indicator\n",
    "$d_t$ of the current state, and let it keep track of the interaction counter\n",
    "$n_t$, and, finally, we can configure it to supply the most recent reward\n",
    "$r_t$ as part of the emitted observation.\n",
    "\n",
    "Hence we essentially consider the following POMDP setup:\n",
    "\\begin{align}\n",
    "    a_t, h_{t+1}, A_t\n",
    "        &\\longleftarrow \\operatorname{Actor}(z_t, h_t; \\theta)\n",
    "        \\,, \\\\\n",
    "    z_{t+1}, r_{t+1}, E_{t+1}, s_{t+1}\n",
    "        &\\longleftarrow \\operatorname{Env}(s_t, a_t)\n",
    "        \\,, \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47754ebb",
   "metadata": {},
   "source": [
    "Specifically, let $\n",
    "(z_t)_{t=0}^T\n",
    "    = (n_t, x_t, a_{t-1}, r_t, d_t)_{t=0}^T\n",
    "$ be the trajectory fragment in `.state`, and $h_0$, `.hx`, be the starting\n",
    "(not necessarily the initial) recurrent state of the actor at the begining\n",
    "of the rollout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba5f78",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "\n",
    "* all nested containers **must be** built from pure python `dicts`, `lists`, `tuples` or `namedtuples`\n",
    "\n",
    "* the environment communicates either in **numpy arrays** or in python **scalars**, but not in data types that are incompatible with pytorch (such as `str` or `bytes`)\n",
    "\n",
    "```python\n",
    "# example\n",
    "obs = {\n",
    "    'camera': {\n",
    "        'rear': numpy.zeros(3, 320, 240),\n",
    "        'front': numpy.zeros(3, 320, 240),\n",
    "    },\n",
    "    'proximity': (+0.1, +0.2, -0.1, +0.0,),\n",
    "    'other': {\n",
    "        'fuel_tank': 78.5,\n",
    "        'passenger': False,\n",
    "    },\n",
    "}\n",
    "```\n",
    "\n",
    "* the actor communicates in torch tensors **only**\n",
    "\n",
    "* the environment produces **float scalar** rewards (other data may be communicated through auxiliary environment info-dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ae43a9",
   "metadata": {},
   "source": [
    "### Container support with `.plyr`\n",
    "\n",
    "One of the core tools used in `rlplay` is a high performing procedure that traverses\n",
    "containers of `list`, `dict` and `tuple` and calls the specified function with the \n",
    "non-container objects found the containers as arguments (like `map`, but not an iterator\n",
    "for arbitrarily and applicable to structured objects).\n",
    "\n",
    "See `README.md` and `rlplay.engine.utils.plyr.apply` for docs.\n",
    "\n",
    "The `apply` procedire has slightly faster specialized version `suply` and `tuply`,\n",
    "which do not waste time on validating the structure of the contianers. They differ\n",
    "in the manner in which they call the specified function: the first passes positional\n",
    "arguments, while the second passes all arguments in one tuple (think of `map` and\n",
    "`starmap` from `functools`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appliers of functions to nested objects\n",
    "from rlplay.engine.utils.plyr import apply, suply, tuply\n",
    "\n",
    "# `setitem` function with argument order, specialized for `apply`\n",
    "from rlplay.engine.utils.plyr import xgetitem, xsetitem\n",
    "\n",
    "# help(apply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e86ba0",
   "metadata": {},
   "source": [
    "How to use `suply` to reset the recurrent state `hx` returned by `torch.nn.LSTM`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dfcb14",
   "metadata": {},
   "source": [
    "```python\n",
    "# the mask of inputs just after env resets\n",
    "fin = torch.randint(2, size=(10, 4), dtype=bool)\n",
    "\n",
    "# the tensors in `hx` must have the same 2nd dim as `fin`\n",
    "hx = torch.randn(2, 1, 4, 32, requires_grad=False).unbind()\n",
    "h0 = torch.zeros(2, 1, 4, 32, requires_grad=True).unbind()\n",
    "# XXX h0 and hx are tuples of tensors (but we're just as good with dicts)\n",
    "\n",
    "# get the masks at step 2, and make it broadcastable with 3d hx\n",
    "m = ~fin[2].unsqueeze(-1)  # reset tensors at fin==False\n",
    "\n",
    "# multiply by zero the current `hx` (diff-able reset and grad stop)\n",
    "suply(\n",
    "    m.mul,  # `.mul` method of the mask upcasts from bool to float if necessary\n",
    "    hx,     # arg `other` of `.mul`\n",
    ")\n",
    "\n",
    "# replace the reset batch elments by a diff-able init value\n",
    "suply(\n",
    "    torch.add,         # .add(input, other, *, alpha=1.)\n",
    "    suply(m.mul, hx),  # arg `input` of `.add`\n",
    "    suply(r.mul, h0),  # arg `other` of `.add`\n",
    "    # alpha=1.         # pass other `alpha` if we want\n",
    ")\n",
    "\n",
    "# XXX `torch.where` does not have an `easily` callable interface\n",
    "suply(\n",
    "    lambda a, b: torch.where(m, a, b),  # or `a.where(m, b)`\n",
    "    hx, h0,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45925d63",
   "metadata": {},
   "source": [
    "For example, this is used to manually run the recurrent network loop:\n",
    "```python\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "\n",
    "if use_cudnn and sticky:\n",
    "    # sequence padding (MUST have sampling with `sticky=True`)\n",
    "    n_steps, n_env, *_ = fin.shape\n",
    "    if n_steps > 1:\n",
    "        # we assume sticky=True\n",
    "        lengths = 1 + (~fin[1:]).sum(0).cpu()  # first observation's fin should be ignored\n",
    "        inputs = pack_padded_sequence(input, lengths, enforce_sorted=False)\n",
    "\n",
    "    output, hx = self.core(inputs, hx)\n",
    "    if n_steps > 1:\n",
    "        output, lens = pad_packed_sequence(\n",
    "            output, batch_first=False, total_length=n_steps)\n",
    "\n",
    "else:\n",
    "    # input is T x B x F, hx is either None, or a proper recurrent state\n",
    "    outputs = []\n",
    "    for x, m in zip(input.unsqueeze(1), ~fin.unsqueeze(-1)):\n",
    "        # `m` indicates if no reset took place, otherwise\n",
    "        #  multiply by zero to stop the grads\n",
    "        if hx is not None:\n",
    "            hx = suply(m.mul, hx)\n",
    "\n",
    "        output, hx = self.core(x, hx)\n",
    "        outputs.append(output)\n",
    "\n",
    "    output = torch.cat(outputs, dim=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a785ef1d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a6c004",
   "metadata": {},
   "source": [
    "### Creating the actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b2bc86",
   "metadata": {},
   "source": [
    "Rollout collection relies on the following API of the actor:\n",
    "* `.reset(j, hx)` reset the recurrent state of the j-th environment in the batch (if applicable)\n",
    "  * `hx` contains tensors with shape `(n_lstm_layers * n_dir) x batch x hidden`, or is an empty tuple\n",
    "  * the returned `hx` is the updated recurrent state\n",
    "\n",
    "\n",
    "* `.step(stepno, obs, act, rew, fin, /, *, hx, virtual)` get the next action $a_t$, the recurrent state $h_{t+1}$, and\n",
    "the **extra info** in response to $n_t$, $x_t$, $a_{t-1}$, $r_t$, $d_t$, and $h_t$ respectively.\n",
    "  * extra info `dict` **might** include `value` key with a `T x B` tensor of state value estimates $\n",
    "      v_t(z_t) \\approx G_t = \\mathbb{E} \\sum_{j\\geq t} \\gamma^{j-t} r_{j+1}\n",
    "  $.\n",
    "  * MUST allocate new `hx` if the recurrent state is updated\n",
    "  * MUST NOT change the inputs in-place\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace79079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine import BaseActorModule\n",
    "\n",
    "help(BaseActorModule.reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f0ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(BaseActorModule.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dadef8",
   "metadata": {},
   "source": [
    "`BaseActorModule` is essentially a thin sub-class of `torch.nn.Module`, that implements\n",
    "the API through `.forward(obs, act, rew, fin, *, hx, stepno)`, which should return three things:\n",
    "\n",
    "1. `actions` prescribed actions in the environment, with data of shape `n_steps x batch x ...`\n",
    "  * can be a nested container of dicts, lists, and tuples\n",
    "\n",
    "\n",
    "2. `hx` data with shape `n_steps x batch x ...`\n",
    "  * can be a nested container of dicts, lists, and tuples\n",
    "  * **if an actor is not recurrent**, then must return an empty container, e.g. a tuple `()`\n",
    "\n",
    "\n",
    "3. `info` object, which might be a tensor or a nested object containing data in tensors\n",
    "`n_steps x batch x ...`. For example, one may communicate the following data:\n",
    "  * `value` -- the state value estimates $v(z_t)$\n",
    "  * `logits` -- the policy logits $\\log \\pi(\\cdot \\mid z_t)$\n",
    "  * `q` -- $Q(z_t, \\cdot)$ values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07459bd5",
   "metadata": {},
   "source": [
    "Here is an example actor, that wraps a simple MLP policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502647a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.utils.common import multinomial\n",
    "\n",
    "class PolicyWrapper(BaseActorModule):\n",
    "    \"\"\"A non-recurrent policy for a flat `Discrete(n)` action space.\"\"\"\n",
    "\n",
    "    def __init__(self, policy):\n",
    "        super().__init__()\n",
    "        self.policy = policy\n",
    "\n",
    "        # for updating the exploration epsilon in the clones\n",
    "        # self.register_buffer('epsilon', torch.tensor(epsilon))\n",
    "\n",
    "    def forward(self, obs, act=None, rew=None, fin=None,\n",
    "                *, hx=None, stepno=None, virtual=False):\n",
    "        # Everything is  [T x B x ...]\n",
    "        logits = self.policy(locals())\n",
    "\n",
    "        actions = multinomial(logits.detach().exp())\n",
    "\n",
    "        return actions, (), dict(logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988603bb",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4bd857",
   "metadata": {},
   "source": [
    "### Manual rollout collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043495dd",
   "metadata": {},
   "source": [
    "We shall need the following procedures from the core of the engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f6fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.core import prepare, startup, collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f39deda",
   "metadata": {},
   "source": [
    "Manual collection requires an `actor` and a batch of environment instances `envs`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b679a43a",
   "metadata": {},
   "source": [
    "Prepare the run-time context for the specified `actor` and the environments\n",
    "```python\n",
    "# settings\n",
    "sticky = False  # whether to stop interacting if an env resets mid-fragment\n",
    "device = None   # specifies the device to put the actor's inputs and data onto\n",
    "pinned = False  # whether to keep the running context in non-resizable pinned\n",
    "                #  (non-paged) memory for faster host-device transfers\n",
    "\n",
    "# initialize a buffer for one rollout fragment\n",
    "buffer = prepare(envs[0], actor, n_steps, len(envs),\n",
    "                 pinned=False, device=device)\n",
    "\n",
    "# the running context tor the actor and the envs (optionally pinned)\n",
    "ctx, fragment = startup(envs, actor, buffer, pinned=pinned)\n",
    "\n",
    "while not done:\n",
    "    # collect the fragment\n",
    "    collect(envs, actor, fragment, ctx, sticky=sticky, device=device)\n",
    "\n",
    "    # fragment.pyt -- torch tensors, fragment.npy -- numpy arrays (aliased on-host)\n",
    "    do_stuff(actor, fragment.pyt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c6c46",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc2cf1",
   "metadata": {},
   "source": [
    "### Rollout collection (same-process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a7d862",
   "metadata": {},
   "source": [
    "Collect rollouts within the current process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf70397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout import same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25301273",
   "metadata": {},
   "source": [
    "The parameters have the following meaning\n",
    "```python\n",
    "it = same.rollout(\n",
    "    envs,           # the batch of environment instances\n",
    "    actor,          # the actor which interacts with the batch\n",
    "    n_steps=51,     # the length of the rollout fragment\n",
    "    sticky=False,   # whether to stop interacting if an env resets mid-fragment\n",
    "    device=None,    # specifies the device to put the actor's inputs onto\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dbe569",
   "metadata": {},
   "source": [
    "`rollout()` returns an iterator, which has, roughly, the same logic,\n",
    "as the manual collection above.\n",
    "\n",
    "Inside the infinite loop it copies `fragment.pyt` onto `device`, before\n",
    "yielding it to the user. It also does not spawn its own batch of environments,\n",
    "unlike parallel variants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16edf5d1",
   "metadata": {},
   "source": [
    "The user has to manually limit the number of iterations using, for example,\n",
    "\n",
    "```python\n",
    "it = same.rollout(...)\n",
    "\n",
    "for b, batch in zip(range(100), it):\n",
    "    # train on batch\n",
    "    pass\n",
    "\n",
    "it.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7550f6e5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0d7e31",
   "metadata": {},
   "source": [
    "### Rollout collection (single-process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fdc822",
   "metadata": {},
   "source": [
    "Single-actor rollout sampler running in a parallel process (double-buffered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout import single"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2989ca0c",
   "metadata": {},
   "source": [
    "Under the hood the functions creates **two** rollout fragment buffers, maintains\n",
    "a reference to the specified `actor`, makes a shared copy of it (on the host), and\n",
    "then spawns one worker process.\n",
    "\n",
    "The worker, in turn, makes its own local copy of the actor on the specified device,\n",
    "initializes the environments and the running context. During collection it altrenates\n",
    "between the buffers, into which it records the rollout fragments it collects. Except\n",
    "for double buffering, the logic is identical to `rollout`.\n",
    "\n",
    "The local copies of the actor are **automatically updated** from the maintained reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563a31cf",
   "metadata": {},
   "source": [
    "```python\n",
    "it = single.rollout(\n",
    "    factory,              # the environment factory\n",
    "    actor,                # the actor reference, used to update the local actors\n",
    "\n",
    "    n_steps,              # the duration of a rollout fragment\n",
    "    n_envs,               # the number of independent environments in the batch\n",
    "\n",
    "    sticky=False,         # do we freeze terminated environments until the end of the rollout?\n",
    "                          #  required if we wish to leverage cudnn's fast RNN implementations,\n",
    "                          #  instead of manually stepping through the RNN core.\n",
    "\n",
    "    clone=True,           # should the worker use a local clone of the reference actor\n",
    "\n",
    "    close=True,           # should we `.close()` the environments when cleaning up?\n",
    "                          #  some envs are very particular about this, e.g. nle\n",
    "\n",
    "    start_method='fork',  # `fork` in notebooks, `spawn` in linux/macos and if we interchange\n",
    "                          #  cuda tensors between processes (we DO NOT do that: we exchange indices\n",
    "                          #  to host-shapred tensors)\n",
    "\n",
    "    device=None,          # the device on which to collect rollouts (the local actor is moved\n",
    "                          #  onto this device)\n",
    ")\n",
    "\n",
    "# ...\n",
    "\n",
    "it.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59902d9",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c807624e",
   "metadata": {},
   "source": [
    "### Rollout collection (multi-process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c8c18a",
   "metadata": {},
   "source": [
    "A more load-balanced multi-actor milti-process sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56b0957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout import multi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ed4f7d",
   "metadata": {},
   "source": [
    "This version of the rollout collector allocates several buffers and spawns\n",
    "many parallel workers. Each worker creates it own local copy of the actor,\n",
    "instantiates `n_envs` local environments and allocates a running context for\n",
    "all of them. The rollout collection in each worker is **hardcoded to run on\n",
    "the host device**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b475cdc",
   "metadata": {},
   "source": [
    "```python\n",
    "it = multi.rollout(\n",
    "    factory,              # the environment factory\n",
    "    actor,                # the actor reference, used to update the local actors\n",
    "\n",
    "    n_steps,              # the duration of each rollout fragment\n",
    "\n",
    "    n_actors,             # the number of parallel actors\n",
    "    n_per_actor,          # the number of independent environments run in each actor\n",
    "    n_buffers,            # the size of the pool of buffers, into which rollout\n",
    "                          #  fragments are collected. Should not be less than `n_actors`.\n",
    "    n_per_batch,          # the number of fragments collated into a batch\n",
    "\n",
    "    sticky=False,         # do we freeze terminated environments until the end of the rollout?\n",
    "                          #  required if we wish to leverage cudnn's fast RNN implementations,\n",
    "                          #  instead of manually stepping through the RNN core.\n",
    "\n",
    "    pinned=False,\n",
    "\n",
    "    clone=True,           # should the parallel actors use a local clone of the reference actor\n",
    "\n",
    "    close=True,           # should we `.close()` the environments when cleaning up?\n",
    "                          #  some envs are very particular about this, e.g. nle\n",
    "\n",
    "    device=None,          # the device onto which to move the rollout batches\n",
    "\n",
    "    start_method='fork',  # `fork` in notebooks, `spawn` in linux/macos and if we interchange\n",
    "                          #  cuda tensors between processes (we DO NOT do that: we exchange indices\n",
    "                          #  to host-shared tensors)\n",
    ")\n",
    "\n",
    "# ...\n",
    "\n",
    "it.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66981a6",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49ac75f",
   "metadata": {},
   "source": [
    "### Evaluation (same-process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fe5202",
   "metadata": {},
   "source": [
    "In order to evaluate an actor in a batch of environments, one can use `evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine import core\n",
    "\n",
    "# help(core.evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee3a21c",
   "metadata": {},
   "source": [
    "The function *does not* collect the rollout data, except for the rewards.\n",
    "Below is the intended use case.\n",
    "* **NB** this is run in the same process, hence blocks until completion, which\n",
    "might take considerable time (esp. if `n_steps` is unbounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a700acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same process\n",
    "def same_evaluate(\n",
    "    factory, actor, n_envs=4,\n",
    "    *, n_steps=None, close=True, render=False, device=None\n",
    "):\n",
    "    # spawn a batch of environments\n",
    "    envs = [factory() for _ in range(n_envs)]\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            rewards, _ = core.evaluate(\n",
    "                envs, actor, n_steps=n_steps,\n",
    "                render=render, device=device)\n",
    "\n",
    "            # get the accumulated rewards (gamma=1)\n",
    "            yield sum(rewards)\n",
    "\n",
    "    finally:\n",
    "        if close:\n",
    "            for e in envs:\n",
    "                e.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a0011a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce457ae0",
   "metadata": {},
   "source": [
    "### Evaluation (parallel process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0299259",
   "metadata": {},
   "source": [
    "Like rollout collection, evaluation can (and probably shoulb) be performed in\n",
    "a parallel process, so that it does not burden the main thread with computations\n",
    "not related to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3193236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout.evaluate import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90b222",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de9187",
   "metadata": {},
   "source": [
    "## CartPole with REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e701e3",
   "metadata": {},
   "source": [
    "### the CartPole Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dbbcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# hotfix for gym's unresponsive viz (spawns gl threads!)\n",
    "import rlplay.utils.integration.gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3716f023",
   "metadata": {},
   "source": [
    "The environment factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f0fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FP32Observation(gym.ObservationWrapper):\n",
    "    def observation(self, observation):\n",
    "        return observation.astype(numpy.float32)\n",
    "#         obs[0] = 0.  # mask the position info\n",
    "#         return obs  # observation.astype(numpy.float32)\n",
    "\n",
    "def factory():\n",
    "    return FP32Observation(gym.make(\"CartPole-v0\").unwrapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8b9ac",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca2b7c",
   "metadata": {},
   "source": [
    "### the algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829b01a4",
   "metadata": {},
   "source": [
    "Service functions for the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0046223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.utils.plyr import apply, suply, xgetitem\n",
    "\n",
    "\n",
    "def timeshift(state, *, shift=1):\n",
    "    \"\"\"Get current and shfited slices of nested objects.\"\"\"\n",
    "    # use xgetitem to lett None through\n",
    "    # XXX `curr[t]` = (x_t, a_{t-1}, r_t, d_t), t=0..T-H\n",
    "    curr = suply(xgetitem, state, index=slice(None, -shift))\n",
    "\n",
    "    # XXX `next[t]` = (x_{t+H}, a_{t+H-1}, r_{t+H}, d_{t+H}), t=0..T-H\n",
    "    next = suply(xgetitem, state, index=slice(shift, None))\n",
    "\n",
    "    return curr, next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdb5309",
   "metadata": {},
   "source": [
    "The reinforce PG algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6d6aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.algo.returns import pyt_returns\n",
    "\n",
    "# @torch.enable_grad()\n",
    "def reinforce(fragment, module, *, gamma=0.99, C_entropy=1e-2,\n",
    "              c_rho=float('inf')):\n",
    "    r\"\"\"The REINFORCE algorithm (importance-weighted off-policy).\n",
    "\n",
    "    The basic policy-gradient alogrithm with a baseline $b_t$:\n",
    "    $$\n",
    "        \\nabla_\\theta J(s_t)\n",
    "            = \\mathbb{E}_{a \\sim \\beta(a\\mid s_t)}\n",
    "                \\frac{\\pi(a\\mid s_t)}{\\beta(a\\mid s_t)}\n",
    "                    \\bigl( r_{t+1} + \\gamma G_{t+1} - b_t \\bigr)\n",
    "                \\nabla_\\theta \\log \\pi(a\\mid s_t)\n",
    "        \\,. $$\n",
    "    \"\"\"\n",
    "    \n",
    "    state, state_next = timeshift(fragment.state)\n",
    "\n",
    "    # REACT: (state[t], h_t) \\to (\\hat{a}_t, h_{t+1}, \\hat{A}_t)\n",
    "    _, _, info = module(\n",
    "        state.obs, state.act, state.rew, state.fin,\n",
    "        hx=fragment.hx, stepno=state.stepno)\n",
    "\n",
    "    # \\pi is the target policy, \\mu is the behaviour policy\n",
    "    log_pi, log_mu = info['logits'], fragment.actor['logits']\n",
    "\n",
    "    # Assume .act is unstructured: `act[t]` = a_{t+1} -->> T x B x 1\n",
    "    act = state_next.act.unsqueeze(-1)\n",
    "\n",
    "    # the importance weights\n",
    "    log_pi_a = log_pi.gather(-1, act).squeeze(-1)\n",
    "    log_mu_a = log_mu.gather(-1, act).squeeze(-1)\n",
    "\n",
    "    bootstrap = torch.tensor(0.)\n",
    "    # XXX bootstrap with the perpetual lasr reward?\n",
    "    # bootstrap = state_next.rew[-1].div(1 - gamma)\n",
    "\n",
    "    # The present value of the future rewards following `state[t]`:\n",
    "    #    G_t = r_{t+1} + \\gamma G_{t+1}\n",
    "    ret = pyt_returns(state_next.rew, state_next.fin,\n",
    "                      gamma=gamma, bootstrap=bootstrap)\n",
    "    # ret.sub_(ret.mean(dim=0))\n",
    "    # ret.div_(ret.std(dim=0))\n",
    "\n",
    "    # the policy surrogate score (max)\n",
    "    #    \\frac1T \\sum_t \\rho_t (G_t - b_t) \\log \\pi(a_t \\mid s_t)\n",
    "    rho = log_mu_a.sub_(log_pi_a.detach()).neg_().exp_().clamp_(max=c_rho)\n",
    "    reinfscore = log_pi_a.mul(ret.mul_(rho)).mean()\n",
    "\n",
    "    # the policy neg-entropy score (min)\n",
    "    #   - H(\\pi(•\\mid s)) = - (-1) \\sum_a \\pi(a\\mid s) \\log \\pi(a\\mid s)\n",
    "    f_min = torch.finfo(log_pi.dtype).min\n",
    "    negentropy = log_pi.exp().mul(log_pi.clamp(min=f_min)).sum(dim=-1).mean()\n",
    "\n",
    "    # maximize the entropy and the reinforce score\n",
    "    # \\ell := - \\frac1T \\sum_t G_t \\log \\pi(a_t \\mid s_t)\n",
    "    #         - C \\mathbb{H} \\pi(\\cdot \\mid s_t)\n",
    "    loss = C_entropy * negentropy - reinfscore\n",
    "    return loss.mean(), dict(\n",
    "        entropy=-float(negentropy),\n",
    "        policy_score=float(reinfscore),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab8352",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8bf14",
   "metadata": {},
   "source": [
    "### the Actor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442a90a2",
   "metadata": {},
   "source": [
    "A procedure and a layer, which converts the input integer data into its\n",
    "little endian binary representation as float $\\{0, 1\\}^m$ vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotbits(input, n_bits=63, dtype=torch.float):\n",
    "    \"\"\"Encode integers to fixed-width binary floating point vectors\"\"\"\n",
    "    assert not input.dtype.is_floating_point\n",
    "    assert 0 < n_bits < 64  # torch.int64 is signed, so 64-1 bits max\n",
    "\n",
    "    # n_bits = {torch.int64: 63, torch.int32: 31, torch.int16: 15, torch.int8 : 7}\n",
    "\n",
    "    # get mask of set bits\n",
    "    pow2 = torch.tensor([1 << j for j in range(n_bits)]).to(input.device)\n",
    "    x = input.unsqueeze(-1).bitwise_and(pow2).to(bool)\n",
    "\n",
    "    # upcast bool to float to get one-hot\n",
    "    return x.to(dtype)\n",
    "\n",
    "\n",
    "class OneHotBits(torch.nn.Module):\n",
    "    def __init__(self, n_bits=63, dtype=torch.float):\n",
    "        assert 1 <= n_bits < 64\n",
    "        super().__init__()\n",
    "        self.n_bits, self.dtype = n_bits, dtype\n",
    "\n",
    "    def forward(self, input):\n",
    "        return onehotbits(input, n_bits=self.n_bits, dtype=self.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b268946e",
   "metadata": {},
   "source": [
    "A special module dictionary, which aplies itself to the input dict of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf00879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Mapping\n",
    "from torch.nn import Module, ModuleDict as BaseModuleDict\n",
    "\n",
    "\n",
    "class ModuleDict(BaseModuleDict):\n",
    "    \"\"\"The ModuleDict, that applies itself to hte indup dicts.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        modules: Optional[Mapping[str, Module]] = None,\n",
    "        dim: Optional[int]=-1\n",
    "    ) -> None:\n",
    "        super().__init__(modules)\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        # enforce concatenation in the order of the declaration in  __init__\n",
    "        return torch.cat([\n",
    "            m(input[k]) for k, m in self.items()\n",
    "        ], dim=self.dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89ef0f4",
   "metadata": {},
   "source": [
    "A policy which uses many inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb459fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "from torch.nn import Embedding, Linear, Identity\n",
    "from torch.nn import ReLU, LogSoftmax\n",
    "\n",
    "def policy():\n",
    "    return Sequential(\n",
    "        ModuleDict(dict(\n",
    "#             stepno=Sequential(\n",
    "#                 OneHotBits(), Linear(63, 4, bias=False)\n",
    "#             ),\n",
    "            obs=Identity(),\n",
    "            act=Embedding(2, 4),\n",
    "        )),\n",
    "        Linear(0 + 4 + 4, 32),\n",
    "        ReLU(),\n",
    "        Linear(32, 2),\n",
    "        LogSoftmax(dim=-1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee0135",
   "metadata": {},
   "source": [
    "The discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50f026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "C_entropy = 0.01\n",
    "c_rho = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99474f26",
   "metadata": {},
   "source": [
    "Initialize the learner and the factories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d0726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "factory_eval = partial(factory)\n",
    "\n",
    "learner, sticky = PolicyWrapper(policy()), False\n",
    "\n",
    "learner.train()\n",
    "device_ = torch.device('cpu')  # torch.device('cuda:0')\n",
    "learner.to(device=device_)\n",
    "\n",
    "# prepare the optimizer for the learner\n",
    "optim = torch.optim.Adam(learner.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82730b1a",
   "metadata": {},
   "source": [
    "Initialize the sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991c289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "T, B = 25, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c13d6d",
   "metadata": {},
   "source": [
    "Pick one collector\n",
    "* NetHack environment `nle`, does not like `fork` method, so we should use `spawn`, which is not notebook friendly :(\n",
    "  * essentially it is better to prototype in notebook with `same.rollout`, then write a submodule non-interactive script with `multi.rollout`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c998c7e2",
   "metadata": {},
   "source": [
    "# generator of rollout batches\n",
    "batchit = same.rollout(\n",
    "    [factory() for _ in range(B)],\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    sticky=sticky,\n",
    "    device=device_,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3dda69a0",
   "metadata": {},
   "source": [
    "# generator of rollout batches\n",
    "batchit = single.rollout(\n",
    "    factory,\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    n_envs=B,\n",
    "    sticky=sticky,  # so that we can leverage cudnn's fast RNN implementations\n",
    "    clone=False,\n",
    "    close=False,\n",
    "    device=device_,\n",
    "    start_method='fork',  # fork in notebook for macos, spawn in linux\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b681a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator of rollout batches\n",
    "batchit = multi.rollout(\n",
    "    factory,\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    n_actors=16,\n",
    "    n_per_actor=B,\n",
    "    n_buffers=24,\n",
    "    n_per_batch=2,\n",
    "    sticky=sticky,  # so that we can leverage cudnn's fast RNN implementations\n",
    "    pinned=False,\n",
    "    clone=True,\n",
    "    close=False,\n",
    "    device=device_,\n",
    "    start_method='fork',  # fork in notebook for macos, spawn in linux\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590fae99",
   "metadata": {},
   "source": [
    "Generator of evaluation rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036c0b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_it = test(factory_eval, learner, n_envs=4, n_steps=500, device=device_)\n",
    "test_it = evaluate(factory_eval, learner, n_envs=4, n_steps=500,\n",
    "                   clone=False, device=device_, start_method='fork')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846d8c9",
   "metadata": {},
   "source": [
    "Implement your favourite training method"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1cbc1265",
   "metadata": {},
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318c6127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "# from math import log, exp\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# pytoch loves to hog all threads on some linux systems \n",
    "torch.set_num_threads(1)\n",
    "\n",
    "# the training loop\n",
    "losses, rewards = [], []\n",
    "# decay = -log(2) / 25  # exploration epsilon halflife\n",
    "for epoch in tqdm.tqdm(range(100)):\n",
    "    for j, batch in zip(range(100), batchit):\n",
    "        loss, info = reinforce(batch, learner, gamma=gamma,\n",
    "                               C_entropy=C_entropy, c_rho=c_rho)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_norm = clip_grad_norm_(learner.parameters(), max_norm=numpy.inf)\n",
    "        optim.step()\n",
    "    \n",
    "        losses.append(dict(\n",
    "            loss=float(loss),\n",
    "            grad=float(grad_norm),\n",
    "            **info\n",
    "        ))\n",
    "\n",
    "    # fetch the evaluation results (lag by one inner loop!)\n",
    "    rewards.append(next(test_it))\n",
    "\n",
    "    # learner.epsilon.mul_(exp(decay)).clip_(0.1, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0633c36c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# close the generators\n",
    "batchit.close()\n",
    "test_it.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45cc8a1",
   "metadata": {},
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce115d82",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(records):\n",
    "    \"\"\"collate identically keyed dicts\"\"\"\n",
    "    out, n_records = {}, 0\n",
    "    for record in records:\n",
    "        for k, v in record.items():\n",
    "            out.setdefault(k, []).append(v)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "data = {k: numpy.array(v) for k, v in collate(losses).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'loss' in data:\n",
    "    plt.plot(data['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2638e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'entropy' in data:\n",
    "    plt.plot(data['entropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ffbbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'policy_score' in data:\n",
    "    plt.plot(data['policy_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01581b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(data['grad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3128f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = numpy.stack(rewards, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b9191",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd6ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = numpy.median(rewards, axis=-1), rewards.std(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5545163",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fi, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "\n",
    "ax.plot(numpy.mean(rewards, axis=-1))\n",
    "ax.plot(numpy.median(rewards, axis=-1))\n",
    "ax.plot(numpy.min(rewards, axis=-1))\n",
    "ax.plot(numpy.std(rewards, axis=-1))\n",
    "# ax.plot(m+s * 1.96)\n",
    "# ax.plot(m-s * 1.96)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0930f407",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dca3838",
   "metadata": {},
   "source": [
    "The ultimate evaluation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d372a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with factory_eval() as env:\n",
    "    learner.eval()\n",
    "    eval_rewards, info = core.evaluate([\n",
    "        env\n",
    "    ], learner, render=True, n_steps=1e4, device=device_)\n",
    "\n",
    "print(sum(eval_rewards))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9852c395",
   "metadata": {},
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c864581",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e288e10",
   "metadata": {},
   "source": [
    "Let's analyze the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a742795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.special import softmax, expit, entr\n",
    "\n",
    "*head, n_actions = info['logits'].shape\n",
    "proba = softmax(info['logits'], axis=-1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "ax.plot(entr(proba).sum(-1)[:, 0])\n",
    "ax.axhline(math.log(n_actions), c='k', alpha=0.5, lw=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a12f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "ax.hist(info['logits'][..., 1] - info['logits'][..., 0], bins=51);  # log-ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4cdeca",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19438b2d",
   "metadata": {},
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09142f04",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stepno = batch.state.stepno\n",
    "stepno = torch.arange(8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91906ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = learner.policy[0]['stepno'](stepno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4953372",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8), dpi=200,\n",
    "                         sharex=True, sharey=True)\n",
    "\n",
    "for j, ax in zip(range(out.shape[1]), axes.flat):\n",
    "    ax.plot(out[:, j], lw=1)\n",
    "\n",
    "fig.tight_layout(pad=0, h_pad=0, w_pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f353e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    plt.imshow(abs(learner.policy[4].weight) @ abs(learner.policy[1].weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    plt.imshow(abs(learner.policy[0]['stepno'][-1].weight)[:, :16].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7256ce6",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
