{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2be71d",
   "metadata": {},
   "source": [
    "# `rlplay`-ing around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b338efe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664b895d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4128b76d",
   "metadata": {},
   "source": [
    "## Rollout collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b1f78",
   "metadata": {},
   "source": [
    "Rollout collection is designed to be as much `plug-n-play` as possible, i.e. it\n",
    "supports **arbitrarily structured nested containers** of arrays or tensors for\n",
    "environment observations and actions. The actor, however, should **expose**\n",
    "certain API (described below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2283976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine import collect  # the collector's core\n",
    "\n",
    "# print(collect.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c293055e",
   "metadata": {},
   "source": [
    "It's role is to serve as a *middle-man* between the **actor-environment** pair\n",
    "and the **training loop**: to track the trajectory of the actor in the environment,\n",
    "and properly record it into the data buffer.\n",
    "\n",
    "For example, it is not responsible for seeding or randomization of environments\n",
    "(i'm looking at you, `AtariEnv`), and datatype casting (except for rewards,\n",
    "which are cast to `fp32` automatically). In theory, there is **no need** for\n",
    "special data preporcessing, except for, perhaps, casting data to proper dtypes,\n",
    "like from `numpy.float64` observations to `float32` in `CartPole`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8d22c",
   "metadata": {},
   "source": [
    "#### Semantics\n",
    "\n",
    "The collector just carefully records the trajectory following the\n",
    "**REACT** $\\rightarrow$ **STEP+EMIT** process:\n",
    "\n",
    "* **REACT** The actor performs the following update\n",
    "$$\n",
    "    (x_t, a_{t-1}, r_t, d_t, h_t)\n",
    "        \\overset{\\mathrm{Actor}}{\\longrightarrow}\n",
    "        (a_t, h_{t+1})\n",
    "    \\,. $$\n",
    "\n",
    "\n",
    "* **STEP+EMIT** The environment updates it's unobserved state and emits observed data\n",
    "$$\n",
    "    (s_t, a_t)\n",
    "        \\overset{\\mathrm{Env}}{\\longrightarrow}\n",
    "        (s_{t+1}, x_{t+1}, r_{t+1}, d_{t+1})\n",
    "    \\,, $$\n",
    "\n",
    "where $d_t = \\top$ if $s_t$ is terminal, else $\\bot$, $r_t$ is a scalar reward\n",
    "(auxiliary actor's and env's info dicts are not shown in this diagram)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623f79a2",
   "metadata": {},
   "source": [
    "##### Requirements\n",
    "\n",
    "* all nested containers **must be** built from pure python `dicts`, `lists`, `tuples` or `namedtuples`\n",
    "\n",
    "* the environment communicates either in **numpy arrays** or in python **scalars**, but not in data types that are incompatible with pytorch (such as `str` or `bytes`)\n",
    "\n",
    "```python\n",
    "# example\n",
    "obs = {\n",
    "    'camera': {\n",
    "        'rear': numpy.zeros(3, 320, 240),\n",
    "        'front': numpy.zeros(3, 320, 240),\n",
    "    },\n",
    "    'proximity': (+0.1, +0.2, -0.1, +0.0,),\n",
    "    'other': {\n",
    "        'fuel_tank': 78.5,\n",
    "        'passenger': False,\n",
    "    },\n",
    "}\n",
    "```\n",
    "\n",
    "* the actor communicates in torch tensors **only**\n",
    "\n",
    "* the environment produces **float scalar** rewards (other data may be communicated through auxiliary environment info-dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a9efdc",
   "metadata": {},
   "source": [
    "### Creating the actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a416b600",
   "metadata": {},
   "source": [
    "Rollout collection relies on the following API of the actor:\n",
    "* `.reset(j, hx)` reset the recurrent state of the j-th environment in the batch (if applicable)\n",
    "  * `hx` contains tensors with shape `n_lstm_layers x batch x hidden`, or is an empty tuple\n",
    "\n",
    "* `.step(obs, act, rew, fin, hx)` get the next action $a_t$, the recurrent state $h_{t+1}$, and\n",
    "the **extra info** in response to $x_t$, $a_{t-1}$, $r_t$, $d_t$, and $h_t$, respectively.\n",
    "  * extra info `dict` **should** include `value` key with a `T x B` tensor of value estimates\n",
    "  * MUST allocate new `hx` if the recurrent state is updated\n",
    "  * MUST NOT change the inputs in-place\n",
    "\n",
    "* `.value(obs, act, rew, fin, hx)` compute the value-function estimate $\n",
    "      v(s_t) \\approx G_t = \\mathbb{E} \\sum_{j\\geq t} \\gamma^{j-t} r_{j+1}\n",
    "  $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine import BaseActorModule\n",
    "\n",
    "# BaseActorModule??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f758a",
   "metadata": {},
   "source": [
    "`BaseActorModule` is essentially a thin sub-class of `torch.nn.Module`,\n",
    "that implements the API through `.forward(obs, act, rew, fin, hx)`, which\n",
    "should return three things:\n",
    "\n",
    "1. `actions` prescribed actions in the environment, with data of shape `n_steps x batch x ...`\n",
    "  * can be a nested container of dicts, lists, and tuples\n",
    "\n",
    "\n",
    "2. `hx` data with shape `n_steps x batch x ...`\n",
    "  * can be a nested container of dicts, lists, and tuples\n",
    "  * **if an actor is not recurrent**, then must return an empty tuple `()`\n",
    "\n",
    "\n",
    "3. `info` dict with extra `n_steps x batch x ...` data\n",
    "  * `value` -- the value function estimates\n",
    "  * `logits` -- the policy logits (if applicable)\n",
    "  * and other stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd026e4",
   "metadata": {},
   "source": [
    "Here is an example actor, that wraps a simple MLP policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54221cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.utils.common import multinomial\n",
    "\n",
    "class nonRecurrentPolicyWrapper(BaseActorModule):\n",
    "    \"\"\"Example wrapper for a non-recurrent policy.\n",
    "    \n",
    "    Details\n",
    "    -------\n",
    "    This example assumes flat `Discrete(n)` action space, and\n",
    "    simple non-structured observation space, e.g. a python scalar\n",
    "    or a `numpy.array`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, policy, *, epsilon=0.1):\n",
    "        super().__init__()\n",
    "        self.policy, self.epsilon = policy, epsilon\n",
    "\n",
    "    def forward(self, obs, act=None, rew=None, fin=None, *, hx=None, stepno=None):\n",
    "        # Everything is  [T x B x ...]\n",
    "        logits, hx = self.policy(obs, act, rew), ()\n",
    "\n",
    "        # value must not have any trailing dims, i.e. T x B\n",
    "        value = logits.new_zeros(fin.shape)\n",
    "\n",
    "        # XXX eps-greedy?\n",
    "        if self.training:\n",
    "            unif = torch.tensor(1. / logits.shape[-1])\n",
    "\n",
    "            prob = logits.detach().exp()\n",
    "            prob.mul_(1 - self.epsilon)\n",
    "            prob.add_(unif, alpha=self.epsilon)\n",
    "\n",
    "            actions = multinomial(prob)\n",
    "\n",
    "        else:\n",
    "            actions = logits.argmax(dim=-1)\n",
    "\n",
    "        return actions, hx, value, dict(logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60432ab",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0a430a",
   "metadata": {},
   "source": [
    "### Rollout collection (same-process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c77045",
   "metadata": {},
   "source": [
    "Collect rollouts within the current process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce3815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout import same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536dca76",
   "metadata": {},
   "source": [
    "The parameters have the following meaning\n",
    "```python\n",
    "n_envs = 16     # the number of envs in the batch\n",
    "n_steps = 51    # the length of each rollout fragment\n",
    "sticky = False  # whether to stop interacting if an env resets mid-fragment\n",
    "device = None   # specifies the device to put the actor's inputs onto\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdfdfe0",
   "metadata": {},
   "source": [
    "`rollout()` returns an iterator, which does the following, roughly.\n",
    "\n",
    "Prepare the run-time context for the specified `actor` and the environments\n",
    "```python\n",
    "# spawn multiple envs\n",
    "envs = [env_factory() for _ in n_envs]\n",
    "\n",
    "# initialize a buffer for one rollout fragment (optionally pinned)\n",
    "buffer = prepare(envs[0], actor, n_steps, len(envs),\n",
    "                 pinned=pinned, device=device)\n",
    "\n",
    "# the running context tor the actor and the envs\n",
    "ctx, fragment = startup(envs, actor, buffer, pinned=pinned)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51374fb",
   "metadata": {},
   "source": [
    "Now within the infinite loop it does the following\n",
    "```python\n",
    "# collect the fragment\n",
    "collect(envs, actor, fragment, ctx, sticky=sticky, device=device)\n",
    "\n",
    "# fragment.pyt -- torch tensors, fragment.npy -- numpy arrays (aliased)\n",
    "# copy fragment.pyt onto `device`, and yield it to the user\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3bdd9f",
   "metadata": {},
   "source": [
    "The user has to manually limit the number of iterations using, for example,\n",
    "\n",
    "```python\n",
    "it = same.rollout(...)\n",
    "\n",
    "for b, batch in zip(range(100), it):\n",
    "    # train on batch\n",
    "    pass\n",
    "\n",
    "it.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d236b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733cc29c",
   "metadata": {},
   "source": [
    "### Rollout collection (single-process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f22ff",
   "metadata": {},
   "source": [
    "Single-actor rollout sampler running in a parallel process (double-buffered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559b8749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout import single"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c30a1d",
   "metadata": {},
   "source": [
    "Under the hood the functions creates **two** rollout fragment buffers, maintains\n",
    "a reference to the specified `actor`, makes a shared copy of it (on the host), and\n",
    "then spawns one worker process.\n",
    "\n",
    "The worker, in turn, makes its own local copy of the actor on the specified device,\n",
    "initializes the environments and the running context. During collection it altrenates\n",
    "between the buffers, into which it records the rollout fragments it collects. Except\n",
    "for double buffering, the logic is identical to `rollout`.\n",
    "\n",
    "The local copies of the actor are **automatically updated** from the maintained reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44520b9",
   "metadata": {},
   "source": [
    "```python\n",
    "it = single.rollout(\n",
    "    factory,              # the environment factory\n",
    "    actor,                # the actor reference, used to update the local actors\n",
    "\n",
    "    n_steps,              # the duration of a rollout fragment\n",
    "    n_envs,               # the number of independent environments in the batch\n",
    "\n",
    "    sticky=False,         # do we freeze terminated environments until the end of the rollout?\n",
    "                          #  required if we wish to leverage cudnn's fast RNN implementations,\n",
    "                          #  instead of manually stepping through the RNN core.\n",
    "\n",
    "    clone=True,           # should the worker use a local clone of the reference actor\n",
    "\n",
    "    close=True,           # should we `.close()` the environments when cleaning up?\n",
    "                          #  some envs are very particular about this, e.g. nle\n",
    "\n",
    "    start_method='fork',  # `fork` in notebooks, `spawn` in linux/macos and if we interchange\n",
    "                          #  cuda tensors between processes (we DO NOT do that: we exchange indices\n",
    "                          #  to host-shapred tensors)\n",
    "\n",
    "    device=None,          # the device on which to collect rollouts (the local actor is moved\n",
    "                          #  onto this device)\n",
    ")\n",
    "\n",
    "# ...\n",
    "\n",
    "it.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438b37c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b54f70",
   "metadata": {},
   "source": [
    "### Rollout collection (multi-process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1512d3f",
   "metadata": {},
   "source": [
    "A more load-balanced multi-actor milti-process sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10fd074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout import multi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9195158",
   "metadata": {},
   "source": [
    "This version of the rollout collector allocates several buffers and spawns\n",
    "many parallel workers. Each worker creates it own local copy of the actor,\n",
    "instantiates `n_envs` local environments and allocates a running context for\n",
    "all of them. The rollout collection in each worker is **hardcoded to run on\n",
    "the host device**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ebe66",
   "metadata": {},
   "source": [
    "```python\n",
    "it = multi.rollout(\n",
    "    factory,              # the environment factory\n",
    "    actor,                # the actor reference, used to update the local actors\n",
    "\n",
    "    n_steps,              # the duration of each rollout fragment\n",
    "\n",
    "    n_actors,             # the number of parallel actors\n",
    "    n_per_actor,          # the number of independent environments run in each actor\n",
    "    n_buffers,            # the size of the pool of buffers, into which rollout\n",
    "                          #  fragments are collected. Should not be less than `n_actors`.\n",
    "    n_per_batch,          # the number of fragments collated into a batch\n",
    "\n",
    "    sticky=False,         # do we freeze terminated environments until the end of the rollout?\n",
    "                          #  required if we wish to leverage cudnn's fast RNN implementations,\n",
    "                          #  instead of manually stepping through the RNN core.\n",
    "\n",
    "    pinned=False,\n",
    "\n",
    "    clone=True,           # should the parallel actors use a local clone of the reference actor\n",
    "\n",
    "    close=True,           # should we `.close()` the environments when cleaning up?\n",
    "                          #  some envs are very particular about this, e.g. nle\n",
    "\n",
    "    device=None,          # the device onto which to move the rollout batches\n",
    "\n",
    "    start_method='fork',  # `fork` in notebooks, `spawn` in linux/macos and if we interchange\n",
    "                          #  cuda tensors between processes (we DO NOT do that: we exchange indices\n",
    "                          #  to host-shared tensors)\n",
    ")\n",
    "\n",
    "# ...\n",
    "\n",
    "it.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd045c3",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c414d5",
   "metadata": {},
   "source": [
    "### Evaluation (same-process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21598bc",
   "metadata": {},
   "source": [
    "In order to evaluate an actor in a batch of environments, one can use `evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc3da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine import evaluate as core_evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357dec8d",
   "metadata": {},
   "source": [
    "The function *does not* collect the rollout data, except for the rewards.\n",
    "Below is the intended use case.\n",
    "* **NB** this is run in the same process, hence blocks until completion, which\n",
    "might take considerable time (esp. if `n_steps` is unbounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748bdb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same process\n",
    "def same_evaluate(\n",
    "    factory, actor, n_envs=4,\n",
    "    *, n_steps=None, close=True, render=False, device=None\n",
    "):\n",
    "    # spawn a batch of environments\n",
    "    envs = [factory() for _ in range(n_envs)]\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            rewards, bootstrap = core_evaluate(\n",
    "                envs, actor, n_steps=n_steps,\n",
    "                render=render, device=device)\n",
    "\n",
    "            # get the accumulated rewards (gamma=1)\n",
    "            yield sum(rewards)\n",
    "\n",
    "    finally:\n",
    "        if close:\n",
    "            for e in envs:\n",
    "                e.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ff233",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5032d84f",
   "metadata": {},
   "source": [
    "### Evaluation (parallel process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b0c7e7",
   "metadata": {},
   "source": [
    "Like rollout collection, evaluation can (and probably shoulb) be performed in\n",
    "a parallel process, so that it does not burden the main thread with computations\n",
    "not related to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1af2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout.evaluate import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6d7855",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a3b8d3",
   "metadata": {},
   "source": [
    "## CartPole with REINFORCE or A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef71868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# hotfix for gym's unresponsive viz (spawns gl threads!)\n",
    "import rlplay.utils.integration.gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3364e7",
   "metadata": {},
   "source": [
    "The environment factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d0177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from rlplay.zoo.env import NarrowPath\n",
    "\n",
    "\n",
    "class FP32Observation(gym.ObservationWrapper):\n",
    "    def observation(self, observation):\n",
    "        obs = observation.astype(numpy.float32)\n",
    "        obs[0] = 0.  # mask the position info\n",
    "        return obs\n",
    "\n",
    "#     def step(self, action):\n",
    "#         obs, reward, done, info = super().step(action)\n",
    "#         reward -= abs(obs[1]) / 10  # punish for non-zero speed\n",
    "#         return obs, reward, done, info\n",
    "\n",
    "class OneHotObservation(gym.ObservationWrapper):\n",
    "    def observation(self, observation):\n",
    "        return numpy.eye(1, self.env.observation_space.n,\n",
    "                         k=observation, dtype=numpy.float32)[0]\n",
    "\n",
    "def factory():\n",
    "    return FP32Observation(gym.make(\"CartPole-v0\").unwrapped)\n",
    "#     return gym.make(\"Taxi-v3\").unwrapped\n",
    "    # return OneHotObservation(NarrowPath())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4482510f",
   "metadata": {},
   "source": [
    "Service functions for the pg algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd61a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.algo.returns import pyt_returns\n",
    "from rlplay.engine.utils.plyr import suply, getitem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81cf24",
   "metadata": {},
   "source": [
    "The reinforce PG algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759fb164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(batch, module, *, gamma=0.99, C_entropy=1e-2,\n",
    "              c_rho=float('inf')):\n",
    "    r\"\"\"The REINFORCE algorithm (importance-weighted off-policy).\n",
    "\n",
    "    The basic policy-gradient alogrithm with a baseline $b_t$:\n",
    "    $$\n",
    "        \\nabla_\\theta J(s_t)\n",
    "            = \\mathbb{E}_{a \\sim \\beta(a\\mid s_t)}\n",
    "                \\frac{\\pi(a\\mid s_t)}{\\beta(a\\mid s_t)}\n",
    "                    \\bigl( r_{t+1} + \\gamma G_{t+1} - b_t \\bigr)\n",
    "                \\nabla_\\theta \\log \\pi(a\\mid s_t)\n",
    "        \\,. $$\n",
    "    \"\"\"\n",
    "\n",
    "    # XXX `state[t]` = (x_t, a_{t-1}, r_t, d_t), t=0..T-1\n",
    "    state = suply(getitem, batch.state, index=slice(None, -1))\n",
    "\n",
    "    # XXX `state_next[t]` = (x_{t+1}, a_t, r_{t+1}, d_{t+1}), t=0..T-1\n",
    "    state_next = suply(getitem, batch.state, index=slice(1, None))\n",
    "\n",
    "    # REACT: (state[t], h_t) \\to (\\hat{a}_t, h_{t+1}, \\hat{A}_{t+1})\n",
    "    _, _, _, info = module(\n",
    "        state.obs, state.act, state.rew, state.fin,\n",
    "        hx=batch.hx, stepno=state.stepno)\n",
    "\n",
    "    # The present value of the future rewards following `state[t]`:\n",
    "    #    G_t = r_{t+1} + \\gamma G_{t+1}\n",
    "    ret = pyt_returns(state_next.rew, state_next.fin,\n",
    "                      gamma=gamma, bootstrap=torch.tensor(0.))\n",
    "\n",
    "    # Assume .act is unstructured: `act[t]` = a_{t+1} -->> T x B x 1\n",
    "    act = state_next.act.unsqueeze(-1)\n",
    "\n",
    "    # \\pi is the target policy, \\mu is the behaviour policy\n",
    "    log_pi, log_mu = info['logits'], batch.actor['logits']\n",
    "\n",
    "    # the importance weights\n",
    "    log_pi_a = log_pi.gather(-1, act).squeeze(-1)\n",
    "    log_mu_a = log_mu.gather(-1, act).squeeze(-1)\n",
    "    rho = log_mu_a.sub_(log_pi_a.detach())\\\n",
    "                  .neg_().exp_().clamp_(max=c_rho)\n",
    "\n",
    "    # the policy surrogate score\n",
    "    #    \\frac1T \\sum_t \\rho_t (G_t - b_t) \\log \\pi(a_t \\mid s_t)\n",
    "    reinfscore = log_pi_a.mul(ret.sub(ret.mean(dim=0)).mul_(rho)).mean()\n",
    "\n",
    "    # the policy entropy score (neg entropy)\n",
    "    #   - H(\\pi(•\\mid s)) = - (-1) \\sum_a \\pi(a\\mid s) \\log \\pi(a\\mid s)\n",
    "    f_min = torch.finfo(log_pi.dtype).min\n",
    "    negentropy = log_pi.exp().mul(log_pi.clamp(min=f_min)).sum(dim=-1).mean()\n",
    "\n",
    "    # maximize the entropy and the reinforce score\n",
    "    # \\ell := - \\frac1T \\sum_t G_t \\log \\pi(a_t \\mid s_t)\n",
    "    #         - C \\mathbb{H} \\pi(\\cdot \\mid s_t)\n",
    "    loss = C_entropy * negentropy - reinfscore\n",
    "    return loss.mean(), dict(entropy=-float(negentropy),\n",
    "                             policy_score=float(reinfscore),)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea00601",
   "metadata": {},
   "source": [
    "Actor-critic algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496e626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def a2c(batch, module, *, gamma=0.99, C_entropy=1e-2, C_value=0.25, c_rho=1.0):\n",
    "    r\"\"\"The Advantage Actor-Critic algorithm (importance-weighted off-policy).\n",
    "\n",
    "    Close to REINFORCE, but uses spearate baseline estimate to compute\n",
    "    advantages in the policy grad.\n",
    "    $$\n",
    "        \\nabla_\\theta J(s_t)\n",
    "            = \\mathbb{E}_{a \\sim \\beta(a\\mid s_t)}\n",
    "                \\frac{\\pi(a\\mid s_t)}{\\beta(a\\mid s_t)}\n",
    "                    \\bigl( r_{t+1} + \\gamma G_{t+1} - v(s_t) \\bigr)\n",
    "                \\nabla_\\theta \\log \\pi(a\\mid s_t)\n",
    "        \\,, $$\n",
    "    where the critic estimates the value function under the current policy\n",
    "    $\n",
    "    v(s_t) \\approx \\mathbb{E}_{\\pi_{\\geq t}}\n",
    "                    G_t(a_t, s_{t+1}, a_{t+1}, ... \\mid s_t)\n",
    "    $.\n",
    "    \"\"\"\n",
    "    # XXX `state[t]` = (x_t, a_{t-1}, r_t, d_t), t=0..T-1\n",
    "    state = suply(getitem, batch.state, index=slice(None, -1))\n",
    "\n",
    "    # XXX `state_next[t]` = (x_{t+1}, a_t, r_{t+1}, d_{t+1}), t=0..T-1\n",
    "    state_next = suply(getitem, batch.state, index=slice(1, None))\n",
    "\n",
    "    # REACT: (state[t], h_t) \\to (\\hat{a}_t, h_{t+1}, \\hat{A}_{t+1})\n",
    "    _, _, value, info = module(\n",
    "        state.obs, state.act, state.rew, state.fin,\n",
    "        hx=batch.hx, stepno=state.stepno)\n",
    "    # value = V(`.state[t]`)\n",
    "    #       <<-->> v(x_t)\n",
    "    #       \\approx \\mathbb{E}( G_t \\mid x_t)\n",
    "    #       \\approx \\mathbb{E}( r_{t+1} + \\gamma r_{t+2} + ... \\mid x_t)\n",
    "    #       <<-->> npv(`.state[t+1:]`)\n",
    "    # info['logits'] = \\log \\pi(... | .state[t] ) <<-->> \\log \\pi( \\cdot \\mid x_t)\n",
    "\n",
    "    # Future rewards following `.state[t]` are recorded in `.state[t+1:]`\n",
    "    # ret[t] = rew[t] + gamma * (1 - fin[t]) * (ret[t+1] or bootstrap)\n",
    "    #     `bootstrap` <<-->> `.value[-1]` = V(`.state[-1]`)\n",
    "    ret = pyt_returns(state_next.rew, state_next.fin,\n",
    "                      gamma=gamma, bootstrap=batch.value[-1])\n",
    "    # XXX post-mul by `1 - \\gamma` fails to train, but seems appropriate\n",
    "    # for the continuation/survival interpretation of the discount factor.\n",
    "    #   <<-- but who says this is a good interpretation?\n",
    "    # ret.mul_(1 - gamma)\n",
    "\n",
    "    # Assume `.act` is unstructured: `act[t]` = a_{t+1} -->> T x B x 1\n",
    "    act = state_next.act.unsqueeze(-1)\n",
    "\n",
    "    # \\pi is the target policy, \\mu is the behaviour policy\n",
    "    log_pi, log_mu = info['logits'], batch.actor['logits']\n",
    "\n",
    "    # the importance weights\n",
    "    log_pi_a = log_pi.gather(-1, act).squeeze(-1)\n",
    "    log_mu_a = log_mu.gather(-1, act).squeeze(-1)\n",
    "    rho = log_mu_a.sub_(log_pi_a.detach())\\\n",
    "                  .neg_().exp_().clamp_(max=c_rho)\n",
    "\n",
    "    # the critic's score (negative mse)\n",
    "    #  \\frac1{2 T} \\sum_t (G_t - v(s_t))^2\n",
    "    critic_mse = 0.5 * F.mse_loss(value, ret, reduction='mean')\n",
    "    # v(x_t) \\approx \\mathbb{E}( G_t \\mid x_t )\n",
    "    #        \\approx G_t (one-point estimate)\n",
    "    #        <<-->> ret[t]\n",
    "\n",
    "    # the policy surrogate score\n",
    "    #    \\frac1T \\sum_t \\rho_t (G_t - v_t) \\log \\pi(a_t \\mid s_t)\n",
    "    a2c_score = log_pi_a.mul(ret.sub(value.detach()).mul_(rho)).mean()\n",
    "\n",
    "    # the policy entropy score (neg entropy)\n",
    "    #   - H(\\pi(•\\mid s)) = - (-1) \\sum_a \\pi(a\\mid s) \\log \\pi(a\\mid s)\n",
    "    f_min = torch.finfo(log_pi.dtype).min\n",
    "    negentropy = log_pi.exp().mul(log_pi.clamp(min=f_min)).sum(dim=-1).mean()\n",
    "\n",
    "    # maximize the entropy and the reinforce score, minimize the critic loss\n",
    "    objective = C_entropy * negentropy + C_value * critic_mse - a2c_score\n",
    "    return objective.mean(), dict(entropy=-float(negentropy),\n",
    "                                  policy_score=float(a2c_score),\n",
    "                                  value_loss=float(critic_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc901b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyt_gae(batch.state.rew, batch.state.fin, batch.actor['value'], gamma=0.99, bootstrap=batch.bootstrap[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a69bde",
   "metadata": {},
   "source": [
    "The policy of the actor"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4be76f2c",
   "metadata": {},
   "source": [
    "def policy():\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(4, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, 2),\n",
    "        torch.nn.LogSoftmax(dim=-1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc56aca",
   "metadata": {},
   "source": [
    "A more sophisticated recurrent learner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7582454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "\n",
    "class CartPoleActor(BaseActorModule):\n",
    "    def __init__(self, epsilon=0.1, lstm=False):\n",
    "        super().__init__()\n",
    "        self.epsilon, self.lstm = epsilon, lstm\n",
    "\n",
    "        self.features = torch.nn.ModuleDict(dict(\n",
    "            obs=torch.nn.Sequential(\n",
    "                torch.nn.Linear(4, 64),\n",
    "                torch.nn.ReLU(),\n",
    "            ),\n",
    "            act=torch.nn.Embedding(2, 4),\n",
    "            rew=torch.nn.Sequential(\n",
    "                torch.nn.Linear(1, 4),\n",
    "                torch.nn.ReLU(),\n",
    "            ),\n",
    "        ))\n",
    "\n",
    "        n_features = 64 + 4 + 4\n",
    "        if not self.lstm:\n",
    "            self.core = torch.nn.Sequential(\n",
    "                torch.nn.Linear(n_features, 64),\n",
    "                torch.nn.ReLU(),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.core = torch.nn.GRU(n_features, 64, 1)\n",
    "\n",
    "        self.policy = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64, 2),\n",
    "            torch.nn.LogSoftmax(dim=-1),\n",
    "        )\n",
    "\n",
    "        self.baseline = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, act, rew, fin, *, hx=None, stepno=None):\n",
    "        # Everything is  [T x B x ...]\n",
    "        inputs = torch.cat([\n",
    "            self.features['obs'](obs),\n",
    "            self.features['act'](act),\n",
    "            self.features['rew'](rew.unsqueeze(-1)),\n",
    "        ], dim=-1)\n",
    "        \n",
    "        if not self.lstm:\n",
    "            output, hx = self.core(inputs), ()\n",
    "\n",
    "        elif False:\n",
    "            # sequence padding (MUST have sampling with `sticky=True`)\n",
    "            n_steps, n_env, *_ = fin.shape\n",
    "            if n_steps > 1:\n",
    "                # we assume sticky=True\n",
    "                lengths = 1 + (~fin[1:]).sum(0).cpu()\n",
    "                input = pack_padded_sequence(input, lengths, enforce_sorted=False)\n",
    "\n",
    "            output, hx = self.core(input, hx)\n",
    "            if n_steps > 1:\n",
    "                output, lens = pad_packed_sequence(\n",
    "                    output, batch_first=False, total_length=n_steps)\n",
    "\n",
    "        else:\n",
    "            # inputs is T x B x F, hx is either None, or a proper recurrent state\n",
    "            outputs = []\n",
    "            # manually step through the RNN core\n",
    "            for input, mask in zip(inputs.unsqueeze(1), fin.unsqueeze(-1)):\n",
    "                # zero if f indicates reset: multiplying by zero stops grad\n",
    "                if hx is not None:\n",
    "                    # stop hx grads if `reset` (mul-by-zero)\n",
    "                    hx = suply(torch.Tensor.mul, hx, other=~mask)\n",
    "\n",
    "                output, hx = self.core(input, hx)\n",
    "                outputs.append(output)\n",
    "\n",
    "            output = torch.cat(outputs, dim=0)\n",
    "\n",
    "        # value must not have any trailing dims, i.e. T x B\n",
    "        logits = self.policy(output)\n",
    "        value = self.baseline(output).squeeze(-1)\n",
    "\n",
    "        # XXX eps-greedy?\n",
    "        if self.training:\n",
    "            unif = torch.tensor(1. / logits.shape[-1])\n",
    "\n",
    "            prob = logits.detach().exp()\n",
    "            prob.mul_(1 - self.epsilon)\n",
    "            prob.add_(unif, alpha=self.epsilon)\n",
    "\n",
    "            actions = multinomial(prob)\n",
    "\n",
    "        else:\n",
    "            actions = logits.argmax(dim=-1)\n",
    "\n",
    "        return actions, hx, value, dict(\n",
    "            logits=logits,\n",
    "            # entropy\n",
    "            # logit_at_the_chosen_action\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c86fc",
   "metadata": {},
   "source": [
    "Initialize the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner, sticky = nonRecurrentPolicyWrapper(policy()), False\n",
    "learner = CartPoleActor(lstm=False)\n",
    "sticky = False  # learner.lstm\n",
    "\n",
    "learner.train()\n",
    "device_ = torch.device('cpu')  # torch.device('cuda:0')\n",
    "learner.to(device=device_)\n",
    "\n",
    "# prepare the optimizer for the learner\n",
    "optim = torch.optim.Adam(learner.parameters(), lr=1e-3, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62100a1c",
   "metadata": {},
   "source": [
    "Load a better trained agent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15cdce6b",
   "metadata": {},
   "source": [
    "filename = '/Users/ivannazarov/Github/repos_with_rl/rlplay/stage/model.pk'\n",
    "learner.policy.load_state_dict(torch.load(filename)['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77720ec9",
   "metadata": {},
   "source": [
    "Initialize the sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e89c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T, B = 120, 20\n",
    "# T, B = 120, 4\n",
    "T, B = 21, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef244d8",
   "metadata": {},
   "source": [
    "Pick one collector\n",
    "* NetHack environment `nle`, does not like `fork` method, so we should use `spawn`, which is not notebook friendly :(\n",
    "  * essentially it is better to prototype in notebook with `same.rollout`, then write a submodule non-interactive script with `multi.rollout`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7cfd27f5",
   "metadata": {},
   "source": [
    "# generator of rollout batches\n",
    "batchit = same.rollout(\n",
    "    [factory()],\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    sticky=sticky,\n",
    "    device=device_,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3dbb3ae2",
   "metadata": {},
   "source": [
    "# generator of rollout batches\n",
    "batchit = single.rollout(\n",
    "    factory,\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    n_envs=B,\n",
    "    sticky=sticky,  # so that we can leverage cudnn's fast RNN implementations\n",
    "    clone=False,\n",
    "    close=False,\n",
    "    device=device_,\n",
    "    start_method='fork',  # fork in notebook for macos, spawn in linux\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ef79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator of rollout batches\n",
    "batchit = multi.rollout(\n",
    "    factory,\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    n_actors=8,\n",
    "    n_per_actor=B,\n",
    "    n_buffers=16,\n",
    "    n_per_batch=2,\n",
    "    sticky=sticky,  # so that we can leverage cudnn's fast RNN implementations\n",
    "    pinned=False,\n",
    "    clone=False,\n",
    "    close=False,\n",
    "    device=device_,\n",
    "    start_method='fork',  # fork in notebook for macos, spawn in linux\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4873e7",
   "metadata": {},
   "source": [
    "Implement your favourite training method"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47fa0911",
   "metadata": {},
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0645166f",
   "metadata": {},
   "source": [
    "from rlplay.algo.returns import npy_returns\n",
    "rew, bv = next(test_it)\n",
    "ret = npy_returns(rew, numpy.zeros_like(rew, dtype=bool), gamma=gamma, bootstrap=bv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b2345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "gamma = 0.99\n",
    "losses, rewards = [], []\n",
    "\n",
    "# generator of evaluation rewards\n",
    "# test_it = test(factory, learner, n_envs=4, n_steps=500, device=device_)\n",
    "test_it = evaluate(factory, learner, n_envs=4, n_steps=500,\n",
    "                   clone=False, device=device_, start_method='fork')\n",
    "\n",
    "# the training loop\n",
    "exclude = {'returns'}\n",
    "ewm, alpha = None, 0.5\n",
    "for epoch in tqdm.tqdm(range(400)):\n",
    "    for j, batch in zip(range(100), batchit):\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss, info = a2c(batch, learner, gamma=gamma, c_rho=1.5)\n",
    "        loss.backward()\n",
    "        grad_norm = clip_grad_norm_(learner.parameters(), max_norm=1e2)\n",
    "        optim.step()\n",
    "\n",
    "        losses.append({\n",
    "            k: float(v) for k, v in info.items() if k not in exclude\n",
    "        })\n",
    "        losses[-1].update({'grad': float(grad_norm)})\n",
    "\n",
    "    rewards.append(next(test_it)[0])\n",
    "    \n",
    "    # track minimal reward\n",
    "    if ewm is None:\n",
    "        ewm = rewards[-1].min()\n",
    "    else:\n",
    "        ewm += alpha * (rewards[-1].min() - ewm)\n",
    "\n",
    "    if ewm > 498:\n",
    "        break\n",
    "\n",
    "# close the generators\n",
    "batchit.close()\n",
    "test_it.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4148ecf",
   "metadata": {},
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64d5469",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchit.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f2793",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62c89e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(records):\n",
    "    \"\"\"collate identically keyed dicts\"\"\"\n",
    "    out, n_records = {}, 0\n",
    "    for record in records:\n",
    "        for k, v in record.items():\n",
    "            out.setdefault(k, []).append(v)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c826c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k: numpy.array(v) for k, v in collate(losses).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f775bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'value_loss' in data:\n",
    "    plt.semilogy(data['value_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da88daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['entropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f676dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['policy_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb820f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(data['grad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be44afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = numpy.stack(rewards, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e3e9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = numpy.median(rewards, axis=-1), rewards.std(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06814f9f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fi, ax = plt.subplots(1, 1, figsize=(5, 3), dpi=300)\n",
    "\n",
    "ax.plot(numpy.mean(rewards, axis=-1))\n",
    "ax.plot(numpy.median(rewards, axis=-1))\n",
    "ax.plot(numpy.min(rewards, axis=-1))\n",
    "ax.plot(numpy.std(rewards, axis=-1))\n",
    "# ax.plot(m+s * 1.96)\n",
    "# ax.plot(m-s * 1.96)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d5f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with factory() as env:\n",
    "    learner.eval()\n",
    "    rewards, bootstrap = core_evaluate([\n",
    "        env\n",
    "    ], learner, render=True, n_steps=1e4, device=device_)\n",
    "\n",
    "print(sum(rewards), bootstrap[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.algo.returns import npy_returns\n",
    "plt.plot(\n",
    "    npy_returns(rewards, numpy.zeros_like(rewards, dtype=bool),\n",
    "                gamma=gamma, bootstrap=bootstrap[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "419e6c39",
   "metadata": {},
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f2ca5a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9df974",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ee9c25",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d40cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p_l, v_l, ent = zip(*losses)\n",
    "\n",
    "plt.plot(p_l)\n",
    "plt.plot(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9cc0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(v_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c5e05",
   "metadata": {},
   "source": [
    "Run in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f501cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([\n",
    "    sum(evaluate(factory, learner, render=False))\n",
    "    for _ in range(200)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe2dfa5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd2808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e3c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bar:\n",
    "    def __init__(self, parent):\n",
    "        self.parent = parent\n",
    "        self._range = range(self.parent.n)\n",
    "        self._it = iter(self._range)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self._it)\n",
    "\n",
    "class Foo:\n",
    "    def __init__(self, n=10):\n",
    "        self.n = n\n",
    "\n",
    "    def __iter__(self):\n",
    "        return Bar(self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22c935",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Foo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e2d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bar:\n",
    "    def __init__(self, parent):\n",
    "        self.parent = parent\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from range(self.parent.n)\n",
    "\n",
    "class Foo:\n",
    "    def __init__(self, n=10):\n",
    "        self.n = n\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(Bar(self))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a1a842",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
