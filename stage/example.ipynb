{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2be71d",
   "metadata": {},
   "source": [
    "# `rlplay`-ing around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b338efe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664b895d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4128b76d",
   "metadata": {},
   "source": [
    "## Rollout collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc520e9",
   "metadata": {},
   "source": [
    "Rollout collection is designed to be as much `plug-n-play` as possible, i.e. it supports\n",
    "arbitrarily structured nested containers of arrays or tensors for environment observations\n",
    "and actions.\n",
    "\n",
    "**Assumptions**\n",
    "* the environment communicates either in python scalars or in numpy arrays\n",
    "* the nested containers are built from `dicts`, `lists`, `tuple` or `namedtuples`\n",
    "\n",
    "In theory, there is no need for special data preporcessing, except for casting data to\n",
    "correct dtypes (like obs to `float32` in `CartPole`).\n",
    "\n",
    "The actor should also support certain API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a9efdc",
   "metadata": {},
   "source": [
    "### Creating the actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8d22c",
   "metadata": {},
   "source": [
    "#### Semantics\n",
    "\n",
    "The actor performs the following update\n",
    "$$\n",
    "    (x_t, a_{t-1}, r_t, d_t, h_t)\n",
    "        \\overset{\\mathrm{Actor}}{\\longrightarrow}\n",
    "        (a_t, h_{t+1})\n",
    "    \\,, $$\n",
    "\n",
    "and the environment -- \n",
    "$$\n",
    "    (s_t, a_t)\n",
    "        \\overset{\\mathrm{Env}}{\\longrightarrow}\n",
    "        (s_{t+1}, x_{t+1}, r_{t+1}, d_{t+1})\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a416b600",
   "metadata": {},
   "source": [
    "Rollout collection relies on the following API of the actor:\n",
    "* `.reset(j, hx)` reset the recurrent state of the j-th environment in the batch (if applicable)\n",
    "  * `hx` contains tensors with shape `n_lstm_layers x batch x hidden`, or is an empty tuple\n",
    "\n",
    "* `.step(obs, act, rew, fin, hx)` get the next action $a_t$, the recurrent state $h_{t+1}$, and\n",
    "the extra info in response to $x_t$, $a_{t-1}$, $r_t$, $d_t$, and $h_t$, respectively.\n",
    "  * extra info dict should include `value`\n",
    "  * MUST allocate new `hx` if the recurrent state is updated\n",
    "  * MUST NOT change the inputs in-place\n",
    "\n",
    "* `.value(obs, act, rew, fin, hx)` compute the value-function estimate $\n",
    "      v(s_t) \\approx G_t = \\mathbb{E} \\sum_{j\\geq t} \\gamma^{j-t} r_{j+1}\n",
    "  $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.base import BaseActorModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8abbcc9",
   "metadata": {},
   "source": [
    "All these methods call `.forward(obs, act, rew, fin, hx)`, which should return three things:\n",
    "1. `actions` prescribed actions in the environment, with data of shape `n_steps x batch x ...`\n",
    "2. `hx` data with shape `n_steps x batch x ...`\n",
    "3. `info` dict with extra `n_steps x batch x ...` data\n",
    "  * `value` -- the value function estimates\n",
    "  * `logits` -- the policy logits (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54221cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.utils.common import multinomial\n",
    "\n",
    "class nonRecurrentPolicyWrapper(BaseActorModule):\n",
    "    \"\"\"Example wrapper for a non-recurrent policy.\n",
    "    \n",
    "    Details\n",
    "    -------\n",
    "    This example assumes flat `Discrete(n)` action space, and\n",
    "    simple non-structured observation space, e.g. a python scalar\n",
    "    or a `numpy.array`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, policy, *, epsilon=0.1):\n",
    "        super().__init__()\n",
    "        self.policy, self.epsilon = policy, epsilon\n",
    "\n",
    "    def forward(self, obs, act=None, rew=None, fin=None, *, hx=None):\n",
    "        # Everything is  [T x B x ...]\n",
    "        logits, hx = self.policy(obs, act, rew), ()\n",
    "\n",
    "        # value must not have any trailing dims, i.e. T x B\n",
    "        value = logits.new_zeros(fin.shape)\n",
    "\n",
    "        # XXX eps-greedy?\n",
    "        if self.training:\n",
    "            unif = torch.tensor(1. / logits.shape[-1])\n",
    "\n",
    "            prob = logits.detach().exp()\n",
    "            prob.mul_(1 - self.epsilon)\n",
    "            prob.add_(unif, alpha=self.epsilon)\n",
    "\n",
    "            actions = multinomial(prob)\n",
    "\n",
    "        else:\n",
    "            actions = logits.argmax(dim=-1)\n",
    "\n",
    "        return actions, hx, dict(value=value, logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60432ab",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0a430a",
   "metadata": {},
   "source": [
    "### Rollout collection (same-process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c77045",
   "metadata": {},
   "source": [
    "Collect rollouts within the current process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce3815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout import same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536dca76",
   "metadata": {},
   "source": [
    "The parameters have the following meaning\n",
    "```python\n",
    "n_envs = 16     # the number of envs in the batch\n",
    "n_steps = 51    # the length of each rollout fragment\n",
    "sticky = False  # whether to stop interacting if an env resets mid-fragment\n",
    "device = None   # specifies the device to put the actor's inputs onto\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdfdfe0",
   "metadata": {},
   "source": [
    "`rollout()` returns an iterator, which does the following, roughly.\n",
    "\n",
    "Prepare the run-time context for the specified `actor` and the environments\n",
    "```python\n",
    "# spawn multiple envs\n",
    "envs = [env_factory() for _ in n_envs]\n",
    "\n",
    "# initialize a buffer for one rollout fragment (optionally pinned)\n",
    "buffer = prepare(envs[0], actor, n_steps, len(envs),\n",
    "                 pinned=pinned, device=device)\n",
    "\n",
    "# the running context tor the actor and the envs\n",
    "ctx, fragment = startup(envs, actor, buffer, pinned=pinned)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51374fb",
   "metadata": {},
   "source": [
    "Now within the infinite loop it does the following\n",
    "```python\n",
    "# collect the fragment\n",
    "collect(envs, actor, fragment, ctx, sticky=sticky, device=device)\n",
    "\n",
    "# fragment.pyt -- torch tensors, fragment.npy -- numpy arrays (aliased)\n",
    "# copy fragment.pyt onto `device`, and yield it to the user\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3bdd9f",
   "metadata": {},
   "source": [
    "The user has to manually limit the number of iterations using, for example,\n",
    "\n",
    "```python\n",
    "it = same.rollout(...)\n",
    "\n",
    "for b, batch in zip(range(100), it):\n",
    "    # train on batch\n",
    "    pass\n",
    "\n",
    "it.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d236b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733cc29c",
   "metadata": {},
   "source": [
    "### Rollout collection (single-process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f22ff",
   "metadata": {},
   "source": [
    "Single-actor rollout sampler running in a parallel process (double-buffered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559b8749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout import single"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c30a1d",
   "metadata": {},
   "source": [
    "Under the hood the functions creates **two** rollout fragment buffers, maintains\n",
    "a reference to the specified `actor`, makes a shared copy of it (on the host), and\n",
    "then spawns one worker process.\n",
    "\n",
    "The worker, in turn, makes its own local copy of the actor on the specified device,\n",
    "initializes the environments and the running context. During collection it altrenates\n",
    "between the buffers, into which it records the rollout fragments it collects. Except\n",
    "for double buffering, the logic is identical to `rollout`.\n",
    "\n",
    "The local copies of the actor are **automatically updated** from the maintained reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44520b9",
   "metadata": {},
   "source": [
    "```python\n",
    "it = single.rollout(\n",
    "    factory,              # the environment factory\n",
    "    actor,                # the actor reference, used to update the local actors\n",
    "\n",
    "    n_steps,              # the duration of a rollout fragment\n",
    "    n_envs,               # the number of independent environments in the batch\n",
    "\n",
    "    sticky=False,         # do we freeze terminated environments until the end of the rollout?\n",
    "                          #  required if we wish to leverage cudnn's fast RNN implementations,\n",
    "                          #  instead of manually stepping through the RNN core.\n",
    "\n",
    "    close=True,           # should we `.close()` the environments when cleaning up?\n",
    "                          #  some envs are very particular about this, e.g. nle\n",
    "\n",
    "    start_method='fork',  # `fork` in notebooks, `spawn` in linux/macos and if we interchange\n",
    "                          #  cuda tensors between processes (we DO NOT do that: we exchange indices\n",
    "                          #  to host-shapred tensors)\n",
    "\n",
    "    device=None,          # the device on which to collect rollouts (the local actor is moved\n",
    "                          #  onto this device)\n",
    ")\n",
    "\n",
    "# ...\n",
    "\n",
    "it.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438b37c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b54f70",
   "metadata": {},
   "source": [
    "### Rollout collection (multi-process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1512d3f",
   "metadata": {},
   "source": [
    "A more load-balanced multi-actor milti-process sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10fd074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout import multi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9195158",
   "metadata": {},
   "source": [
    "This version of the rollout collector allocates several buffers and spawns\n",
    "many parallel workers. Each worker creates it own local copy of the actor,\n",
    "instantiates `n_envs` local environments and allocates a running context for\n",
    "all of them. The rollout collection in each worker is **hardcoded to run on\n",
    "the host device**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ebe66",
   "metadata": {},
   "source": [
    "```python\n",
    "it = multi.rollout(\n",
    "    factory,              # the environment factory\n",
    "    actor,                # the actor reference, used to update the local actors\n",
    "\n",
    "    n_steps,              # the duration of each rollout fragment\n",
    "\n",
    "    n_actors,             # the number of parallel actors\n",
    "    n_per_actor,          # the number of independent environments run in each actor\n",
    "    n_buffers,            # the size of the pool of buffers, into which rollout\n",
    "                          #  fragments are collected. Should not be less than `n_actors`.\n",
    "    n_per_batch,          # the number of fragments collated into a batch\n",
    "\n",
    "    sticky=False,         # do we freeze terminated environments until the end of the rollout?\n",
    "                          #  required if we wish to leverage cudnn's fast RNN implementations,\n",
    "                          #  instead of manually stepping through the RNN core.\n",
    "\n",
    "    pinned=False,\n",
    "\n",
    "    close=True,           # should we `.close()` the environments when cleaning up?\n",
    "                          #  some envs are very particular about this, e.g. nle\n",
    "\n",
    "    device=None,          # the device onto which to move the rollout batches\n",
    "\n",
    "    start_method='fork',  # `fork` in notebooks, `spawn` in linux/macos and if we interchange\n",
    "                          #  cuda tensors between processes (we DO NOT do that: we exchange indices\n",
    "                          #  to host-shared tensors)\n",
    ")\n",
    "\n",
    "# ...\n",
    "\n",
    "it.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd045c3",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c414d5",
   "metadata": {},
   "source": [
    "### Evaluation (same-process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21598bc",
   "metadata": {},
   "source": [
    "In order to evaluate an actor in a batch of environments, one can use `evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc3da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.base import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357dec8d",
   "metadata": {},
   "source": [
    "The function *does not* collect the rollout data, except for the rewards.\n",
    "Below is the intended use case.\n",
    "* **NB** this is run in the same process, hence blocks until completion, which\n",
    "might take considerable time (esp. if `n_steps` is unbounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    factory, actor, n_envs=4,\n",
    "    *, n_steps=None, close=True, render=False, device=None\n",
    "):\n",
    "    # spawn a batch of environments\n",
    "    envs = [factory() for _ in range(n_envs)]\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            rewards = evaluate(envs, learner, n_steps=n_steps,\n",
    "                               render=render, device=device)\n",
    "\n",
    "            # get the accumulated rewards (gamma=1)\n",
    "            yield sum(rewards)\n",
    "\n",
    "    finally:\n",
    "        if close:\n",
    "            for e in envs:\n",
    "                e.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6d7855",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a3b8d3",
   "metadata": {},
   "source": [
    "## CartPole with REINFORCE or A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81cf24",
   "metadata": {},
   "source": [
    "The reinforce PG algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7587557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from rlplay.engine.returns import pyt_returns\n",
    "\n",
    "def reinforce(batch, module, *, gamma=0.99, C_entropy=1e-2):\n",
    "    # actor responses to `x_t`, `a_{t-1}`, `r_t`, `d_t` with `a_t`\n",
    "    #  (obs[t], act[t-1], rew[t-1], fin[t-1]) -->> act[t]\n",
    "    #  (o[1:-1], a[:-1], r[:-1], d[:-1]), a[1:]\n",
    "    _, _, info = module(\n",
    "        batch.state.obs[1:-1],\n",
    "        batch.state.act[:-1],\n",
    "        batch.state.rew[:-1],\n",
    "        batch.state.fin[:-1], hx=batch.hx)\n",
    "\n",
    "    # REINFORCE\n",
    "    # compute returns: G_t = r_{t+1} + \\gamma G_{t+1}, and\n",
    "    #  v(s_t) \\approx \\mathbb{E}_{\\pi_{\\geq t}} G_t(a_t, s_{t+1}, a_{t+1}, ... \\mid s_t).\n",
    "    # XXX GAE?\n",
    "    G_t = pyt_returns(batch.state.rew, batch.state.fin, gamma=gamma,\n",
    "                      bootstrap=batch.bootstrap[0])[1:]\n",
    "\n",
    "    # \\pi is the target policy, mu is the behaviour policy\n",
    "    logits, actions = info['logits'], batch.state.act[1:].unsqueeze(-1)\n",
    "    log_pi = logits.gather(-1, actions).squeeze(-1)\n",
    "\n",
    "    log_mu = batch.actor['logits'].gather(-1, actions).squeeze(-1)\n",
    "\n",
    "    # the importance weights\n",
    "    rho = log_mu.sub_(log_pi.detach()).neg_().exp_().clamp_(max=1)\n",
    "\n",
    "    # reinforce grads G_t \\nabla \\log \\pi(a_t\\mid s_t)\n",
    "    policy_score = log_pi.mul(G_t - G_t.mean(dim=0)).mul(rho).mean()\n",
    "\n",
    "    # maximize policy entropy:\n",
    "    #   H(\\pi(•\\mid s)) = - \\sum_a \\pi(a\\mid s) \\log \\pi(a\\mid s)\n",
    "    f_min = torch.finfo(logits.dtype).min\n",
    "    entropy = logits.exp().mul(logits.clamp(min=f_min))\\\n",
    "                    .sum(dim=-1).neg().mean()\n",
    "\n",
    "    # weighted sum of the policy score and entropy\n",
    "    # \\ell := - \\frac1T \\sum_t G_t \\log \\pi(a_t \\mid s_t)\n",
    "    #         + C \\mathbb{H} \\pi(\\cdot \\mid s_t)\n",
    "    objective = policy_score + C_entropy * entropy\n",
    "\n",
    "    # optimize: use inplace neg_ for maximization\n",
    "    return objective.neg().mean(), dict(returns=G_t, entropy=float(entropy),\n",
    "                                        policy_score=float(policy_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea00601",
   "metadata": {},
   "source": [
    "Actor-critic algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd81d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2c(batch, module, *, gamma=0.99, C_entropy=1e-2, C_value=0.25):\n",
    "    # actor responses to `x_t`, `a_{t-1}`, `r_t`, `d_t` with `a_t`\n",
    "    #  (obs[t], act[t-1], rew[t-1], fin[t-1]) -->> act[t]\n",
    "    #  (o[1:-1], a[:-1], r[:-1], d[:-1]), a[1:]\n",
    "    _, _, info = module(\n",
    "        batch.state.obs[1:-1],\n",
    "        batch.state.act[:-1],\n",
    "        batch.state.rew[:-1],\n",
    "        batch.state.fin[:-1], hx=batch.hx)\n",
    "\n",
    "    # Advantage Actor-Critic\n",
    "    G_t = pyt_returns(batch.state.rew, batch.state.fin,\n",
    "                      gamma=gamma, bootstrap=batch.bootstrap[0])[1:]\n",
    "\n",
    "    # \\pi is the target policy, mu is the behaviour policy\n",
    "    logits, actions = info['logits'], batch.state.act[1:].unsqueeze(-1)\n",
    "    log_pi = logits.gather(-1, actions).squeeze(-1)\n",
    "\n",
    "    log_mu = batch.actor['logits'].gather(-1, actions).squeeze(-1)\n",
    "\n",
    "    # the importance weights\n",
    "    rho = log_mu.sub_(log_pi.detach()).neg_().exp_().clamp_(max=1)\n",
    "\n",
    "    # compute the critics loss\n",
    "    #  \\frac1{NT} \\sum_{jt} (G_t(\\tau_j) - v(s_t(\\tau_j)))^2\n",
    "    adv = G_t - info['value']\n",
    "    value_score = F.mse_loss(info['value'], G_t, reduction='mean').neg()\n",
    "\n",
    "    # compute the policy loss\n",
    "    #  - \\frac1N\\sum_j (G_j - v(s_j)) \\log \\pi(a_j \\mid s_j)\n",
    "    policy_score = log_pi.mul(adv.detach()).mul(rho).mean()\n",
    "\n",
    "    # maximize policy entropy:\n",
    "    #   H(\\pi(•\\mid s)) = - \\sum_a \\pi(a\\mid s) \\log \\pi(a\\mid s)\n",
    "    f_min = torch.finfo(logits.dtype).min\n",
    "    entropy = logits.exp().mul(logits.clamp(min=f_min))\\\n",
    "                    .sum(dim=-1).neg().mean()\n",
    "\n",
    "    # weighted sum of the policy and value score and the entropy\n",
    "    objective = policy_score + C_entropy * entropy + C_value * value_score\n",
    "\n",
    "    # optimize: use inplace neg_ for maximization\n",
    "    return objective.neg().mean(), dict(\n",
    "        returns=G_t, value_score=float(value_score),\n",
    "        entropy=float(entropy), policy_score=float(policy_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a69bde",
   "metadata": {},
   "source": [
    "The policy of the actor"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4be76f2c",
   "metadata": {},
   "source": [
    "def policy():\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(4, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, 2),\n",
    "        torch.nn.LogSoftmax(dim=-1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3364e7",
   "metadata": {},
   "source": [
    "The environment factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d0177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "import rlplay.utils.integration.gym\n",
    "from rlplay.zoo.env import NarrowPath\n",
    "\n",
    "\n",
    "class FP32Observation(gym.ObservationWrapper):\n",
    "    def observation(self, observation):\n",
    "        return observation.astype(numpy.float32)\n",
    "\n",
    "class OneHotObservation(gym.ObservationWrapper):\n",
    "    def observation(self, observation):\n",
    "        return numpy.eye(1, self.env.observation_space.n,\n",
    "                         k=observation, dtype=numpy.float32)[0]\n",
    "\n",
    "def factory():\n",
    "    return FP32Observation(gym.make(\"CartPole-v0\").unwrapped)\n",
    "#     return gym.make(\"Taxi-v3\").unwrapped\n",
    "    # return OneHotObservation(NarrowPath())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc56aca",
   "metadata": {},
   "source": [
    "Initialize the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7582454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "\n",
    "class CartPoleActor(BaseActorModule):\n",
    "    def __init__(self, epsilon=0.1, lstm=False):\n",
    "        super().__init__()\n",
    "        self.epsilon, self.lstm = epsilon, lstm\n",
    "\n",
    "        self.features = torch.nn.ModuleDict(dict(\n",
    "            obs=torch.nn.Sequential(\n",
    "                torch.nn.Linear(4, 64),\n",
    "                torch.nn.ReLU(),\n",
    "            ),\n",
    "            act=torch.nn.Embedding(2, 4),\n",
    "            rew=torch.nn.Sequential(\n",
    "                torch.nn.Linear(1, 4),\n",
    "                torch.nn.ReLU(),\n",
    "            ),\n",
    "        ))\n",
    "\n",
    "        n_features = 64 + 4 + 4\n",
    "        if not self.lstm:\n",
    "            self.core = torch.nn.Sequential(\n",
    "                torch.nn.Linear(n_features, 64),\n",
    "                torch.nn.ReLU(),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.core = torch.nn.GRU(n_features, 64, 1)\n",
    "\n",
    "        self.policy = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64, 2),\n",
    "            torch.nn.LogSoftmax(dim=-1),\n",
    "        )\n",
    "\n",
    "        self.baseline = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, act, rew, fin, *, hx=None):\n",
    "        # Everything is  [T x B x ...]\n",
    "        input = torch.cat([\n",
    "            self.features['obs'](obs),\n",
    "            self.features['act'](act),\n",
    "            self.features['rew'](rew.unsqueeze(-1)),\n",
    "        ], dim=-1)\n",
    "        \n",
    "        if not self.lstm:\n",
    "            output, hx = self.core(input), ()\n",
    "\n",
    "        else:\n",
    "            # sequence padding (MUST have sampling with `sticky=True`)\n",
    "            n_steps, n_env, *_ = fin.shape\n",
    "            if n_steps > 1:\n",
    "                # we assume sticky=True\n",
    "                lengths = 1 + (~fin[1:]).sum(0)\n",
    "                input = pack_padded_sequence(input, lengths, enforce_sorted=False)\n",
    "\n",
    "            output, hx = self.core(input, hx)\n",
    "            if n_steps > 1:\n",
    "                output, lens = pad_packed_sequence(\n",
    "                    output, batch_first=False, total_length=n_steps)\n",
    "\n",
    "        # value must not have any trailing dims, i.e. T x B\n",
    "        logits = self.policy(output)\n",
    "        value = self.baseline(output).squeeze(-1)\n",
    "\n",
    "        # XXX eps-greedy?\n",
    "        if self.training:\n",
    "            unif = torch.tensor(1. / logits.shape[-1])\n",
    "\n",
    "            prob = logits.detach().exp()\n",
    "            prob.mul_(1 - self.epsilon)\n",
    "            prob.add_(unif, alpha=self.epsilon)\n",
    "\n",
    "            actions = multinomial(prob)\n",
    "\n",
    "        else:\n",
    "            actions = logits.argmax(dim=-1)\n",
    "\n",
    "        return actions, hx, dict(value=value, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a75143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyt_gae(batch.state.rew, batch.state.fin, batch.actor['value'], gamma=0.99, bootstrap=batch.bootstrap[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner, sticky = nonRecurrentPolicyWrapper(policy()), False\n",
    "learner = CartPoleActor(lstm=False)\n",
    "sticky = learner.lstm\n",
    "\n",
    "learner.train()\n",
    "device_ = torch.device('cpu')  # torch.device('cuda:0')\n",
    "learner.to(device=device_)\n",
    "\n",
    "# prepare the optimizer for the learner\n",
    "optim = torch.optim.Adam(learner.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da8da3f",
   "metadata": {},
   "source": [
    "Handy procedure to evaluate the actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8467546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import rlplay.utils.integration.gym  # hotfix for gym's poor viz (spawns gl threads!)\n",
    "\n",
    "from rlplay.engine.base import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62100a1c",
   "metadata": {},
   "source": [
    "Load a better trained agent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15cdce6b",
   "metadata": {},
   "source": [
    "filename = '/Users/ivannazarov/Github/repos_with_rl/rlplay/stage/model.pk'\n",
    "learner.policy.load_state_dict(torch.load(filename)['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77720ec9",
   "metadata": {},
   "source": [
    "Initialize the sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e89c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# T, B = 120, 20\n",
    "# T, B = 120, 4\n",
    "T, B = 51, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef244d8",
   "metadata": {},
   "source": [
    "Pick one collector"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8b3852a",
   "metadata": {},
   "source": [
    "# generator of rollout batches\n",
    "batchit = same.rollout(\n",
    "    [factory()],\n",
    "    learner,\n",
    "    n_steps=n_steps,\n",
    "    sticky=sticky,\n",
    "    device=device_,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dd3c9808",
   "metadata": {},
   "source": [
    "# generator of rollout batches\n",
    "batchit = single.rollout(\n",
    "    factory,\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    n_envs=B,\n",
    "    sticky=sticky,  # so that we can leverage cudnn's fast RNN implementations\n",
    "    close=False,\n",
    "    device=device_,\n",
    "    start_method='fork',  # fork in notebook for macos, spawn in linux\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82311b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator of rollout batches\n",
    "batchit = multi.rollout(\n",
    "    factory,\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    n_actors=8,\n",
    "    n_per_actor=B,\n",
    "    n_buffers=16,\n",
    "    n_per_batch=2,\n",
    "    sticky=sticky,  # so that we can leverage cudnn's fast RNN implementations\n",
    "    pinned=False,\n",
    "    close=False,\n",
    "    device=device_,\n",
    "    start_method='fork',  # fork in notebook for macos, spawn in linux\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4873e7",
   "metadata": {},
   "source": [
    "Implement your favourite training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b2345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "gamma = 0.99\n",
    "losses, rewards = [], []\n",
    "\n",
    "# generator of evaluation rewards\n",
    "test_it = test(factory, learner, n_envs=4, n_steps=500)\n",
    "\n",
    "# the training loop\n",
    "for epoch in tqdm.tqdm(range(100)):\n",
    "    for j, batch in zip(range(100), batchit):\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss, info = a2c(batch, learner, gamma=gamma)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        losses.append(tuple(map(\n",
    "            float, (info['policy_score'], info['entropy'])\n",
    "        )))\n",
    "\n",
    "    rewards.append(next(test_it))\n",
    "\n",
    "    if rewards[-1].min() > 900:\n",
    "        break\n",
    "\n",
    "# close the generators\n",
    "batchit.close()\n",
    "test_it.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4148ecf",
   "metadata": {},
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f2793",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f676dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be44afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = numpy.stack(rewards, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e3e9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = numpy.median(rewards, axis=-1), rewards.std(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06814f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(numpy.mean(rewards, axis=-1))\n",
    "plt.plot(numpy.median(rewards, axis=-1))\n",
    "plt.plot(numpy.min(rewards, axis=-1))\n",
    "plt.plot(numpy.std(rewards, axis=-1))\n",
    "# plt.plot(m+s * 1.96)\n",
    "# plt.plot(m-s * 1.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d5f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with factory() as env:\n",
    "    learner.eval()\n",
    "    print(sum(evaluate([\n",
    "        env\n",
    "    ], learner, render=True)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "419e6c39",
   "metadata": {},
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f2ca5a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9df974",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ee9c25",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcadbfc5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f27edb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf2b2c44",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d40cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p_l, v_l, ent = zip(*losses)\n",
    "\n",
    "plt.plot(p_l)\n",
    "plt.plot(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9cc0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(v_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c5e05",
   "metadata": {},
   "source": [
    "Run in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f501cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([\n",
    "    sum(evaluate(factory, learner, render=False))\n",
    "    for _ in range(200)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe2dfa5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd2808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e3c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bar:\n",
    "    def __init__(self, parent):\n",
    "        self.parent = parent\n",
    "        self._range = range(self.parent.n)\n",
    "        self._it = iter(self._range)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self._it)\n",
    "\n",
    "class Foo:\n",
    "    def __init__(self, n=10):\n",
    "        self.n = n\n",
    "\n",
    "    def __iter__(self):\n",
    "        return Bar(self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22c935",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Foo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e2d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bar:\n",
    "    def __init__(self, parent):\n",
    "        self.parent = parent\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from range(self.parent.n)\n",
    "\n",
    "class Foo:\n",
    "    def __init__(self, n=10):\n",
    "        self.n = n\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(Bar(self))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a1a842",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
