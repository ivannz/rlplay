{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "830df33b",
   "metadata": {},
   "source": [
    "# `rlplay`-ing around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718fe01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f6bf1",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb18c007",
   "metadata": {},
   "source": [
    "## Rollout collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc6e6c8",
   "metadata": {},
   "source": [
    "Rollout collection is designed to be as much `plug-n-play` as possible, i.e. it\n",
    "supports **arbitrarily structured nested containers** of arrays or tensors for\n",
    "environment observations and actions. The actor, however, should **expose**\n",
    "certain API (described below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec2a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine import collect  # the collector's core\n",
    "\n",
    "# print(collect.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f7cdf2",
   "metadata": {},
   "source": [
    "It's role is to serve as a *middle-man* between the **actor-environment** pair\n",
    "and the **training loop**: to track the trajectory of the actor in the environment,\n",
    "and properly record it into the data buffer.\n",
    "\n",
    "For example, it is not responsible for seeding or randomization of environments\n",
    "(i'm looking at you, `AtariEnv`), and datatype casting (except for rewards,\n",
    "which are cast to `fp32` automatically). In theory, there is **no need** for\n",
    "special data preporcessing, except for, perhaps, casting data to proper dtypes,\n",
    "like from `numpy.float64` observations to `float32` in `CartPole`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e813f733",
   "metadata": {},
   "source": [
    "#### Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb5c99",
   "metadata": {},
   "source": [
    "The collector just carefully records the trajectory by alternating between\n",
    "the **REACT** and **STEP+EMIT** phases in the following fashion:\n",
    "\n",
    "$$\n",
    "    \\cdots\n",
    "        \\longrightarrow t\n",
    "        \\overset{\\mathrm{REACT}}{\\longrightarrow} t + \\tfrac12\n",
    "        \\overset{\\mathrm{STEP+EMIT}}{\\longrightarrow} t + 1\n",
    "        \\longrightarrow \\cdots\n",
    "    \\,, $$\n",
    "\n",
    "where the half-times $t + \\tfrac12$ are commonly reffered to as the `afterstates`:\n",
    "the actor has chosen an action in response to the current observation, yet has\n",
    "not interacted with the environment.\n",
    "\n",
    "So the `time` advances in halves, and the proper names for the half times\n",
    "in the diagram above are the `state`, the `afterstate` and the `next state`,\n",
    "respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fdf5ec",
   "metadata": {},
   "source": [
    "The collected `fragment` data has the following structure:\n",
    "* `.state` $z_t$ **the current \"extended\" observation**\n",
    "  * `.stepno` $n_t$ the step counter\n",
    "  * `.obs` $x_t$ **the current observation** emitted by transitioning to $s_t$\n",
    "  * `.act` $a_{t-1}$ **the last action** which caused $s_{t-1} \\longrightarrow s_t$\n",
    "  * `.rew` $r_t$ **the previous reward** received by getting to $s_t$\n",
    "  * `.fin` $d_t$ **the termination flag** indicating if $s_t$ is terminal in the env\n",
    "\n",
    "* `.actor` $A_t$ auxiliary data from the actor due to **REACT**\n",
    "\n",
    "* `.env` $E_{t+1}$ auxiliary data from the environment due to **STEP+EMIT**\n",
    "\n",
    "* `.hx` $h_0$ the starting recurrent state of the actor\n",
    "\n",
    "Here $s_t$ denotes **the unobserved true full state** of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482a59a",
   "metadata": {},
   "source": [
    "The actor $\\theta$ interacts with the environment and generates the following\n",
    "<span style=\"color:orange\">**tracked**</span> data during the rollout,\n",
    "unobserved/non-tracked data <span style=\"color:red\">**in red**</span> and\n",
    "$t = 0..T-1$:\n",
    "\n",
    "*  ${\\color{orange}{h_0}}$, the starting recurrent state, is recorded in $\\,.\\!\\mathtt{hx}$\n",
    "\n",
    "* **REACT**: the actor performs the following update ($t \\to t + \\frac12$)\n",
    "\n",
    "$$ \n",
    "    \\bigl(\n",
    "        \\underbrace{\n",
    "            .\\!\\mathtt{state}[\\mathtt{t}]\n",
    "        }_{{\\color{orange}{z_t}}},\\,\n",
    "        {\\color{red}{h_t}}\n",
    "    \\bigr)\n",
    "        \\overset{\\text{Actor}_{\\theta_{\\text{old}}}}{\\longrightarrow}\n",
    "        \\bigl(\n",
    "            \\underbrace{\n",
    "                .\\!\\mathtt{state}.\\!\\mathtt{act}[\\mathtt{t+1}]\n",
    "            }_{a_t \\leadsto {\\color{orange}{z_{t+1}}}},\\,\n",
    "            \\underbrace{\n",
    "                .\\!\\mathtt{actor}[\\mathtt{t}]\n",
    "            }_{{\\color{orange}{A_t}}},\\,\n",
    "            {\\color{red}{h_{t+1}}}\n",
    "        \\bigr)\n",
    "    \\,, $$\n",
    "\n",
    "* **STEP+EMIT**: the environment updates it's unobserved state and emits\n",
    "the observed data ($t + \\frac12 \\to t+1_-$)\n",
    "\n",
    "$$ \n",
    "    \\bigl(\n",
    "        {\\color{red}{s_t}},\\,\n",
    "        \\underbrace{\n",
    "            .\\!\\mathtt{state}.\\!\\mathtt{act}[\\mathtt{t+1}]\n",
    "        }_{a_t \\leadsto {\\color{orange}{z_{t+1}}}}\n",
    "    \\bigr)\n",
    "        \\overset{\\text{Env}}{\\longrightarrow}\n",
    "        \\bigl(\n",
    "            {\\color{red}{s_{t+1}}},\\,\n",
    "            \\underbrace{\n",
    "                .\\!\\mathtt{state}.\\!\\mathtt{obs}[\\mathtt{t+1}]\n",
    "            }_{x_{t+1} \\leadsto {\\color{orange}{z_{t+1}}}},\\,\n",
    "            \\underbrace{\n",
    "                .\\!\\mathtt{state}.\\!\\mathtt{rew}[\\mathtt{t+1}]\n",
    "            }_{r_{t+1} \\leadsto {\\color{orange}{z_{t+1}}}},\\,\n",
    "            \\underbrace{\n",
    "                .\\!\\mathtt{state}.\\!\\mathtt{fin}[\\mathtt{t+1}]\n",
    "            }_{d_{t+1} \\leadsto {\\color{orange}{z_{t+1}}}},\\,\n",
    "            \\underbrace{\n",
    "                .\\!\\mathtt{env}[\\mathtt{t}]\n",
    "            }_{{\\color{orange}{E_{t+1}}}}\n",
    "        \\bigr)\n",
    "    \\,, $$\n",
    "\n",
    "* collect loop ($t + 1_- \\to t+1$)\n",
    "\n",
    "$$ \n",
    "    \\bigl(\n",
    "        {\\color{orange}{n_t}},\\,\n",
    "        {\\color{orange}{d_{t+1}}}\n",
    "    \\bigr)\n",
    "        \\longrightarrow\n",
    "            \\underbrace{\n",
    "                .\\!\\mathtt{state}.\\!\\mathtt{stepno}[\\mathtt{t+1}]\n",
    "            }_{n_{t+1} \\leadsto {\\color{orange}{z_{t+1}}}}\n",
    "    \\,. $$\n",
    "\n",
    "Here $r_t$ is a scalar reward, $d_t = \\top$ if $s_t$ is terminal, or $\\bot$\n",
    "otherwise, $n_{t+1} = 0$ if $d_t = \\top$, else $1 + n_t$, and $a \\leadsto b$\n",
    "means $a$ being recored into $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab719f2",
   "metadata": {},
   "source": [
    "In general, we may treat $z_t$, the extended observation, as an ordinary\n",
    "observation, by **suitably modifying** the environment: we can make it\n",
    "recall the most recent action $a_{t-1}$ and compute the termination indicator\n",
    "$d_t$ of the current state, and let it keep track of the interaction counter\n",
    "$n_t$, and, finally, we can configure it to supply the most recent reward\n",
    "$r_t$ as part of the emitted observation.\n",
    "\n",
    "Hence we essentially consider the following POMDP setup:\n",
    "\\begin{align}\n",
    "    a_t, h_{t+1}, A_t\n",
    "        &\\longleftarrow \\operatorname{Actor}(z_t, h_t; \\theta)\n",
    "        \\,, \\\\\n",
    "    z_{t+1}, r_{t+1}, E_{t+1}, s_{t+1}\n",
    "        &\\longleftarrow \\operatorname{Env}(s_t, a_t)\n",
    "        \\,, \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47754ebb",
   "metadata": {},
   "source": [
    "Specifically, let $\n",
    "(z_t)_{t=0}^T\n",
    "    = (n_t, x_t, a_{t-1}, r_t, d_t)_{t=0}^T\n",
    "$ be the trajectory fragment in `.state`, and $h_0$, `.hx`, be the starting\n",
    "(not necessarily the initial) recurrent state of the actor at the begining\n",
    "of the rollout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba5f78",
   "metadata": {},
   "source": [
    "##### Requirements\n",
    "\n",
    "* all nested containers **must be** built from pure python `dicts`, `lists`, `tuples` or `namedtuples`\n",
    "\n",
    "* the environment communicates either in **numpy arrays** or in python **scalars**, but not in data types that are incompatible with pytorch (such as `str` or `bytes`)\n",
    "\n",
    "```python\n",
    "# example\n",
    "obs = {\n",
    "    'camera': {\n",
    "        'rear': numpy.zeros(3, 320, 240),\n",
    "        'front': numpy.zeros(3, 320, 240),\n",
    "    },\n",
    "    'proximity': (+0.1, +0.2, -0.1, +0.0,),\n",
    "    'other': {\n",
    "        'fuel_tank': 78.5,\n",
    "        'passenger': False,\n",
    "    },\n",
    "}\n",
    "```\n",
    "\n",
    "* the actor communicates in torch tensors **only**\n",
    "\n",
    "* the environment produces **float scalar** rewards (other data may be communicated through auxiliary environment info-dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a6c004",
   "metadata": {},
   "source": [
    "### Creating the actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b2bc86",
   "metadata": {},
   "source": [
    "Rollout collection relies on the following API of the actor:\n",
    "* `.reset(j, hx)` reset the recurrent state of the j-th environment in the batch (if applicable)\n",
    "  * `hx` contains tensors with shape `(n_lstm_layers * n_dir) x batch x hidden`, or is an empty tuple\n",
    "  * the returned `hx` is the updated recurrent state\n",
    "\n",
    "\n",
    "* `.step(stepno, obs, act, rew, fin, /, *, hx, virtual)` get the next action $a_t$, the recurrent state $h_{t+1}$, and\n",
    "the **extra info** in response to $x_t$, $a_{t-1}$, $r_t$, $d_t$, $h_t$, and $n_t$, respectively.\n",
    "  * extra info `dict` **might** include `value` key with a `T x B` tensor of state value estimates $\n",
    "      v(s_t) \\approx G_t = \\mathbb{E} \\sum_{j\\geq t} \\gamma^{j-t} r_{j+1}\n",
    "  $.\n",
    "  * MUST allocate new `hx` if the recurrent state is updated\n",
    "  * MUST NOT change the inputs in-place\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace79079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine import BaseActorModule\n",
    "\n",
    "# BaseActorModule.step??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dadef8",
   "metadata": {},
   "source": [
    "`BaseActorModule` is essentially a thin sub-class of `torch.nn.Module`, that implements\n",
    "the API through `.forward(obs, act, rew, fin, *, hx, stepno)`, which should return three things:\n",
    "\n",
    "1. `actions` prescribed actions in the environment, with data of shape `n_steps x batch x ...`\n",
    "  * can be a nested container of dicts, lists, and tuples\n",
    "\n",
    "\n",
    "2. `hx` data with shape `n_steps x batch x ...`\n",
    "  * can be a nested container of dicts, lists, and tuples\n",
    "  * **if an actor is not recurrent**, then must return an empty container, e.g. a tuple `()`\n",
    "\n",
    "\n",
    "3. `info` object, which might be a tensor or a nested object containing data in tensors\n",
    "`n_steps x batch x ...`. For example, one may communicate the following data:\n",
    "  * `value` -- the state value estimates $v(z_t)$\n",
    "  * `logits` -- the policy logits $\\log \\pi(\\cdot \\mid z_t)$\n",
    "  * `q` -- $Q(z_t, \\cdot)$ values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07459bd5",
   "metadata": {},
   "source": [
    "Here is an example actor, that wraps a simple MLP policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502647a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.utils.common import multinomial\n",
    "\n",
    "class nonRecurrentPolicyWrapper(BaseActorModule):\n",
    "    \"\"\"Example wrapper for a non-recurrent policy.\n",
    "    \n",
    "    Details\n",
    "    -------\n",
    "    This example assumes flat `Discrete(n)` action space, and\n",
    "    simple non-structured observation space, e.g. a python scalar\n",
    "    or a `numpy.array`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, policy, *, epsilon=0.1):\n",
    "        super().__init__()\n",
    "        self.policy, self.epsilon = policy, epsilon\n",
    "\n",
    "    def forward(self, obs, act=None, rew=None, fin=None,\n",
    "                *, hx=None, stepno=None, virtual=False):\n",
    "        # Everything is  [T x B x ...]\n",
    "        logits, hx = self.policy(obs, act, rew), ()\n",
    "\n",
    "        # XXX eps-greedy?\n",
    "        if self.training:\n",
    "            unif = torch.tensor(1. / logits.shape[-1])\n",
    "\n",
    "            prob = logits.detach().exp()\n",
    "            prob.mul_(1 - self.epsilon)\n",
    "            prob.add_(unif, alpha=self.epsilon)\n",
    "\n",
    "            actions = multinomial(prob)\n",
    "\n",
    "        else:\n",
    "            actions = logits.argmax(dim=-1)\n",
    "\n",
    "        # return just policy logits, for example, for REINFORCE algo\n",
    "        return actions, hx, dict(logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988603bb",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4bd857",
   "metadata": {},
   "source": [
    "### Manual rollout collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043495dd",
   "metadata": {},
   "source": [
    "We shall need the following procedures from the core of the engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f6fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.core import prepare, startup, collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f39deda",
   "metadata": {},
   "source": [
    "Manual collection requires an `actor` and a batch of environment instances `envs`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b679a43a",
   "metadata": {},
   "source": [
    "Prepare the run-time context for the specified `actor` and the environments\n",
    "```python\n",
    "# settings\n",
    "sticky = False  # whether to stop interacting if an env resets mid-fragment\n",
    "device = None   # specifies the device to put the actor's inputs and data onto\n",
    "pinned = False  # whether to keep the running context in non-resizable pinned\n",
    "                #  (non-paged) memory for faster host-device transfers\n",
    "\n",
    "# initialize a buffer for one rollout fragment\n",
    "buffer = prepare(envs[0], actor, n_steps, len(envs),\n",
    "                 pinned=False, device=device)\n",
    "\n",
    "# the running context tor the actor and the envs (optionally pinned)\n",
    "ctx, fragment = startup(envs, actor, buffer, pinned=pinned)\n",
    "\n",
    "while not done:\n",
    "    # collect the fragment\n",
    "    collect(envs, actor, fragment, ctx, sticky=sticky, device=device)\n",
    "\n",
    "    # fragment.pyt -- torch tensors, fragment.npy -- numpy arrays (aliased on-host)\n",
    "    do_stuff(actor, fragment.pyt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c6c46",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc2cf1",
   "metadata": {},
   "source": [
    "### Rollout collection (same-process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a7d862",
   "metadata": {},
   "source": [
    "Collect rollouts within the current process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf70397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout import same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25301273",
   "metadata": {},
   "source": [
    "The parameters have the following meaning\n",
    "```python\n",
    "it = same.rollout(\n",
    "    envs,           # the batch of environment instances\n",
    "    actor,          # the actor which interacts with the batch\n",
    "    n_steps=51,     # the length of the rollout fragment\n",
    "    sticky=False,   # whether to stop interacting if an env resets mid-fragment\n",
    "    device=None,    # specifies the device to put the actor's inputs onto\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dbe569",
   "metadata": {},
   "source": [
    "`rollout()` returns an iterator, which has, roughly, the same logic,\n",
    "as the manual collection above.\n",
    "\n",
    "Inside the infinite loop it copies `fragment.pyt` onto `device`, before\n",
    "yielding it to the user. It also does not spawn its own batch of environments,\n",
    "unlike parallel variants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16edf5d1",
   "metadata": {},
   "source": [
    "The user has to manually limit the number of iterations using, for example,\n",
    "\n",
    "```python\n",
    "it = same.rollout(...)\n",
    "\n",
    "for b, batch in zip(range(100), it):\n",
    "    # train on batch\n",
    "    pass\n",
    "\n",
    "it.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7550f6e5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0d7e31",
   "metadata": {},
   "source": [
    "### Rollout collection (single-process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fdc822",
   "metadata": {},
   "source": [
    "Single-actor rollout sampler running in a parallel process (double-buffered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout import single"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2989ca0c",
   "metadata": {},
   "source": [
    "Under the hood the functions creates **two** rollout fragment buffers, maintains\n",
    "a reference to the specified `actor`, makes a shared copy of it (on the host), and\n",
    "then spawns one worker process.\n",
    "\n",
    "The worker, in turn, makes its own local copy of the actor on the specified device,\n",
    "initializes the environments and the running context. During collection it altrenates\n",
    "between the buffers, into which it records the rollout fragments it collects. Except\n",
    "for double buffering, the logic is identical to `rollout`.\n",
    "\n",
    "The local copies of the actor are **automatically updated** from the maintained reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563a31cf",
   "metadata": {},
   "source": [
    "```python\n",
    "it = single.rollout(\n",
    "    factory,              # the environment factory\n",
    "    actor,                # the actor reference, used to update the local actors\n",
    "\n",
    "    n_steps,              # the duration of a rollout fragment\n",
    "    n_envs,               # the number of independent environments in the batch\n",
    "\n",
    "    sticky=False,         # do we freeze terminated environments until the end of the rollout?\n",
    "                          #  required if we wish to leverage cudnn's fast RNN implementations,\n",
    "                          #  instead of manually stepping through the RNN core.\n",
    "\n",
    "    clone=True,           # should the worker use a local clone of the reference actor\n",
    "\n",
    "    close=True,           # should we `.close()` the environments when cleaning up?\n",
    "                          #  some envs are very particular about this, e.g. nle\n",
    "\n",
    "    start_method='fork',  # `fork` in notebooks, `spawn` in linux/macos and if we interchange\n",
    "                          #  cuda tensors between processes (we DO NOT do that: we exchange indices\n",
    "                          #  to host-shapred tensors)\n",
    "\n",
    "    device=None,          # the device on which to collect rollouts (the local actor is moved\n",
    "                          #  onto this device)\n",
    ")\n",
    "\n",
    "# ...\n",
    "\n",
    "it.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59902d9",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c807624e",
   "metadata": {},
   "source": [
    "### Rollout collection (multi-process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c8c18a",
   "metadata": {},
   "source": [
    "A more load-balanced multi-actor milti-process sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56b0957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout import multi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ed4f7d",
   "metadata": {},
   "source": [
    "This version of the rollout collector allocates several buffers and spawns\n",
    "many parallel workers. Each worker creates it own local copy of the actor,\n",
    "instantiates `n_envs` local environments and allocates a running context for\n",
    "all of them. The rollout collection in each worker is **hardcoded to run on\n",
    "the host device**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b475cdc",
   "metadata": {},
   "source": [
    "```python\n",
    "it = multi.rollout(\n",
    "    factory,              # the environment factory\n",
    "    actor,                # the actor reference, used to update the local actors\n",
    "\n",
    "    n_steps,              # the duration of each rollout fragment\n",
    "\n",
    "    n_actors,             # the number of parallel actors\n",
    "    n_per_actor,          # the number of independent environments run in each actor\n",
    "    n_buffers,            # the size of the pool of buffers, into which rollout\n",
    "                          #  fragments are collected. Should not be less than `n_actors`.\n",
    "    n_per_batch,          # the number of fragments collated into a batch\n",
    "\n",
    "    sticky=False,         # do we freeze terminated environments until the end of the rollout?\n",
    "                          #  required if we wish to leverage cudnn's fast RNN implementations,\n",
    "                          #  instead of manually stepping through the RNN core.\n",
    "\n",
    "    pinned=False,\n",
    "\n",
    "    clone=True,           # should the parallel actors use a local clone of the reference actor\n",
    "\n",
    "    close=True,           # should we `.close()` the environments when cleaning up?\n",
    "                          #  some envs are very particular about this, e.g. nle\n",
    "\n",
    "    device=None,          # the device onto which to move the rollout batches\n",
    "\n",
    "    start_method='fork',  # `fork` in notebooks, `spawn` in linux/macos and if we interchange\n",
    "                          #  cuda tensors between processes (we DO NOT do that: we exchange indices\n",
    "                          #  to host-shared tensors)\n",
    ")\n",
    "\n",
    "# ...\n",
    "\n",
    "it.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66981a6",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49ac75f",
   "metadata": {},
   "source": [
    "### Evaluation (same-process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fe5202",
   "metadata": {},
   "source": [
    "In order to evaluate an actor in a batch of environments, one can use `evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine import evaluate as core_evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee3a21c",
   "metadata": {},
   "source": [
    "The function *does not* collect the rollout data, except for the rewards.\n",
    "Below is the intended use case.\n",
    "* **NB** this is run in the same process, hence blocks until completion, which\n",
    "might take considerable time (esp. if `n_steps` is unbounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a700acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same process\n",
    "def same_evaluate(\n",
    "    factory, actor, n_envs=4,\n",
    "    *, n_steps=None, close=True, render=False, device=None\n",
    "):\n",
    "    # spawn a batch of environments\n",
    "    envs = [factory() for _ in range(n_envs)]\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            rewards, _ = core_evaluate(\n",
    "                envs, actor, n_steps=n_steps,\n",
    "                render=render, device=device)\n",
    "\n",
    "            # get the accumulated rewards (gamma=1)\n",
    "            yield sum(rewards)\n",
    "\n",
    "    finally:\n",
    "        if close:\n",
    "            for e in envs:\n",
    "                e.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a0011a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce457ae0",
   "metadata": {},
   "source": [
    "### Evaluation (parallel process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0299259",
   "metadata": {},
   "source": [
    "Like rollout collection, evaluation can (and probably shoulb) be performed in\n",
    "a parallel process, so that it does not burden the main thread with computations\n",
    "not related to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3193236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout.evaluate import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90b222",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de9187",
   "metadata": {},
   "source": [
    "## CartPole with REINFORCE or A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e701e3",
   "metadata": {},
   "source": [
    "### the CartPole Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dbbcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# hotfix for gym's unresponsive viz (spawns gl threads!)\n",
    "import rlplay.utils.integration.gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3716f023",
   "metadata": {},
   "source": [
    "The environment factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d3464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from rlplay.zoo.env import NarrowPath\n",
    "\n",
    "\n",
    "class FP32Observation(gym.ObservationWrapper):\n",
    "    def observation(self, observation):\n",
    "        obs = observation.astype(numpy.float32)\n",
    "        obs[0] = 0.  # mask the position info\n",
    "        return obs\n",
    "\n",
    "#     def step(self, action):\n",
    "#         obs, reward, done, info = super().step(action)\n",
    "#         reward -= abs(obs[1]) / 10  # punish for non-zero speed\n",
    "#         return obs, reward, done, info\n",
    "\n",
    "class OneHotObservation(gym.ObservationWrapper):\n",
    "    def observation(self, observation):\n",
    "        return numpy.eye(1, self.env.observation_space.n,\n",
    "                         k=observation, dtype=numpy.float32)[0]\n",
    "\n",
    "def factory(unwrap=True):\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    env = env.unwrapped if unwrap else env\n",
    "    return FP32Observation(env)\n",
    "\n",
    "#     return gym.make(\"Taxi-v3\").unwrapped\n",
    "    # return OneHotObservation(NarrowPath())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea16960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "factory_eval = partial(factory, unwrap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8b9ac",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca2b7c",
   "metadata": {},
   "source": [
    "### the algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829b01a4",
   "metadata": {},
   "source": [
    "Service functions for the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0046223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.utils.plyr import suply, xgetitem\n",
    "\n",
    "def timeshift(state, *, shift=1):\n",
    "    \"\"\"Get current and shfited slices of nested objects.\"\"\"\n",
    "    # use xgetitem to lett None through\n",
    "    # XXX `curr[t]` = (x_t, a_{t-1}, r_t, d_t), t=0..T-H\n",
    "    curr = suply(xgetitem, state, index=slice(None, -shift))\n",
    "\n",
    "    # XXX `next[t]` = (x_{t+H}, a_{t+H-1}, r_{t+H}, d_{t+H}), t=0..T-H\n",
    "    next = suply(xgetitem, state, index=slice(shift, None))\n",
    "\n",
    "    return curr, next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdb5309",
   "metadata": {},
   "source": [
    "The reinforce PG algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6d6aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.algo.returns import pyt_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f68ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.enable_grad()\n",
    "def reinforce(batch, module, *, gamma=0.99, C_entropy=1e-2,\n",
    "              c_rho=float('inf')):\n",
    "    r\"\"\"The REINFORCE algorithm (importance-weighted off-policy).\n",
    "\n",
    "    The basic policy-gradient alogrithm with a baseline $b_t$:\n",
    "    $$\n",
    "        \\nabla_\\theta J(s_t)\n",
    "            = \\mathbb{E}_{a \\sim \\beta(a\\mid s_t)}\n",
    "                \\frac{\\pi(a\\mid s_t)}{\\beta(a\\mid s_t)}\n",
    "                    \\bigl( r_{t+1} + \\gamma G_{t+1} - b_t \\bigr)\n",
    "                \\nabla_\\theta \\log \\pi(a\\mid s_t)\n",
    "        \\,. $$\n",
    "    \"\"\"\n",
    "    \n",
    "    state, state_next = timeshift(batch.state)\n",
    "\n",
    "    # REACT: (state[t], h_t) \\to (\\hat{a}_t, h_{t+1}, \\hat{A}_t)\n",
    "    _, _, info = module(\n",
    "        state.obs, state.act, state.rew, state.fin,\n",
    "        hx=batch.hx, stepno=state.stepno)\n",
    "\n",
    "    # The present value of the future rewards following `state[t]`:\n",
    "    #    G_t = r_{t+1} + \\gamma G_{t+1}\n",
    "    ret = pyt_returns(state_next.rew, state_next.fin,\n",
    "                      gamma=gamma, bootstrap=torch.tensor(0.))\n",
    "\n",
    "    # Assume .act is unstructured: `act[t]` = a_{t+1} -->> T x B x 1\n",
    "    act = state_next.act.unsqueeze(-1)\n",
    "\n",
    "    # \\pi is the target policy, \\mu is the behaviour policy\n",
    "    log_pi, log_mu = info['logits'], batch.actor['logits']\n",
    "\n",
    "    # the importance weights\n",
    "    log_pi_a = log_pi.gather(-1, act).squeeze(-1)\n",
    "    log_mu_a = log_mu.gather(-1, act).squeeze(-1)\n",
    "    rho = log_mu_a.sub_(log_pi_a.detach())\\\n",
    "                  .neg_().exp_().clamp_(max=c_rho)\n",
    "\n",
    "    # the policy surrogate score\n",
    "    #    \\frac1T \\sum_t \\rho_t (G_t - b_t) \\log \\pi(a_t \\mid s_t)\n",
    "    reinfscore = log_pi_a.mul(ret.sub(ret.mean(dim=0)).mul_(rho)).mean()\n",
    "\n",
    "    # the policy entropy score (neg entropy)\n",
    "    #   - H(\\pi(•\\mid s)) = - (-1) \\sum_a \\pi(a\\mid s) \\log \\pi(a\\mid s)\n",
    "    f_min = torch.finfo(log_pi.dtype).min\n",
    "    negentropy = log_pi.exp().mul(log_pi.clamp(min=f_min)).sum(dim=-1).mean()\n",
    "\n",
    "    # maximize the entropy and the reinforce score\n",
    "    # \\ell := - \\frac1T \\sum_t G_t \\log \\pi(a_t \\mid s_t)\n",
    "    #         - C \\mathbb{H} \\pi(\\cdot \\mid s_t)\n",
    "    loss = C_entropy * negentropy - reinfscore\n",
    "    return loss.mean(), dict(entropy=-float(negentropy),\n",
    "                             policy_score=float(reinfscore),)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d77989",
   "metadata": {},
   "source": [
    "Actor-critic algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db891a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# @torch.enable_grad()\n",
    "def a2c(batch, module, *, gamma=0.99, C_entropy=1e-2, C_value=0.25, c_rho=1.0):\n",
    "    r\"\"\"The Advantage Actor-Critic algorithm (importance-weighted off-policy).\n",
    "\n",
    "    Close to REINFORCE, but uses spearate baseline estimate to compute\n",
    "    advantages in the policy grad.\n",
    "    $$\n",
    "        \\nabla_\\theta J(s_t)\n",
    "            = \\mathbb{E}_{a \\sim \\beta(a\\mid s_t)}\n",
    "                \\frac{\\pi(a\\mid s_t)}{\\beta(a\\mid s_t)}\n",
    "                    \\bigl( r_{t+1} + \\gamma G_{t+1} - v(s_t) \\bigr)\n",
    "                \\nabla_\\theta \\log \\pi(a\\mid s_t)\n",
    "        \\,, $$\n",
    "\n",
    "    where the critic estimates the value function under the current policy\n",
    "    $$\n",
    "        v(s_t) \\approx \\mathbb{E}_{\\pi_{\\geq t}}\n",
    "                        G_t(a_t, s_{t+1}, a_{t+1}, ... \\mid s_t)\n",
    "        \\,. $$\n",
    "    \"\"\"\n",
    "    state, state_next = timeshift(batch.state)\n",
    "\n",
    "    # REACT: (state[t], h_t) \\to (\\hat{a}_t, h_{t+1}, \\hat{A}_t)\n",
    "    _, _, info = module(\n",
    "        state.obs, state.act, state.rew, state.fin,\n",
    "        hx=batch.hx, stepno=state.stepno)\n",
    "    # info['value'] = V(`.state[t]`)\n",
    "    #               <<-->> v(x_t)\n",
    "    #               \\approx \\mathbb{E}( G_t \\mid x_t)\n",
    "    #               \\approx \\mathbb{E}( r_{t+1} + \\gamma r_{t+2} + ... \\mid x_t)\n",
    "    #               <<-->> npv(`.state[t+1:]`)\n",
    "    # info['logits'] = \\log \\pi(... | .state[t] )\n",
    "    #                <<-->> \\log \\pi( \\cdot \\mid x_t)\n",
    "\n",
    "    # `.actor[t]` is actor's extra info in reaction to `.state[t]`, t=0..T\n",
    "    bootstrap = batch.actor['value'][-1]\n",
    "    #     `bootstrap` <<-->> `.value[-1]` = V(`.state[-1]`)\n",
    "\n",
    "    # Future rewards following after `.state[t]` are recorded in `.state[t+1:]`\n",
    "    # ret[t] = rew[t] + gamma * (1 - fin[t]) * (ret[t+1] or bootstrap)\n",
    "    ret = pyt_returns(state_next.rew, state_next.fin,\n",
    "                      gamma=gamma, bootstrap=bootstrap)\n",
    "\n",
    "    # XXX post-mul by `1 - \\gamma` fails to train, but seems appropriate\n",
    "    # for the continuation/survival interpretation of the discount factor.\n",
    "    #   <<-- but who says this is a good interpretation?\n",
    "    # ret.mul_(1 - gamma)\n",
    "\n",
    "    # Assume `.act` is unstructured: `act[t]` = a_{t+1} -->> T x B x 1\n",
    "    act = state_next.act.unsqueeze(-1)  # actions taken during the rollout\n",
    "\n",
    "    # \\pi is the target policy, \\mu is the behaviour policy\n",
    "    log_pi, log_mu = info['logits'], batch.actor['logits']\n",
    "\n",
    "    # the importance weights\n",
    "    log_pi_a = log_pi.gather(-1, act).squeeze(-1)\n",
    "    log_mu_a = log_mu.gather(-1, act).squeeze(-1)\n",
    "    rho = log_mu_a.sub_(log_pi_a.detach())\\\n",
    "                  .neg_().exp_().clamp_(max=c_rho)\n",
    "\n",
    "    # the critic's score (negative mse)\n",
    "    #  \\frac1{2 T} \\sum_t (G_t - v(s_t))^2\n",
    "    value = info['value']\n",
    "    critic_mse = 0.5 * F.mse_loss(value, ret, reduction='mean')\n",
    "    # v(x_t) \\approx \\mathbb{E}( G_t \\mid x_t )\n",
    "    #        \\approx G_t (one-point estimate)\n",
    "    #        <<-->> ret[t]\n",
    "\n",
    "    # the policy surrogate score\n",
    "    #    \\frac1T \\sum_t \\rho_t (G_t - v_t) \\log \\pi(a_t \\mid s_t)\n",
    "    a2c_score = log_pi_a.mul(ret.sub(value.detach()).mul_(rho)).mean()\n",
    "\n",
    "    # the policy entropy score (neg entropy)\n",
    "    #   - H(\\pi(•\\mid s)) = - (-1) \\sum_a \\pi(a\\mid s) \\log \\pi(a\\mid s)\n",
    "    f_min = torch.finfo(log_pi.dtype).min\n",
    "    negentropy = log_pi.exp().mul(log_pi.clamp(min=f_min)).sum(dim=-1).mean()\n",
    "\n",
    "    # maximize the entropy and the reinforce score, minimize the critic loss\n",
    "    objective = C_entropy * negentropy + C_value * critic_mse - a2c_score\n",
    "    return objective.mean(), dict(entropy=-float(negentropy),\n",
    "                                  policy_score=float(a2c_score),\n",
    "                                  value_loss=float(critic_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfe28ad",
   "metadata": {},
   "source": [
    "For example we can also use GAE\n",
    "```python\n",
    "from rlplay.algo.returns import pyt_gae\n",
    "\n",
    "# the positional arguments are $r_{t+1}$, $d_{t+1}$, and $v(s_t)$,\n",
    "#  respectively, for $t=0..T-1$. The bootstrap is $v(S_T)$.\n",
    "pyt_gae(batch.state.rew[1:], batch.state.fin[1:], batch.actor['value'][:-1],\n",
    "        gamma=0.99, bootstrap=batch.actor['value'][-1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9af740",
   "metadata": {},
   "source": [
    "#### D-DQN loss\n",
    "\n",
    "Double DQN loss for contiguous trajectory fragements. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3404021",
   "metadata": {},
   "source": [
    "* `.state[t+1].rew` -- $r_{t+1}$\n",
    "* `.state[t+1].fin` -- $d_{t+1}$\n",
    "* `.state[t+1].act` -- $a_t$ which caused $\n",
    "    s_t \\longrightarrow (s_{t+1}, r_{t+1}, d_{t+1})\n",
    "$\n",
    "* `_, _, batch.actor[t] = actor(.state[t])` -- $\n",
    "    Q(z_t, \\cdot; h_t, \\theta_{\\text{old}})\n",
    "$ -- used for rollout collection\n",
    "* `_, _, info_module[t] = module(.state[t])` -- $\n",
    "    Q(z_t, \\cdot; h_t, \\theta)\n",
    "$ -- the current q-function, producing $(h_t)_{t=0}^{T+1}$\n",
    "* `_, _, info_target[t] = target(.state[t])` -- $\n",
    "    Q(z_t, \\cdot; h_t, \\theta_-)\n",
    "$ -- the q-target with $(h^-_t)_{t=0}^{T+1}$\n",
    "\n",
    "The td-error is\n",
    "$$\n",
    "\\delta_t\n",
    "    = r_{t+1} + \\gamma 1_{\\neg d_{t+1}} v^*(z_{t+1})\n",
    "        - Q(z_t, a_t; h_t, \\theta)\n",
    "    \\,, $$\n",
    "\n",
    "where the approximate state value estimate is $\n",
    "    v^*(z_{t+1})\n",
    "$ is one of\n",
    "* $\n",
    "    \\max_a Q(z_{t+1}, a; h_{t+1}, \\theta)\n",
    "$ for the `Q-learning`\n",
    "* $\n",
    "    Q(z_{t+1}, \\hat{a}; h_{t+1}, \\theta_-)\n",
    "$ for $\n",
    "    \\hat{a} = \\arg \\max_a Q(z_{t+1}, a; h_{t+1}, \\theta)\n",
    "$ in the `double DQN`\n",
    "* $\n",
    "    \\max_a Q(z_{t+1}, a; h_{t+1}, \\theta_-)\n",
    "$ in the `DQN`\n",
    "\n",
    "Notice that currently we have two trajectories of $h_t \\to h_{t+1}$:\n",
    "* computed using $\\theta_-$ and $\\theta$\n",
    "  * representational drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbae1aa7",
   "metadata": {},
   "source": [
    "One of the works realted to DQN learning of recurrent agents is [Kapturowski et al. (2018)](https://openreview.net/forum?id=r1lyTjAqYX), who propose to use the burn=in period in\n",
    "the contiguous trajectory fragment in order to compenaste for the representation drift.\n",
    "However, there is noe clear cut evidence suggestingthat the hidden recurrent sequences\n",
    "$h_t$ and $h^-_t$ yield significantly different results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5fa264",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c80ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# @torch.enable_grad()\n",
    "def rddqn(batch, module,\n",
    "          *, target, gamma=0.95, double=True, weights=None, loss=F.mse_loss):\n",
    "    r\"\"\"Compute the Double-DQN loss over a contiguous fragment of a trajectory.\n",
    "\n",
    "    Details\n",
    "    -------\n",
    "    In Q-learning the action value function minimizes the TD-error\n",
    "    $$\n",
    "        r_{t+1} + \\gamma v^*(z_{t+1}) 1_{\\neg d_{t+1}}\n",
    "            - Q(z_t, a_t; \\theta)\n",
    "        \\,, $$\n",
    "    w.r.t. Q-network parameters $\\theta$ where $z_t$ is the actionable , e.g.\n",
    "    the current observation $x_t$ and the recurrent state $h_t$, the last\n",
    "    action $a_{t-1}$, reward $r_t$, and termination flag $d_t$.\n",
    "\n",
    "    If classic Q-learning there is no target network and the next state optimal value function is bootstrapped\n",
    "    using the current Q-network (`module`):\n",
    "    $$\n",
    "        v^*(s_{t+1})\n",
    "            \\approx \\max_a Q(s_{t+1}, a; \\theta)\n",
    "        \\,. $$\n",
    "    The DQN method, proposed by\n",
    "        [Minh et al. (2013)](https://arxiv.org/abs/1312.5602),\n",
    "    uses a secondary Q-network to estimate the value of the next state:\n",
    "    $$\n",
    "        v^*(s_{t+1})\n",
    "            \\approx \\max_a Q(s_{t+1}, a; \\theta^-)\n",
    "        \\,, $$\n",
    "    where $\\theta^-$ are frozen parameters of the Q-network (`target`). The\n",
    "    Double DQN algorithm of\n",
    "        [van Hasselt et al. (2015)](https://arxiv.org/abs/1509.06461)\n",
    "    unravels the $\\max$ operator as $\n",
    "        \\max_k u_k\n",
    "            \\equiv u_{\\arg \\max_k u_k}\n",
    "    $ and replaces the outer $u$ with the Q-values of the target net, while\n",
    "    the inner $u$ (inside the $\\arg\\max$) is computed with the Q-values of the\n",
    "    current Q-network. Specifically, the Double DQN value estimate is\n",
    "    $$\n",
    "        v^*(s_{t+1})\n",
    "            \\approx Q(s_{t+1}, \\hat{a}_{t+1}; \\theta^-)\n",
    "        \\,, $$\n",
    "    for $\n",
    "        \\hat{a}_{t+1}\n",
    "            = \\arg \\max_a Q(s_{t+1}, a; \\theta)\n",
    "    $ being the action taken by the current Q-network $\\theta$ at $s_{t+1}$.\n",
    "    \"\"\"\n",
    "\n",
    "    if target is None:\n",
    "        # use $Q(\\cdot; \\theta)$ instead of $Q(\\cdot; \\theta^-)$\n",
    "        target, double = module, False\n",
    "\n",
    "    fragment = batch.state\n",
    "\n",
    "    # get $Q(z_t, \\cdot; h_t, \\theta)$ for all t=0..T, and $(h_t)_t$\n",
    "    _, _, info_module = module(\n",
    "        fragment.obs, fragment.act,\n",
    "        fragment.rew, fragment.fin,\n",
    "        hx=batch.hx, stepno=fragment.stepno)\n",
    "\n",
    "    # get $Q(z_t, \\cdot; h^-_t \\theta^-)$ for all t=0..T, and $(h^-_t)_t$\n",
    "    # XXX make sure no grads are computed for the target Q-value\n",
    "    with torch.no_grad():\n",
    "        info_target = info_module\n",
    "        if target is not module:\n",
    "            _, _, info_target = target(\n",
    "                fragment.obs, fragment.act,\n",
    "                fragment.rew, fragment.fin,\n",
    "                hx=batch.hx, stepno=fragment.stepno)\n",
    "\n",
    "    # $\\hat{A}_t$: get the module's response to current and next state by shifting\n",
    "    info_module_curr, info_module_next = timeshift(info_module)\n",
    "\n",
    "    # get the next state `state[t+1]`, and the target's response to it\n",
    "    state_next, info_target_next = suply(\n",
    "        xgetitem, (fragment, info_target), index=slice(1, None))\n",
    "\n",
    "    # get $Q(z_t, a_t; h_t, \\theta)$ for all t=0..T-1\n",
    "    q_replay = info_module_curr['q'].gather(-1, state_next.act.unsqueeze(-1))\n",
    "\n",
    "    # get the target value\n",
    "    with torch.no_grad():\n",
    "        if double and module is not target:\n",
    "            # get $\\hat{a} = \\arg \\max_a Q(z_{t+1}, a; h_{t+1}, \\theta)$\n",
    "            # XXX can we reuse $\\hat{a}$ from the REACT step?\n",
    "            hat_a = info_module_next['q'].max(dim=-1).indices\n",
    "\n",
    "            # get $\\hat{v}(z_{t+1}) = Q(z_{t+1}, \\hat{a}; h^-_{t+1}, \\theta^-)$\n",
    "            q_value = info_target_next['q'].gather(-1, hat_a.unsqueeze(-1))\n",
    "\n",
    "        else:\n",
    "            # get $\\hat{v}(z_{t+1}) = \\max_a Q(z_{t+1}, a; h^-_{t+!}, \\theta^-)$\n",
    "            q_value = info_target_next['q'].max(dim=-1, keepdim=True).values\n",
    "\n",
    "        # get $r_{t+1} + \\gamma 1_{d_{t+1}} \\hat{v}(z_{t+1})$ using inplace ops\n",
    "        q_value.masked_fill_(state_next.fin.unsqueeze(-1), 0.)\n",
    "        q_value.mul_(gamma).add_(state_next.rew.unsqueeze(-1))\n",
    "\n",
    "        # compute the Temp. Diff. error (δ) for experience prioritization\n",
    "        td_error = q_replay - q_value\n",
    "    # end with\n",
    "\n",
    "    # the (weighted) td-error loss\n",
    "    if weights is None:\n",
    "        value = loss(q_replay, q_value, reduction='mean')\n",
    "        return value, {'td_error': td_error}\n",
    "\n",
    "    values = loss(q_replay, q_value, reduction='none')\n",
    "    return weights.mul(values).mean(), {'td_error': td_error}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab8352",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8bf14",
   "metadata": {},
   "source": [
    "### the Actor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2d9b3",
   "metadata": {},
   "source": [
    "The policy of the actor"
   ]
  },
  {
   "cell_type": "raw",
   "id": "428d6e21",
   "metadata": {},
   "source": [
    "def policy():\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(4, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, 2),\n",
    "        torch.nn.LogSoftmax(dim=-1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442a90a2",
   "metadata": {},
   "source": [
    "A procedure and a layer, which converts the input integer data into its\n",
    "little endian binary representation as float $\\{0, 1\\}^m$ vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotbits(input, n_bits=63, dtype=torch.float):\n",
    "    \"\"\"Encode integers to fixed-width binary floating point vectors\"\"\"\n",
    "    assert not input.dtype.is_floating_point\n",
    "    assert 0 < n_bits < 64  # torch.int64 is signed, so 64-1 bits max\n",
    "\n",
    "    # n_bits = {torch.int64: 63, torch.int32: 31, torch.int16: 15, torch.int8 : 7}\n",
    "\n",
    "    # get mask of set bits\n",
    "    pow2 = torch.tensor([1 << j for j in range(n_bits)]).to(input.device)\n",
    "    x = input.unsqueeze(-1).bitwise_and(pow2).to(bool)\n",
    "\n",
    "    # upcast bool to float to get one-hot\n",
    "    return x.to(dtype)\n",
    "\n",
    "class OneHotBits(torch.nn.Module):\n",
    "    def __init__(self, n_bits=63, dtype=torch.float):\n",
    "        assert 1 <= n_bits < 63\n",
    "        super().__init__()\n",
    "        self.n_bits, self.dtype = n_bits, dtype\n",
    "\n",
    "    def forward(self, input):\n",
    "        return onehotbits(input, n_bits=self.n_bits, dtype=self.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e56dd07",
   "metadata": {},
   "source": [
    "A more sophisticated recurrent learner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652072e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "\n",
    "class CartPoleActor(BaseActorModule):\n",
    "    def __init__(self, epsilon=0.1, lstm=False, use_cudnn=True):\n",
    "        super().__init__()\n",
    "        self.epsilon, self.lstm, self.use_cudnn = epsilon, lstm, use_cudnn\n",
    "\n",
    "        self.features = torch.nn.ModuleDict(dict(\n",
    "            obs=torch.nn.Sequential(\n",
    "                torch.nn.Linear(4, 64),\n",
    "                torch.nn.ReLU(),\n",
    "            ),\n",
    "            act=torch.nn.Embedding(2, 4),\n",
    "            rew=torch.nn.Sequential(\n",
    "                torch.nn.Linear(1, 4),\n",
    "                torch.nn.ReLU(),\n",
    "            ),\n",
    "            stepno=torch.nn.Sequential(\n",
    "                OneHotBits(16),\n",
    "                torch.nn.Linear(16, 16),\n",
    "                torch.nn.ReLU(),\n",
    "            ),\n",
    "        ))\n",
    "\n",
    "        n_features = 64 + 4 + 4 + 16\n",
    "        if not self.lstm:\n",
    "            self.core = torch.nn.Sequential(\n",
    "                torch.nn.Linear(n_features, 64),\n",
    "                torch.nn.ReLU(),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.core = torch.nn.LSTM(n_features, 64, 1)\n",
    "\n",
    "        self.policy = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64, 2),\n",
    "            torch.nn.LogSoftmax(dim=-1),\n",
    "        )\n",
    "\n",
    "        self.baseline = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, act, rew, fin,\n",
    "                *, hx=None, stepno=None, virtual=False):\n",
    "        # Everything is  [T x B x ...]\n",
    "        inputs = torch.cat([\n",
    "            self.features['obs'](obs),\n",
    "            self.features['act'](act),\n",
    "            self.features['rew'](rew.unsqueeze(-1)),\n",
    "            self.features['stepno'](stepno),\n",
    "        ], dim=-1)\n",
    "        \n",
    "        if not self.lstm:\n",
    "            output, hx = self.core(inputs), ()\n",
    "\n",
    "        elif self.use_cudnn:\n",
    "            # sequence padding (MUST have sampling with `sticky=True`)\n",
    "            n_steps, n_env, *_ = fin.shape\n",
    "            if n_steps > 1:\n",
    "                # we assume sticky=True\n",
    "                lengths = 1 + (~fin[1:]).sum(0).cpu()\n",
    "                inputs = pack_padded_sequence(inputs, lengths, enforce_sorted=False)\n",
    "\n",
    "            output, hx = self.core(inputs, hx)\n",
    "            if n_steps > 1:\n",
    "                output, lens = pad_packed_sequence(\n",
    "                    output, batch_first=False, total_length=n_steps)\n",
    "\n",
    "        else:\n",
    "            # inputs is T x B x F, hx is either None, or a proper recurrent state\n",
    "            outputs = []\n",
    "            # manually step through the RNN core\n",
    "            for input, mask in zip(inputs.unsqueeze(1), fin.unsqueeze(-1)):\n",
    "                # zero if f indicates reset: multiplying by zero stops grad\n",
    "                if hx is not None:\n",
    "                    # stop hx grads if `reset` (mul-by-zero)\n",
    "                    hx = suply(torch.Tensor.mul, hx, other=~mask)\n",
    "\n",
    "                output, hx = self.core(input, hx)\n",
    "                outputs.append(output)\n",
    "\n",
    "            output = torch.cat(outputs, dim=0)\n",
    "\n",
    "        # value must not have any trailing dims, i.e. T x B\n",
    "        logits = self.policy(output)\n",
    "        value = self.baseline(output).squeeze(-1)\n",
    "\n",
    "        # XXX eps-greedy?\n",
    "        if self.training:\n",
    "            # blend the policy with a uniform distribution\n",
    "            prob = logits.detach().exp().mul_(1 - self.epsilon)\n",
    "            prob.add_(self.epsilon / logits.shape[-1])\n",
    "\n",
    "            actions = multinomial(prob)\n",
    "\n",
    "        else:\n",
    "            actions = logits.argmax(dim=-1)\n",
    "\n",
    "        return actions, hx, dict(\n",
    "            value=value,\n",
    "            logits=logits,\n",
    "            # entropy\n",
    "            # logit_at_the_chosen_action\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c21f422",
   "metadata": {},
   "source": [
    "Initialize the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f2f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner, sticky = nonRecurrentPolicyWrapper(policy()), False\n",
    "learner = CartPoleActor(lstm=False, use_cudnn=False)\n",
    "sticky = learner.use_cudnn and learner.lstm\n",
    "\n",
    "learner.train()\n",
    "device_ = torch.device('cpu')  # torch.device('cuda:0')\n",
    "learner.to(device=device_)\n",
    "\n",
    "# prepare the optimizer for the learner\n",
    "optim = torch.optim.Adam(learner.parameters(), lr=1e-3, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5296a078",
   "metadata": {},
   "source": [
    "Load a better trained agent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d1e5907",
   "metadata": {},
   "source": [
    "filename = '/Users/ivannazarov/Github/repos_with_rl/rlplay/stage/model.pk'\n",
    "learner.policy.load_state_dict(torch.load(filename)['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82730b1a",
   "metadata": {},
   "source": [
    "Initialize the sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991c289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T, B = 120, 20\n",
    "# T, B = 120, 4\n",
    "T, B = 21, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c13d6d",
   "metadata": {},
   "source": [
    "Pick one collector\n",
    "* NetHack environment `nle`, does not like `fork` method, so we should use `spawn`, which is not notebook friendly :(\n",
    "  * essentially it is better to prototype in notebook with `same.rollout`, then write a submodule non-interactive script with `multi.rollout`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "487cd579",
   "metadata": {},
   "source": [
    "# generator of rollout batches\n",
    "batchit = same.rollout(\n",
    "    [factory()],\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    sticky=sticky,\n",
    "    device=device_,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ddb25f04",
   "metadata": {},
   "source": [
    "# generator of rollout batches\n",
    "batchit = single.rollout(\n",
    "    factory,\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    n_envs=B,\n",
    "    sticky=sticky,  # so that we can leverage cudnn's fast RNN implementations\n",
    "    clone=False,\n",
    "    close=False,\n",
    "    device=device_,\n",
    "    start_method='fork',  # fork in notebook for macos, spawn in linux\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdea1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator of rollout batches\n",
    "batchit = multi.rollout(\n",
    "    factory,\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    n_actors=8,\n",
    "    n_per_actor=B,\n",
    "    n_buffers=16,\n",
    "    n_per_batch=2,\n",
    "    sticky=sticky,  # so that we can leverage cudnn's fast RNN implementations\n",
    "    pinned=False,\n",
    "    clone=False,\n",
    "    close=False,\n",
    "    device=device_,\n",
    "    start_method='fork',  # fork in notebook for macos, spawn in linux\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846d8c9",
   "metadata": {},
   "source": [
    "Implement your favourite training method"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1cbc1265",
   "metadata": {},
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0633c36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# pytoch loves to hog all threads on some linux systems \n",
    "torch.set_num_threads(1)\n",
    "\n",
    "gamma = 0.99\n",
    "losses, rewards = [], []\n",
    "\n",
    "# generator of evaluation rewards\n",
    "# test_it = test(factory_eval, learner, n_envs=4, n_steps=500, device=device_)\n",
    "test_it = evaluate(factory_eval, learner, n_envs=4, n_steps=500,\n",
    "                   clone=False, device=device_, start_method='fork')\n",
    "\n",
    "# the training loop\n",
    "exclude = {'returns'}\n",
    "ewm, alpha = None, 0.5\n",
    "for epoch in tqdm.tqdm(range(400)):\n",
    "    for j, batch in zip(range(100), batchit):\n",
    "        loss, info = a2c(batch, learner, gamma=gamma, c_rho=1.5)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        grad_norm = clip_grad_norm_(learner.parameters(), max_norm=1e2)\n",
    "        optim.step()\n",
    "\n",
    "        losses.append({k: float(v) for k, v in info.items() if k not in exclude})\n",
    "        losses[-1]['grad'] = float(grad_norm)\n",
    "\n",
    "    # fetch the evaluation results (lag by one innre loop!)\n",
    "    rewards.append(next(test_it))\n",
    "\n",
    "    # eqm track the minimal evaluation reward\n",
    "    low = rewards[-1].min()\n",
    "    if ewm is None:\n",
    "        ewm = low\n",
    "\n",
    "    else:\n",
    "        ewm += alpha * (low - ewm)\n",
    "\n",
    "    if ewm > 498:\n",
    "        break\n",
    "\n",
    "# close the generators\n",
    "batchit.close()\n",
    "test_it.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b7a97b3",
   "metadata": {},
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchit.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce115d82",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(records):\n",
    "    \"\"\"collate identically keyed dicts\"\"\"\n",
    "    out, n_records = {}, 0\n",
    "    for record in records:\n",
    "        for k, v in record.items():\n",
    "            out.setdefault(k, []).append(v)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k: numpy.array(v) for k, v in collate(losses).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'value_loss' in data:\n",
    "    plt.semilogy(data['value_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2638e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['entropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ffbbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['policy_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01581b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(data['grad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3128f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = numpy.stack(rewards, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5429aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd6ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = numpy.median(rewards, axis=-1), rewards.std(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5545163",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fi, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "\n",
    "ax.plot(numpy.mean(rewards, axis=-1))\n",
    "ax.plot(numpy.median(rewards, axis=-1))\n",
    "ax.plot(numpy.min(rewards, axis=-1))\n",
    "ax.plot(numpy.std(rewards, axis=-1))\n",
    "# ax.plot(m+s * 1.96)\n",
    "# ax.plot(m-s * 1.96)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d372a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with factory_eval() as env:\n",
    "    learner.eval()\n",
    "    eval_rewards, info = core_evaluate([\n",
    "        env\n",
    "    ], learner, render=True, n_steps=1e4, device=device_)\n",
    "\n",
    "print(sum(eval_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d924c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.algo.returns import npy_returns, npy_deltas\n",
    "\n",
    "td_target = eval_rewards + gamma * info['value'][1:]\n",
    "td_error = td_target - info['value'][:-1]\n",
    "# td_error = npy_deltas(\n",
    "#     eval_rewards, numpy.zeros_like(eval_rewards, dtype=bool), info['value'][:-1],\n",
    "#     gamma=gamma, bootstrap=info['value'][-1])\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "ax.semilogy(abs(td_error) / abs(td_target))\n",
    "# ax.set_ylim(-0.0001, 0.002);\n",
    "ax.set_title('relative td(1)-error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef93ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.algo.returns import npy_returns, npy_deltas\n",
    "\n",
    "# plt.plot(\n",
    "#     npy_returns(eval_rewards, numpy.zeros_like(eval_rewards, dtype=bool),\n",
    "#                 gamma=gamma, bootstrap=info['value'][-1]))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "ax.plot(info['value'])\n",
    "ax.axhline(1 / (1 - gamma), c='k', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a742795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.special import softmax, expit, entr\n",
    "\n",
    "*head, n_actions = info['logits'].shape\n",
    "proba = softmax(info['logits'], axis=-1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "ax.plot(entr(proba).sum(-1)[:, 0])\n",
    "ax.axhline(math.log(n_actions), c='k', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a12f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "ax.hist(info['logits'][..., 1] - info['logits'][..., 0], bins=51);  # log-ratio"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19438b2d",
   "metadata": {},
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09142f04",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stepno = batch.state.stepno\n",
    "stepno = torch.arange(16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91906ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = learner.features['stepno'](stepno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4953372",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(8, 8), dpi=200,\n",
    "                         sharex=True, sharey=True)\n",
    "\n",
    "for j, ax in zip(range(out.shape[1]), axes.flat):\n",
    "    ax.plot(out[:, j], lw=1)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    plt.imshow(abs(learner.core[0].weight[:, 64+4+4:]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f353e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    plt.imshow(abs(learner.features.stepno[1].weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f455968",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(learner.features.stepno[1].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4cdeca",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7256ce6",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5199682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p_l, v_l, ent = zip(*losses)\n",
    "\n",
    "plt.plot(p_l)\n",
    "plt.plot(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29409218",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(v_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270e3622",
   "metadata": {},
   "source": [
    "Run in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c62738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([\n",
    "    sum(evaluate(factory, learner, render=False))\n",
    "    for _ in range(200)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e3e89",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b98592",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215a7758",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bar:\n",
    "    def __init__(self, parent):\n",
    "        self.parent = parent\n",
    "        self._range = range(self.parent.n)\n",
    "        self._it = iter(self._range)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self._it)\n",
    "\n",
    "class Foo:\n",
    "    def __init__(self, n=10):\n",
    "        self.n = n\n",
    "\n",
    "    def __iter__(self):\n",
    "        return Bar(self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4fe92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Foo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e4ec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bar:\n",
    "    def __init__(self, parent):\n",
    "        self.parent = parent\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from range(self.parent.n)\n",
    "\n",
    "class Foo:\n",
    "    def __init__(self, n=10):\n",
    "        self.n = n\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(Bar(self))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68eeff0",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
