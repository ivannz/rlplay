{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "830df33b",
   "metadata": {},
   "source": [
    "# `rlplay`-ing with world models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718fe01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "\n",
    "# hotfix for gym's unresponsive viz (spawns gl threads!)\n",
    "import rlplay.utils.integration.gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f2682",
   "metadata": {},
   "source": [
    "See example.ipynb for the overview of `rlplay`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a46b32",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16edf8af",
   "metadata": {},
   "source": [
    "A base class for deep gaussian networks, taken from a prior project on Deep Weight Prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4223bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softplus\n",
    "from torch.distributions import Normal, Independent\n",
    "\n",
    "\n",
    "class BaseDeepIndependentGaussian(torch.nn.Module):\n",
    "    def __init__(self, input_shape, event_shape):\n",
    "        super().__init__()\n",
    "        self.input_shape = torch.Size(input_shape)\n",
    "        self.event_shape = torch.Size(event_shape)\n",
    "\n",
    "        # zero and one for the std Gaussian prior\n",
    "        # XXX keep as a buffer for device sync\n",
    "        self.register_buffer('nilone', torch.tensor([0., 1.]))\n",
    "    \n",
    "    @property\n",
    "    def prior(self):\n",
    "        shape = self.event_shape\n",
    "\n",
    "        # kl-std normal: factorized std gaussian prior\n",
    "        return Independent(Normal(*self.nilone).expand(shape), len(shape))\n",
    "\n",
    "    def forward(self, input):\n",
    "        n_dim_input, n_dim_event = len(self.input_shape), len(self.event_shape)\n",
    "        assert input.shape[-n_dim_input:] == self.input_shape\n",
    "\n",
    "        # flatten the composite batch dim, keeping feature dims intact\n",
    "        # XXX the final layer must have twice the number of channels/features for chunking\n",
    "        output = self.features(input.flatten(0, -n_dim_input-1))\n",
    "\n",
    "        # get location and scale with original batch dims (doubled features)\n",
    "        output = output.reshape(*input.shape[:-n_dim_input], *output.shape[1:])\n",
    "        loc, logscale = torch.chunk(output, 2, dim=-n_dim_event)\n",
    "        # assert self.event_shape == loc.shape[-n_dim_event:]\n",
    "\n",
    "        return Independent(Normal(loc, softplus(logscale)), n_dim_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b374c6",
   "metadata": {},
   "source": [
    "A possible redesign of the indep gaussian class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaf269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENH wrap, not inherit!\n",
    "\n",
    "# cast a dual-output network as a factorized gaussian\n",
    "class AsIndependentGaussian(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        module,             # dims In: (B*)CS* -->> dims Out: \\1FS*  # re syntax\n",
    "        n_dim_in=1,         # determines the number of trailing dims designated as input features\n",
    "        n_dim_out=1,        # the number of trailing dims allotted to a single random draw (event_size)\n",
    "        prior=None,         # The prior associated with this Gaussian, standard if None\n",
    "        batch_first=None,   # bool, the order of batch and sequence dims for recurrent nets\n",
    "                            # XXX might be very awkward to implement...\n",
    "        scale_fn=softplus,  # The transformation to apply to the output desiganted as scale\n",
    "    ):\n",
    "        assert n_dim_out >= 1\n",
    "        assert batch_first is None\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_dim_in, self.n_dim_out = n_dim_in, n_dim_out\n",
    "        self.scale_fn, self.batch_first = scale_fn, batch_first\n",
    "\n",
    "        # construct the standard Gaussian prior as default\n",
    "        if prior is None:\n",
    "            # zero and one for the prior, kept as a buffer for sync\n",
    "            self.register_buffer('nilone', torch.tensor([0., 1.]))\n",
    "\n",
    "        else:\n",
    "            self.prior = prior\n",
    "\n",
    "        self.module = module\n",
    "\n",
    "    @property\n",
    "    def prior(self):\n",
    "        # the default standard gaussian prior, can be overridden\n",
    "        nd = self.n_dim_out\n",
    "        nilone = self.nilone.reshape(2, *(1,) * nd)\n",
    "        return Independent(Normal(*nilone), nd)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # If network outputs a tuple, then interpret its as a ready loc-scale pair.\n",
    "        # Otherwise split the tensor in half along the correct trailing dim.\n",
    "\n",
    "        assert self.batch_first is None\n",
    "        if isinstance(input, torch.Tensor) and self.n_dim_in is not None:\n",
    "            # we need this to assign proper batch dims to the returned\n",
    "            # distribution object, and to make sure not to confuse events dims.\n",
    "            assert self.n_dim_in >= 1\n",
    "\n",
    "            # flatten the batch dims, keeping feature dims intact, then undo it on the output\n",
    "            out = self.module(input.reshape(-1, *input.shape[-self.n_dim_in:]))\n",
    "            out = out.reshape(*input.shape[:-self.n_dim_in],\n",
    "                              *out.shape[-self.n_dim_out:])\n",
    "\n",
    "            # the output's correct dim must have even size!\n",
    "            loc, scale_ = torch.chunk(out, 2, dim=-self.n_dim_out)\n",
    "\n",
    "            # apply the +ve valued monotonic transformation to the prescale\n",
    "            input = loc, self.scale_fn(scale_)\n",
    "\n",
    "        # cannot auto-infer the event dim, rely on the `n_dim_out` parameter\n",
    "        # assume the shapes are proper\n",
    "        return Independent(Normal(*input), self.n_dim_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27647420",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = AsIndependentGaussian(torch.nn.Linear(32, 8*2))\n",
    "\n",
    "enc = AsIndependentGaussian(torch.nn.Linear(32, 8*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8e7c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 2, 3, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c9425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = enc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a51db9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pi = enc.prior(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.kl_divergence(q, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b85d43e",
   "metadata": {},
   "source": [
    "A generic $\n",
    "    (q(z \\mid x), \\pi(z), p(y \\mid z))\n",
    "$ loss for var-Bayes with an explicit prior and SGVB or IWAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbb4059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import torch.distributions as dist\n",
    "\n",
    "def vbayes(enc, dec, /, X, Y=None, *, prior=None, beta=1., n_draws=1, iwae=False):\n",
    "    \"\"\"Compute the SGVB or the IWAE objective.\n",
    "    enc is the approximate posterior q(z \\mid x)\n",
    "    dec is the approximate model p(y \\mid z)\n",
    "    \n",
    "    See the supplementary material of\n",
    "\n",
    "        [Bachman and Precup (2015)](http://proceedings.mlr.press/v37/bachman15.html)\n",
    "\n",
    "    for some brief but clear discussion of what turns out to be\n",
    "    the idea below (variational trasncoder if X \\neq Y).\n",
    "    \"\"\"\n",
    "    pi = enc.prior if prior is None else prior\n",
    "    Y = X if Y is None else Y  # auto-encode is Y is not X\n",
    "\n",
    "    # `X=Y` is `*batch x *dec.event_shape`\n",
    "    q = enc(X)  # q.batch_shape is `batch`\n",
    "\n",
    "    # `Z` is `n_draws x *q.batch_shape x *q.event_shape`\n",
    "    Z = q.rsample([n_draws])  # XXX diff-able sampling with (implicit) rep-trick!\n",
    "\n",
    "    # `log_p` has shape `n_draws x *q.batch_shape x *q.event_shape`\n",
    "    log_p = dec(Z).log_prob(Y)  # XXX may consume a lot of mem!\n",
    "\n",
    "    ll = log_p.mean()  # dim=(0, 1)\n",
    "    if iwae and n_draws > 1:\n",
    "        # (iwae)_k = E_{x y} E_{S~q^k(z|x)} log E_{z~S} p(y|z) pi(z) / q(z|x)\n",
    "        #  * like (sgvb)_k but E_z and log are interchanged\n",
    "        #  * beta-anneal the pi(z) / q(z|x) ratio\n",
    "        log_iw = log_p + (pi.log_prob(Z) - q.log_prob(Z)) * beta\n",
    "        loss = log(n_draws) - torch.logsumexp(log_iw, dim=0).mean()  # dim=0\n",
    "\n",
    "    else:\n",
    "        # (sgvb)_k = E_{x y} E_{S~q^k(z|x)} E_{z~S} log p(y|z) pi(z) / q(z|x)\n",
    "        kl_q_pi = dist.kl_divergence(q, pi)\n",
    "        loss = beta * kl_q_pi.mean() - ll  # dim=0\n",
    "\n",
    "    return loss, q, float(ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90b222",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nilone = torch.tensor([0., 1.])\n",
    "nilone = nilone.reshape(-1, *(1,)*1)\n",
    "pi = Independent(Normal(*nilone), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5538f361",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.kl_divergence(q, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac74150f",
   "metadata": {},
   "source": [
    "A simple function to collate a list of dicts into a dict of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f5b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(records):\n",
    "    \"\"\"collate identically keyed dicts\"\"\"\n",
    "    out, n_records = {}, 0\n",
    "    for record in records:\n",
    "        for k, v in record.items():\n",
    "            out.setdefault(k, []).append(v)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de9187",
   "metadata": {},
   "source": [
    "## Sophisticated CartPole with PG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f92f1c9",
   "metadata": {},
   "source": [
    "### The environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390dfcb3",
   "metadata": {},
   "source": [
    "The environment factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.zoo.env import NarrowPath\n",
    "\n",
    "\n",
    "class FP32Observation(gym.ObservationWrapper):\n",
    "    def observation(self, observation):\n",
    "        return observation.astype(numpy.float32)\n",
    "#         obs = observation.astype(numpy.float32)\n",
    "#         obs[0] = 0.  # mask the position info\n",
    "#         return obs\n",
    "\n",
    "#     def step(self, action):\n",
    "#         obs, reward, done, info = super().step(action)\n",
    "#         reward -= abs(obs[1]) / 10  # punish for non-zero speed\n",
    "#         return obs, reward, done, info\n",
    "\n",
    "class OneHotObservation(gym.ObservationWrapper):\n",
    "    def observation(self, observation):\n",
    "        return numpy.eye(1, self.env.observation_space.n,\n",
    "                         k=observation, dtype=numpy.float32)[0]\n",
    "\n",
    "def base_factory():\n",
    "    return gym.make(\"LunarLander-v2\")\n",
    "#     return FP32Observation(gym.make(\"CartPole-v0\").unwrapped)\n",
    "    # return OneHotObservation(NarrowPath())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8b9ac",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8bf14",
   "metadata": {},
   "source": [
    "### the Actor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442a90a2",
   "metadata": {},
   "source": [
    "A procedure and a layer, which converts the input integer data into its\n",
    "little endian binary representation as float $\\{0, 1\\}^m$ vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotbits(input, n_bits=63, dtype=torch.float):\n",
    "    \"\"\"Encode integers to fixed-width binary floating point vectors\"\"\"\n",
    "    assert not input.dtype.is_floating_point\n",
    "    assert 0 < n_bits < 64  # torch.int64 is signed, so 64-1 bits max\n",
    "\n",
    "    # n_bits = {torch.int64: 63, torch.int32: 31, torch.int16: 15, torch.int8 : 7}\n",
    "\n",
    "    # get mask of set bits\n",
    "    pow2 = torch.tensor([1 << j for j in range(n_bits)]).to(input.device)\n",
    "    x = input.unsqueeze(-1).bitwise_and(pow2).to(bool)\n",
    "\n",
    "    # upcast bool to float to get one-hot\n",
    "    return x.to(dtype)\n",
    "\n",
    "\n",
    "class OneHotBits(torch.nn.Module):\n",
    "    def __init__(self, n_bits=63, dtype=torch.float):\n",
    "        assert 1 <= n_bits < 64\n",
    "        super().__init__()\n",
    "        self.n_bits, self.dtype = n_bits, dtype\n",
    "\n",
    "    def forward(self, input):\n",
    "        return onehotbits(input, n_bits=self.n_bits, dtype=self.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984f850",
   "metadata": {},
   "source": [
    "A special module dictionary, which aplies itself to the input dict of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f3fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Mapping\n",
    "from torch.nn import Module, ModuleDict as BaseModuleDict\n",
    "\n",
    "\n",
    "class ModuleDict(BaseModuleDict):\n",
    "    \"\"\"The ModuleDict, that applies itself to hte indup dicts.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        modules: Optional[Mapping[str, Module]] = None,\n",
    "        dim: Optional[int]=-1\n",
    "    ) -> None:\n",
    "        super().__init__(modules)\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        # enforce concatenation in the order of the declaration in  __init__\n",
    "        return torch.cat([\n",
    "            m(input[k]) for k, m in self.items()\n",
    "        ], dim=self.dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b945a330",
   "metadata": {},
   "source": [
    "A more sophisticated policy learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf4300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine import BaseActorModule\n",
    "from rlplay.utils.common import multinomial\n",
    "\n",
    "from torch.nn import Sequential, Linear, ReLU, LogSoftmax\n",
    "\n",
    "class CartPoleActor(BaseActorModule):\n",
    "    def __init__(self, lstm='none'):\n",
    "        assert lstm in ('none', 'loop', 'cudnn')\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_lstm = self.use_cudnn = False\n",
    "\n",
    "        # blend the policy with a uniform distribution, determined by\n",
    "        #  the exploration epsilon. We update it in the actor clones via a buffer\n",
    "        # self.register_buffer('epsilon', torch.tensor(epsilon))\n",
    "        # XXX isn't the stochastic policy random enough by itself?\n",
    "\n",
    "        z_dim, a_dim = 8, 4\n",
    "        self.baseline = Sequential(\n",
    "            Linear(z_dim, 128),\n",
    "            ReLU(),\n",
    "            Linear(128, 1),\n",
    "        )\n",
    "        self.policy = Sequential(\n",
    "            Linear(z_dim, 128),\n",
    "            ReLU(),\n",
    "            Linear(128, a_dim),\n",
    "            LogSoftmax(dim=-1),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, act, rew, fin, *, hx=None, stepno=None, virtual=False):\n",
    "        # value must not have any trailing dims, i.e. T x B\n",
    "        logits = self.policy(obs)\n",
    "        value = self.baseline(obs).squeeze(-1)\n",
    "\n",
    "        if not self.training:\n",
    "            actions = logits.argmax(dim=-1)\n",
    "\n",
    "        else:\n",
    "            actions = multinomial(logits.detach().exp())\n",
    "\n",
    "        return actions, (), dict(value=value, logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2cdf87",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a735f8",
   "metadata": {},
   "source": [
    "Modules for the WM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af365e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepThreeLayerGaussian(BaseDeepIndependentGaussian):\n",
    "    def __init__(self, dim_in=4, dim_out=2, h_dim=32):\n",
    "        super().__init__([dim_in], [dim_out])\n",
    "        self.features = torch.nn.Sequential(\n",
    "            torch.nn.Linear(dim_in, h_dim),\n",
    "            torch.nn.ELU(),\n",
    "            torch.nn.Linear(h_dim, h_dim),\n",
    "            torch.nn.ELU(),\n",
    "            torch.nn.Linear(h_dim, 2 * dim_out),\n",
    "        )\n",
    "\n",
    "class Encoder:\n",
    "    def __new__(self, x_dim=4, z_dim=2, h_dim=32):\n",
    "        return DeepThreeLayerGaussian(x_dim, z_dim, h_dim=h_dim)\n",
    "\n",
    "class Decoder:\n",
    "    def __new__(self, z_dim=2, x_dim=4, h_dim=32):\n",
    "        return DeepThreeLayerGaussian(z_dim, x_dim, h_dim=h_dim)\n",
    "\n",
    "class Dynamics(DeepThreeLayerGaussian):\n",
    "    def __init__(self, z_dim=2, a_dim=2, u_dim=4, h_dim=32):\n",
    "        super().__init__(z_dim + u_dim, z_dim, h_dim=h_dim)\n",
    "        self.act = torch.nn.Embedding(a_dim, u_dim)  # hardcoded\n",
    "    \n",
    "    def forward(self, zed, act, fin=None, hx=None):\n",
    "        input = torch.cat([zed, self.act(act)], dim=-1)\n",
    "        return super().forward(input), ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca00b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "class WMCartPoleActor(BaseActorModule):\n",
    "    def __init__(self, lstm='none', n_draws=1):\n",
    "        assert lstm in ('none', 'loop', 'cudnn')\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_lstm = self.use_cudnn = False\n",
    "\n",
    "        x_dim, z_dim, a_dim = 8, 4, 4\n",
    "        self.enc = Encoder(x_dim=x_dim, z_dim=z_dim)\n",
    "        self.dec = Decoder(z_dim=z_dim, x_dim=x_dim)\n",
    "#         self.dyn = Dynamics(z_dim=z_dim, a_dim=a_dim, u_dim=2)\n",
    "        self.dyn = None\n",
    "\n",
    "        self.baseline = Sequential(\n",
    "            Linear(z_dim, 20),\n",
    "            ReLU(),\n",
    "            Linear(20, 1),\n",
    "        )\n",
    "        self.policy = Sequential(\n",
    "            Linear(z_dim, 20),\n",
    "            ReLU(),\n",
    "            Linear(20, a_dim),\n",
    "            LogSoftmax(dim=-1),\n",
    "        )\n",
    "        \n",
    "        self.n_draws = n_draws\n",
    "        self.x_dim, self.z_dim, self.a_dim = x_dim, z_dim, a_dim\n",
    "\n",
    "    def forward(self, obs, act, rew, fin, *, hx=None, stepno=None, virtual=False):\n",
    "        # diff-able pass through the encoder to nudge it towards\n",
    "        #  task-meaningful abstractions! non-diffable pass yield poor\n",
    "        #  evaluation performance\n",
    "        Z = self.enc(obs).rsample([self.n_draws])\n",
    "        \n",
    "        # NO single-step foresight\n",
    "#         for zx in Z:  # for each draw run a shallow mcts\n",
    "#             path = [Node(0., zx, )]\n",
    "#             for a in range(self.a_dim):\n",
    "#                 pass\n",
    "#         # n_draws x T x B x ...\n",
    "#         for a_hyp in range(self.a_dim):\n",
    "#             Z_hyp = self.dyn(Z, torch.full(Z.shape[:3], a_hyp))\n",
    "#             value = self.baseline(Z_hyp).squeeze(-1)\n",
    "\n",
    "        # value must not have any trailing dims, i.e. T x B\n",
    "        logits = self.policy(Z).logsumexp(dim=0) - log(self.n_draws)\n",
    "        value = self.baseline(Z).mean(dim=0).squeeze(-1)\n",
    "\n",
    "        actions = multinomial(logits.detach().exp())\n",
    "\n",
    "        return actions, (), dict(value=value, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50038f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Node = namedtuple('Node', 'pior, zx, children')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870a94be",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30c29f8",
   "metadata": {},
   "source": [
    "### A2C algo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829b01a4",
   "metadata": {},
   "source": [
    "Service functions for the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0046223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.utils.plyr import apply, suply, xgetitem\n",
    "\n",
    "\n",
    "def timeshift(state, *, shift=1):\n",
    "    \"\"\"Get current and shfited slices of nested objects.\"\"\"\n",
    "    # use xgetitem to lett None through\n",
    "    # XXX `curr[t]` = (x_t, a_{t-1}, r_t, d_t), t=0..T-H\n",
    "    curr = suply(xgetitem, state, index=slice(None, -shift))\n",
    "\n",
    "    # XXX `next[t]` = (x_{t+H}, a_{t+H-1}, r_{t+H}, d_{t+H}), t=0..T-H\n",
    "    next = suply(xgetitem, state, index=slice(shift, None))\n",
    "\n",
    "    return curr, next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e112bf",
   "metadata": {},
   "source": [
    "The Advantage Actor-Critic algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b0309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from rlplay.algo.returns import pyt_vtrace\n",
    "\n",
    "# @torch.enable_grad()\n",
    "def vtrace(fragment, module, *, gamma=0.99, C_entropy=1e-2, C_value=0.5):\n",
    "    # REACT: (state[t], h_t) \\to (\\hat{a}_t, h_{t+1}, \\hat{A}_t)\n",
    "    _, _, info = module(\n",
    "        fragment.state.obs, fragment.state.act,\n",
    "        fragment.state.rew, fragment.state.fin,\n",
    "        hx=fragment.hx, stepno=fragment.state.stepno)\n",
    "\n",
    "    # Assume `.act` is unstructured: `act[t]` = a_{t+1} -->> T x B x 1\n",
    "    state, state_next = timeshift(fragment.state)\n",
    "    act = state_next.act.unsqueeze(-1)  # actions taken during the rollout\n",
    "\n",
    "    # \\pi is the target policy, \\mu is the behaviour policy (T+1 x B x ...)\n",
    "    log_pi, log_mu = info['logits'], fragment.actor['logits']\n",
    "\n",
    "    # the importance weights\n",
    "    log_pi_a = log_pi.gather(-1, act).squeeze(-1)\n",
    "    log_mu_a = log_mu.gather(-1, act).squeeze(-1)\n",
    "    log_rho = log_mu_a.sub_(log_pi_a.detach()).neg_()\n",
    "\n",
    "    # `.actor[t]` is actor's extra info in reaction to `.state[t]`, t=0..T\n",
    "    val = fragment.actor['value']  # info['value'].detach()\n",
    "    # XXX Although Esperholt et al. (2018, sec.~4.2) use the value estimate of\n",
    "    # the rollout policy for the V-trace target in eq. (1), it makes more sense\n",
    "    # to use the estimates of the current policy, as has been done in monobeast.\n",
    "    #  https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752\n",
    "    val, bootstrap = val[:-1], val[-1]\n",
    "    target = pyt_vtrace(state_next.rew, state_next.fin, val,\n",
    "                        gamma=gamma, bootstrap=bootstrap,\n",
    "                        omega=log_rho, r_bar=1., c_bar=1.)\n",
    "\n",
    "    # the critic's mse score against v-trace targets (min)\n",
    "    critic_mse = F.mse_loss(info['value'][:-1], target, reduction='mean') / 2\n",
    "\n",
    "    # \\delta_t = r_{t+1} + \\gamma \\nu(s_{t+1}) 1_{\\neg d_{t+1}} - v(s_t)\n",
    "    adv = torch.empty_like(state_next.rew).copy_(bootstrap)\n",
    "    adv[:-1].copy_(target[1:])  # copy the v-trace targets \\nu(s_{t+1})\n",
    "    adv.masked_fill_(state_next.fin, 0.).mul_(gamma)\n",
    "    adv.add_(state_next.rew).sub_(val)\n",
    "    # XXX note `val` here, not `target`! see sec.~4.2 in (Esperholt et al.; 2018)\n",
    "\n",
    "    # the policy surrogate score (max)\n",
    "    # \\rho_t = \\min\\{ \\bar{\\rho}, \\frac{\\pi_t(a_t)}{\\mu_t(a_t)} \\}\n",
    "    rho = log_rho.exp_().clamp_(max=1.)\n",
    "    vtrace_score = log_pi_a.mul(adv.mul_(rho)).mean()\n",
    "\n",
    "    # the policy's neg-entropy score (min)\n",
    "    f_min = torch.finfo(log_pi.dtype).min\n",
    "    negentropy = log_pi.exp().mul(log_pi.clamp(min=f_min)).sum(dim=-1).mean()\n",
    "\n",
    "    # maximize the entropy and the reinforce score, minimize the critic loss\n",
    "    objective = C_entropy * negentropy + C_value * critic_mse - vtrace_score\n",
    "    return objective.mean(), dict(\n",
    "        entropy=-float(negentropy),\n",
    "        policy_score=float(vtrace_score),\n",
    "        value_loss=float(critic_mse),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c964effc",
   "metadata": {},
   "source": [
    "The word model loss: the VAE loss and dynamic predicton loss of\n",
    "a World Model of [Ha and Schdmihuber (2018)](https://proceedings.neurips.cc/paper/2018/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html)\n",
    "* formal losses may not be as in the paper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79adafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def wm_loss(enc, dec, dyn, /, fragment, *, beta=1., n_draws=10, iwae=False):\n",
    "    info = {}\n",
    "\n",
    "    # ASSSUME obs and act are UNSTRUCTURED\n",
    "    state_curr, state_next = timeshift(fragment.state)\n",
    "\n",
    "    # get VAE loss and the encoding distribution q\n",
    "    loss_vae, enc_q, info['vae'] = vbayes(\n",
    "        enc, dec, fragment.state.obs,\n",
    "        beta=beta, n_draws=n_draws, iwae=iwae\n",
    "    )\n",
    "\n",
    "    loss_dyn = 0.\n",
    "    if dyn is not None:\n",
    "        # prepare the dynamics model (takes in action, mask, and recurrent state)\n",
    "        # XXX dyn(...) returns a distirbution and the hx update\n",
    "        dyn_ = lambda X: dyn(X, state_next.act, state_next.fin, hx=fragment.hx)[0]\n",
    "\n",
    "        # get r(z_{t+1} \\mid z_t, a_t)\n",
    "        Z = enc_q.sample()  # XXX non diff-able sampling!\n",
    "        loss_dyn, dyn_r, info['dyn'] = vbayes(\n",
    "            dyn_, dec, X=Z[:-1], Y=state_next.obs,\n",
    "            prior=dyn.prior, beta=1., n_draws=n_draws, iwae=False\n",
    "        )\n",
    "\n",
    "    return loss_vae + loss_dyn, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc87efe",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fe8c12",
   "metadata": {},
   "source": [
    "### Run!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842dc177",
   "metadata": {},
   "source": [
    "Initialize the learner and the environment factories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d06c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "factory_eval = partial(base_factory)\n",
    "factory = partial(base_factory)\n",
    "\n",
    "# learner = CartPoleActor(lstm='none')\n",
    "learner = WMCartPoleActor(n_draws=1)\n",
    "\n",
    "learner.train()\n",
    "device_ = torch.device('cpu')  # torch.device('cuda:0')\n",
    "learner.to(device=device_)\n",
    "\n",
    "optim = torch.optim.Adam(learner.parameters(), lr=1e-3)\n",
    "sched = None  # torch.optim.lr_scheduler.ReduceLROnPlateau(optim, mode='max', min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82730b1a",
   "metadata": {},
   "source": [
    "Initialize the sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121317e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "T, B = 25, 4\n",
    "\n",
    "sticky = learner.use_cudnn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ec9086f",
   "metadata": {},
   "source": [
    "from rlplay.engine.rollout import same\n",
    "\n",
    "batchit = same.rollout(\n",
    "    [factory() for _ in range(B)],\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    sticky=sticky,\n",
    "    device=device_,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c298875",
   "metadata": {},
   "source": [
    "from rlplay.engine.rollout import single\n",
    "\n",
    "batchit = single.rollout(\n",
    "    factory,\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    n_envs=B,\n",
    "    sticky=sticky,\n",
    "    clone=True,\n",
    "    close=False,\n",
    "    device=device_,\n",
    "    start_method='fork',  # fork in notebook for macos, spawn in linux\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bebc6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout import multi\n",
    "\n",
    "batchit = multi.rollout(\n",
    "    factory,\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    n_actors=6,\n",
    "    n_per_actor=B,\n",
    "    n_buffers=24,\n",
    "    n_per_batch=1,\n",
    "    sticky=sticky,\n",
    "    pinned=False,\n",
    "    clone=True,\n",
    "    close=False,\n",
    "    device=device_,\n",
    "    start_method='fork',  # fork in notebook for macos, spawn in linux\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9a0a4",
   "metadata": {},
   "source": [
    "A generator of evaluation rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f3229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout.evaluate import evaluate\n",
    "\n",
    "test_it = evaluate(factory_eval, learner, n_envs=4, n_steps=200,\n",
    "                   clone=False, device=device_, start_method='fork')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846d8c9",
   "metadata": {},
   "source": [
    "Implement your favourite training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ecff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "C_entropy, C_wm = 0.1, 0.1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3547d77",
   "metadata": {},
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aaba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from math import exp\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "losses, rewards = [], []\n",
    "for epoch in tqdm.tqdm(range(200)):\n",
    "    for j, batch in zip(range(100), batchit):\n",
    "        loss, info = vtrace(batch, learner, gamma=gamma, C_value=1., C_entropy=C_entropy)\n",
    "        loss_wm, info_ = wm_loss(\n",
    "            learner.enc, learner.dec, learner.dyn, batch,\n",
    "            beta=1., iwae=False, n_draws=1\n",
    "        )\n",
    "        info.update(info_)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        (loss + loss_wm * C_wm).backward()\n",
    "        grad = clip_grad_norm_(learner.parameters(), max_norm=1.0)\n",
    "        optim.step()\n",
    "\n",
    "        losses.append(dict(\n",
    "            **info, loss=float(loss), grad=float(grad),\n",
    "            C_entropy=C_entropy,\n",
    "            perplexity=exp(info['entropy']),\n",
    "        ))\n",
    "        \n",
    "#         if info['entropy'] * 1.5 < ent_target:\n",
    "#             C_entropy *= 2\n",
    "        \n",
    "#         elif info['entropy'] > ent_target * 1.5:\n",
    "#             C_entropy /= 2\n",
    "\n",
    "    # fetch the evaluation results lagged by one inner loop!\n",
    "    rewards.append(next(test_it))\n",
    "    if sched is not None:\n",
    "        sched.step(rewards[-1].mean())\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e377c77",
   "metadata": {},
   "source": [
    "plt.plot(burnin['vae'])\n",
    "log_p = learner.dec(learner.enc(batch.state.obs).sample()).log_prob(batch.state.obs)\n",
    "log_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c295b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0633c36c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# close the generators\n",
    "batchit.close()\n",
    "test_it.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62079579",
   "metadata": {},
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce115d82",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k: numpy.array(v) for k, v in collate(losses).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'value_loss' in data:\n",
    "    plt.semilogy(data['value_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d8f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'entropy' in data:\n",
    "    plt.plot(data['entropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2d49a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'policy_score' in data:\n",
    "    plt.plot(data['policy_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01581b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'grad' in data:\n",
    "    plt.semilogy(data['grad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b06fce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'vae' in data:\n",
    "    plt.plot(data['vae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9329b1a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'dyn' in data:\n",
    "    plt.plot(data['dyn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3128f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = numpy.stack(rewards, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ffd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd6ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = numpy.median(rewards, axis=-1), rewards.std(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5545163",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fi, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "\n",
    "ax.plot(numpy.mean(rewards, axis=-1))\n",
    "ax.plot(numpy.median(rewards, axis=-1))\n",
    "ax.plot(numpy.min(rewards, axis=-1))\n",
    "ax.plot(numpy.std(rewards, axis=-1))\n",
    "# ax.plot(m+s * 1.96)\n",
    "# ax.plot(m-s * 1.96)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9856d6a4",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace0ac8",
   "metadata": {},
   "source": [
    "The ultimate evaluation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d372a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from rlplay.engine import core\n",
    "\n",
    "with factory_eval() as env:\n",
    "    learner.eval()\n",
    "    eval_rewards, info = core.evaluate([\n",
    "        env.env\n",
    "    ], learner, render=True, n_steps=1e4, device=device_)\n",
    "\n",
    "print(sum(eval_rewards))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "01ec89e5",
   "metadata": {},
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade2f90c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df22ff",
   "metadata": {},
   "source": [
    "Let's analyze the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d924c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.algo.returns import npy_returns, npy_deltas\n",
    "\n",
    "td_target = eval_rewards + gamma * info['value'][1:]\n",
    "td_error = td_target - info['value'][:-1]\n",
    "# td_error = npy_deltas(\n",
    "#     eval_rewards, numpy.zeros_like(eval_rewards, dtype=bool), info['value'][:-1],\n",
    "#     gamma=gamma, bootstrap=info['value'][-1])\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "ax.semilogy(abs(td_error) / abs(td_target))\n",
    "ax.set_title('relative td(1)-error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef93ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.algo.returns import npy_returns, npy_deltas\n",
    "\n",
    "# plt.plot(\n",
    "#     npy_returns(eval_rewards, numpy.zeros_like(eval_rewards, dtype=bool),\n",
    "#                 gamma=gamma, bootstrap=info['value'][-1]))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "ax.plot(info['value']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26837bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.special import softmax, expit, entr\n",
    "\n",
    "*head, n_actions = info['logits'].shape\n",
    "proba = softmax(info['logits'], axis=-1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "ax.plot(entr(proba).sum(-1)[:, 0])\n",
    "ax.axhline(math.log(n_actions), c='k', alpha=0.5, lw=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "ax.hist(info['logits'][..., 1] - info['logits'][..., 0], bins=51);  # log-ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4cdeca",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f523576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = learner.enc(batch.state.obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff46c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pi_raw = learner.policy(q.rsample([100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c5e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pi = log_pi_raw.logsumexp(dim=0) - log(log_pi_raw.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ffd756",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a4c16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa969c81",
   "metadata": {},
   "source": [
    "$$\n",
    "y = \\bigvee_k\n",
    "  \\neg \\bigl(\n",
    "    \\bigvee_i (\n",
    "      \\neg x_i \\wedge w_{i k}\n",
    "    )\n",
    "  \\bigr) \\wedge 1_k\n",
    "  = \\bigvee_k \\bigwedge_i (\n",
    "      x_i \\vee \\neg w_{i k}\n",
    "    )\n",
    "  \\,.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68eeff0",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
