{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "830df33b",
   "metadata": {},
   "source": [
    "# `rlplay`-ing around with Policy Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718fe01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "\n",
    "# hotfix for gym's unresponsive viz (spawns gl threads!)\n",
    "import rlplay.utils.integration.gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f2682",
   "metadata": {},
   "source": [
    "See example.ipynb for the overview of `rlplay`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90b222",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de9187",
   "metadata": {},
   "source": [
    "## Sophisticated CartPole with PG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f92f1c9",
   "metadata": {},
   "source": [
    "### The environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390dfcb3",
   "metadata": {},
   "source": [
    "The environment factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.zoo.env import NarrowPath\n",
    "\n",
    "\n",
    "class FP32Observation(gym.ObservationWrapper):\n",
    "    def observation(self, observation):\n",
    "        obs = observation.astype(numpy.float32)\n",
    "        obs[0] = 0.  # mask the position info\n",
    "        return obs\n",
    "\n",
    "#     def step(self, action):\n",
    "#         obs, reward, done, info = super().step(action)\n",
    "#         reward -= abs(obs[1]) / 10  # punish for non-zero speed\n",
    "#         return obs, reward, done, info\n",
    "\n",
    "class OneHotObservation(gym.ObservationWrapper):\n",
    "    def observation(self, observation):\n",
    "        return numpy.eye(1, self.env.observation_space.n,\n",
    "                         k=observation, dtype=numpy.float32)[0]\n",
    "\n",
    "def base_factory(seed=None):\n",
    "    # return gym.make(\"LunarLander-v2\")\n",
    "    return FP32Observation(gym.make(\"CartPole-v0\").unwrapped)\n",
    "    # return OneHotObservation(NarrowPath())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8b9ac",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8bf14",
   "metadata": {},
   "source": [
    "### the Actor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442a90a2",
   "metadata": {},
   "source": [
    "A procedure and a layer, which converts the input integer data into its\n",
    "little-endian binary representation as float $\\{0, 1\\}^m$ vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotbits(input, n_bits=63, dtype=torch.float):\n",
    "    \"\"\"Encode integers to fixed-width binary floating point vectors\"\"\"\n",
    "    assert not input.dtype.is_floating_point\n",
    "    assert 0 < n_bits < 64  # torch.int64 is signed, so 64-1 bits max\n",
    "\n",
    "    # n_bits = {torch.int64: 63, torch.int32: 31, torch.int16: 15, torch.int8 : 7}\n",
    "\n",
    "    # get mask of set bits\n",
    "    pow2 = torch.tensor([1 << j for j in range(n_bits)]).to(input.device)\n",
    "    x = input.unsqueeze(-1).bitwise_and(pow2).to(bool)\n",
    "\n",
    "    # upcast bool to float to get one-hot\n",
    "    return x.to(dtype)\n",
    "\n",
    "\n",
    "class OneHotBits(torch.nn.Module):\n",
    "    def __init__(self, n_bits=63, dtype=torch.float):\n",
    "        assert 1 <= n_bits < 64\n",
    "        super().__init__()\n",
    "        self.n_bits, self.dtype = n_bits, dtype\n",
    "\n",
    "    def forward(self, input):\n",
    "        return onehotbits(input, n_bits=self.n_bits, dtype=self.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984f850",
   "metadata": {},
   "source": [
    "A special module dictionary, which applies itself to the input dict of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f3fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Mapping\n",
    "from torch.nn import Module, ModuleDict as BaseModuleDict\n",
    "\n",
    "\n",
    "class ModuleDict(BaseModuleDict):\n",
    "    \"\"\"The ModuleDict, that applies itself to the input dicts.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        modules: Optional[Mapping[str, Module]] = None,\n",
    "        dim: Optional[int]=-1\n",
    "    ) -> None:\n",
    "        super().__init__(modules)\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        # enforce concatenation in the order of the declaration in  __init__\n",
    "        return torch.cat([\n",
    "            m(input[k]) for k, m in self.items()\n",
    "        ], dim=self.dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c291c5",
   "metadata": {},
   "source": [
    "An $\\ell_2$ normalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a11c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import normalize\n",
    "\n",
    "class Normalize(torch.nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        return normalize(input, dim=self.dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b945a330",
   "metadata": {},
   "source": [
    "A more sophisticated policy learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf4300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine import BaseActorModule\n",
    "from rlplay.utils.common import multinomial\n",
    "\n",
    "from torch.nn import Sequential, Linear, ReLU, LogSoftmax\n",
    "\n",
    "class CartPoleActor(BaseActorModule):\n",
    "    def __init__(self, lstm='none'):\n",
    "        assert lstm in ('none', 'loop', 'cudnn')\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_lstm = self.use_cudnn = False\n",
    "\n",
    "        # blend the policy with a uniform distribution, determined by\n",
    "        #  the exploration epsilon. We update it in the actor clones via a buffer\n",
    "        # self.register_buffer('epsilon', torch.tensor(epsilon))\n",
    "        # XXX isn't the stochastic policy random enough by itself?\n",
    "\n",
    "        self.baseline = Sequential(\n",
    "            Linear(4, 20),\n",
    "            ReLU(),\n",
    "            Linear(20, 1),\n",
    "        )\n",
    "        self.policy = Sequential(\n",
    "            Linear(4, 20),\n",
    "            ReLU(),\n",
    "            Linear(20, 2),\n",
    "            LogSoftmax(dim=-1),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, act, rew, fin, *, hx=None, stepno=None, virtual=False):\n",
    "        # value must not have any trailing dims, i.e. T x B\n",
    "        logits = self.policy(obs)\n",
    "        value = self.baseline(obs).squeeze(-1)\n",
    "\n",
    "        if not self.training:\n",
    "            actions = logits.argmax(dim=-1)\n",
    "\n",
    "        else:\n",
    "            actions = multinomial(logits.detach().exp())\n",
    "\n",
    "        return actions, (), dict(value=value, logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870a94be",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30c29f8",
   "metadata": {},
   "source": [
    "### PPO/GAE A2C and V-trace A2C algos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829b01a4",
   "metadata": {},
   "source": [
    "Service functions for the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0046223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plyr import apply, suply, xgetitem\n",
    "\n",
    "\n",
    "def timeshift(state, *, shift=1):\n",
    "    \"\"\"Get current and shifted slices of nested objects.\"\"\"\n",
    "    # use `xgetitem` to let None through\n",
    "    # XXX `curr[t]` = (x_t, a_{t-1}, r_t, d_t), t=0..T-H\n",
    "    curr = suply(xgetitem, state, index=slice(None, -shift))\n",
    "\n",
    "    # XXX `next[t]` = (x_{t+H}, a_{t+H-1}, r_{t+H}, d_{t+H}), t=0..T-H\n",
    "    next = suply(xgetitem, state, index=slice(shift, None))\n",
    "\n",
    "    return curr, next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e112bf",
   "metadata": {},
   "source": [
    "The Advantage Actor-Critic algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013f8fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from rlplay.algo.returns import pyt_gae, pyt_returns, pyt_multistep\n",
    "\n",
    "# @torch.enable_grad()\n",
    "def a2c(\n",
    "    fragment, module, *, gamma=0.99, gae=1., ppo=0.,\n",
    "    C_entropy=1e-2, C_value=0.5, c_rho=1.0, multistep=0,\n",
    "):\n",
    "    r\"\"\"The Advantage Actor-Critic algorithm (importance-weighted off-policy).\n",
    "\n",
    "    Close to REINFORCE, but uses separate baseline value estimate to compute\n",
    "    advantages in the policy gradient:\n",
    "    $$\n",
    "        \\nabla_\\theta J(s_t)\n",
    "            = \\mathbb{E}_{a \\sim \\beta(a\\mid s_t)}\n",
    "                \\frac{\\pi(a\\mid s_t)}{\\beta(a\\mid s_t)}\n",
    "                    \\bigl( r_{t+1} + \\gamma G_{t+1} - v(s_t) \\bigr)\n",
    "                \\nabla_\\theta \\log \\pi(a\\mid s_t)\n",
    "        \\,, $$\n",
    "\n",
    "    where the critic estimates the state's value under the current policy\n",
    "    $$\n",
    "        v(s_t)\n",
    "            \\approx \\mathbb{E}_{\\pi_{\\geq t}}\n",
    "                G_t(a_t, s_{t+1}, a_{t+1}, ... \\mid s_t)\n",
    "        \\,. $$\n",
    "    \"\"\"\n",
    "    state, state_next = timeshift(fragment.state)\n",
    "\n",
    "    # REACT: (state[t], h_t) \\to (\\hat{a}_t, h_{t+1}, \\hat{A}_t)\n",
    "    _, _, info = module(\n",
    "        state.obs, state.act, state.rew, state.fin,\n",
    "        hx=fragment.hx, stepno=state.stepno)\n",
    "    # info['value'] = V(`.state[t]`)\n",
    "    #               <<-->> v(x_t)\n",
    "    #               \\approx \\mathbb{E}( G_t \\mid x_t)\n",
    "    #               \\approx \\mathbb{E}( r_{t+1} + \\gamma r_{t+2} + ... \\mid x_t)\n",
    "    #               <<-->> npv(`.state[t+1:]`)\n",
    "    # info['logits'] = \\log \\pi(... | .state[t] )\n",
    "    #                <<-->> \\log \\pi( \\cdot \\mid x_t)\n",
    "\n",
    "    # `.actor[t]` is actor's extra info in reaction to `.state[t]`, t=0..T\n",
    "    bootstrap = fragment.actor['value'][-1]\n",
    "    #     `bootstrap` <<-->> `.value[-1]` = V(`.state[-1]`)\n",
    "\n",
    "    # XXX post-mul by `1 - \\gamma` fails to train, but seems appropriate\n",
    "    # for the continuation/survival interpretation of the discount factor.\n",
    "    #   <<-- but who says this is a good interpretation?\n",
    "    # ret.mul_(1 - gamma)\n",
    "\n",
    "    # \\pi is the target policy, \\mu is the behaviour policy\n",
    "    log_pi, log_mu = info['logits'], fragment.actor['logits']\n",
    "\n",
    "    # Future rewards after `.state[t]` are recorded in `.state[t+1:]`\n",
    "    #  G_t <<-->> ret[t] = rew[t] + gamma * (1 - fin[t]) * (ret[t+1] or bootstrap)\n",
    "    if multistep > 0:\n",
    "        ret = pyt_multistep(state_next.rew, state_next.fin,\n",
    "                            info['value'].detach(),\n",
    "                            gamma=gamma, n_lookahead=multistep,\n",
    "                            bootstrap=bootstrap.unsqueeze(0))\n",
    "\n",
    "    else:\n",
    "        ret = pyt_returns(state_next.rew, state_next.fin,\n",
    "                          gamma=gamma, bootstrap=bootstrap)\n",
    "\n",
    "    # the critic's mse score (min)\n",
    "    #  \\frac1{2 T} \\sum_t (G_t - v(s_t))^2\n",
    "    value = info['value']\n",
    "    critic_mse = F.mse_loss(value, ret, reduction='mean') / 2\n",
    "    # v(x_t) \\approx \\mathbb{E}( G_t \\mid x_t )\n",
    "    #        \\approx G_t (one-point estimate)\n",
    "    #        <<-->> ret[t]\n",
    "\n",
    "    # compute the advantages $G_t - v(s_t)$\n",
    "    #  or GAE [Schulman et al. (2016)](http://arxiv.org/abs/1506.02438)\n",
    "    # XXX sec 6.1 in the GAE paper uses V from the `current` value\n",
    "    #  network, not the one used during the rollout.\n",
    "    # value = fragment.actor['value'][:-1]\n",
    "    if gae < 1.:\n",
    "        # the positional arguments are $r_{t+1}$, $d_{t+1}$, and $v(s_t)$,\n",
    "        #  respectively, for $t=0..T-1$. The bootstrap is $v(S_T)$ from\n",
    "        #  the rollout.\n",
    "        adv = pyt_gae(state_next.rew, state_next.fin, value.detach(),\n",
    "                      gamma=gamma, C=gae, bootstrap=bootstrap)\n",
    "\n",
    "    else:\n",
    "        adv = ret.sub(value.detach())\n",
    "\n",
    "    # adv.sub_(adv.mean())\n",
    "    # adv.div_(adv.std(dim=0))\n",
    "\n",
    "    # Assume `.act` is unstructured: `act[t]` = a_{t+1} -->> T x B x 1\n",
    "    act = state_next.act.unsqueeze(-1)  # actions taken during the rollout\n",
    "\n",
    "    # the importance weights\n",
    "    log_pi_a = log_pi.gather(-1, act).squeeze(-1)\n",
    "    log_mu_a = log_mu.gather(-1, act).squeeze(-1)\n",
    "\n",
    "    # the policy surrogate score (max)\n",
    "    if ppo > 0:\n",
    "        # the PPO loss is the properly clipped rho times the advantage\n",
    "        ratio = log_pi_a.sub(log_mu_a).exp()        \n",
    "        a2c_score = torch.min(\n",
    "            ratio * adv,\n",
    "            ratio.clamp(1. - ppo, 1. + ppo) * adv\n",
    "        ).mean()\n",
    "\n",
    "    else:\n",
    "        # \\exp{- ( \\log \\mu - \\log \\pi )}, evaluated at $a_t \\mid z_t$\n",
    "        rho = log_mu_a.sub_(log_pi_a.detach()).neg_()\\\n",
    "                      .exp_().clamp_(max=c_rho)\n",
    "\n",
    "        # \\frac1T \\sum_t \\rho_t (G_t - v_t) \\log \\pi(a_t \\mid z_t)\n",
    "        a2c_score = log_pi_a.mul(adv.mul_(rho)).mean()\n",
    "\n",
    "    # the policy's neg-entropy score (min)\n",
    "    #   - H(\\pi(•\\mid s)) = - (-1) \\sum_a \\pi(a\\mid s) \\log \\pi(a\\mid s)\n",
    "    f_min = torch.finfo(log_pi.dtype).min\n",
    "    negentropy = log_pi.exp().mul(log_pi.clamp(min=f_min)).sum(dim=-1).mean()\n",
    "\n",
    "    # breakpoint()\n",
    "\n",
    "    # maximize the entropy and the reinforce score, minimize the critic loss\n",
    "    objective = C_entropy * negentropy + C_value * critic_mse - a2c_score\n",
    "    return objective.mean(), dict(\n",
    "        entropy=-float(negentropy),\n",
    "        policy_score=float(a2c_score),\n",
    "        value_loss=float(critic_mse),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9466d073",
   "metadata": {},
   "source": [
    "A couple of three things:\n",
    "* a2c is on-policy and no importance weight could change this!\n",
    "* L72-80: [stable_baselines3](./common/on_policy_algorithm.py#L183-192)\n",
    "  and [rlpyt](./algos/pg/base.py#L49-58) use rollout data, when computing the GAE\n",
    "\n",
    "* L61-62: [stable_baselines3](./stable_baselines3/a2c/a2c.py#L147-156) uses `vf_coef=0.5`,\n",
    "  and **unhalved** `F.mse-loss`, while [rlpyt](./rlpyt/rlpyt/algos/pg/a2c.py#L93-94)\n",
    "  uses `value_loss_coeff=0.5`, and **halved** $\\ell_2$ loss!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e4c1c",
   "metadata": {},
   "source": [
    "The off-policy actor-critic algorithm for the learner, called V-trace,\n",
    "from [Espeholt et al. (2018)](http://proceedings.mlr.press/v80/espeholt18a.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a2d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.algo.returns import pyt_vtrace\n",
    "\n",
    "# @torch.enable_grad()\n",
    "def vtrace(fragment, module, *, gamma=0.99, C_entropy=1e-2, C_value=0.5):\n",
    "    # REACT: (state[t], h_t) \\to (\\hat{a}_t, h_{t+1}, \\hat{A}_t)\n",
    "    _, _, info = module(\n",
    "        fragment.state.obs, fragment.state.act,\n",
    "        fragment.state.rew, fragment.state.fin,\n",
    "        hx=fragment.hx, stepno=fragment.state.stepno)\n",
    "\n",
    "    # Assume `.act` is unstructured: `act[t]` = a_{t+1} -->> T x B x 1\n",
    "    state, state_next = timeshift(fragment.state)\n",
    "    act = state_next.act.unsqueeze(-1)  # actions taken during the rollout\n",
    "\n",
    "    # \\pi is the target policy, \\mu is the behaviour policy (T+1 x B x ...)\n",
    "    log_pi, log_mu = info['logits'], fragment.actor['logits']\n",
    "\n",
    "    # the importance weights\n",
    "    log_pi_a = log_pi.gather(-1, act).squeeze(-1)\n",
    "    log_mu_a = log_mu.gather(-1, act).squeeze(-1)\n",
    "    log_rho = log_mu_a.sub_(log_pi_a.detach()).neg_()\n",
    "\n",
    "    # `.actor[t]` is actor's extra info in reaction to `.state[t]`, t=0..T\n",
    "    val = fragment.actor['value']  # info['value'].detach()\n",
    "    # XXX Although Esperholt et al. (2018, sec.~4.2) use the value estimate of\n",
    "    # the rollout policy for the V-trace target in eq. (1), it makes more sense\n",
    "    # to use the estimates of the current policy, as has been done in monobeast.\n",
    "    #  https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752\n",
    "    val, bootstrap = val[:-1], val[-1]\n",
    "    target = pyt_vtrace(state_next.rew, state_next.fin, val,\n",
    "                        gamma=gamma, bootstrap=bootstrap,\n",
    "                        omega=log_rho, r_bar=1., c_bar=1.)\n",
    "\n",
    "    # the critic's mse score against v-trace targets (min)\n",
    "    critic_mse = F.mse_loss(info['value'][:-1], target, reduction='mean') / 2\n",
    "\n",
    "    # \\delta_t = r_{t+1} + \\gamma \\nu(s_{t+1}) 1_{\\neg d_{t+1}} - v(s_t)\n",
    "    adv = torch.empty_like(state_next.rew).copy_(bootstrap)\n",
    "    adv[:-1].copy_(target[1:])  # copy the v-trace targets \\nu(s_{t+1})\n",
    "    adv.masked_fill_(state_next.fin, 0.).mul_(gamma)\n",
    "    adv.add_(state_next.rew).sub_(val)\n",
    "    # XXX note `val` here, not `target`! see sec.~4.2 in (Esperholt et al.; 2018)\n",
    "\n",
    "    # the policy surrogate score (max)\n",
    "    # \\rho_t = \\min\\{ \\bar{\\rho}, \\frac{\\pi_t(a_t)}{\\mu_t(a_t)} \\}\n",
    "    rho = log_rho.exp_().clamp_(max=1.)\n",
    "    vtrace_score = log_pi_a.mul(adv.mul_(rho)).mean()\n",
    "\n",
    "    # the policy's neg-entropy score (min)\n",
    "    f_min = torch.finfo(log_pi.dtype).min\n",
    "    negentropy = log_pi.exp().mul(log_pi.clamp(min=f_min)).sum(dim=-1).mean()\n",
    "\n",
    "    # maximize the entropy and the reinforce score, minimize the critic loss\n",
    "    objective = C_entropy * negentropy + C_value * critic_mse - vtrace_score\n",
    "    return objective.mean(), dict(\n",
    "        entropy=-float(negentropy),\n",
    "        policy_score=float(vtrace_score),\n",
    "        value_loss=float(critic_mse),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc87efe",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fe8c12",
   "metadata": {},
   "source": [
    "### Run!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842dc177",
   "metadata": {},
   "source": [
    "Initialize the learner and the environment factories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d06c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "factory_eval = partial(base_factory)\n",
    "factory = partial(base_factory)\n",
    "\n",
    "learner = CartPoleActor(lstm='none')\n",
    "\n",
    "learner.train()\n",
    "device_ = torch.device('cpu')  # torch.device('cuda:0')\n",
    "learner.to(device=device_)\n",
    "\n",
    "optim = torch.optim.Adam(learner.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82730b1a",
   "metadata": {},
   "source": [
    "Initialize the sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991c289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "T, B = 25, 4\n",
    "\n",
    "sticky = learner.use_cudnn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f17902fe",
   "metadata": {},
   "source": [
    "from rlplay.engine.rollout import same\n",
    "\n",
    "batchit = same.rollout(\n",
    "    [factory() for _ in range(B)],\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    sticky=sticky,\n",
    "    device=device_,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c298875",
   "metadata": {},
   "source": [
    "from rlplay.engine.rollout import single\n",
    "\n",
    "batchit = single.rollout(\n",
    "    factory,\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    n_envs=B,\n",
    "    sticky=sticky,\n",
    "    clone=True,\n",
    "    close=False,\n",
    "    device=device_,\n",
    "    start_method='fork',  # fork in notebook for macos, spawn in linux\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60ab21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout import multi\n",
    "\n",
    "batchit = multi.rollout(\n",
    "    factory,\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    n_actors=6,\n",
    "    n_per_actor=B,\n",
    "    n_buffers=15,\n",
    "    n_per_batch=2,\n",
    "    sticky=sticky,\n",
    "    pinned=False,\n",
    "    clone=True,\n",
    "    close=False,\n",
    "    device=device_,\n",
    "    start_method='fork',  # fork in notebook for macos, spawn in linux\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9a0a4",
   "metadata": {},
   "source": [
    "A generator of evaluation rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f3229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout.evaluate import evaluate\n",
    "\n",
    "test_it = evaluate(factory_eval, learner, n_envs=4, n_steps=500,\n",
    "                   clone=False, device=device_, start_method='fork')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846d8c9",
   "metadata": {},
   "source": [
    "Implement your favourite training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ecff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "use_vtrace = True\n",
    "\n",
    "# gamma, gae, ppo = 0.99, 0.92, 0.2\n",
    "gamma, gae, ppo, multistep = 0.99, 1., 0.2, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ab50a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "losses, rewards = [], []\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    for j, batch in zip(range(100), batchit):\n",
    "        if use_vtrace:\n",
    "            loss, info = vtrace(batch, learner, gamma=gamma)\n",
    "\n",
    "        else:\n",
    "            loss, info = a2c(batch, learner, gamma=gamma, gae=gae, ppo=ppo, multistep=multistep)\n",
    "        \n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        grad = clip_grad_norm_(learner.parameters(), max_norm=1.0)\n",
    "        optim.step()\n",
    "\n",
    "        losses.append(dict(\n",
    "            loss=float(loss), grad=float(grad), **info\n",
    "        ))\n",
    "\n",
    "    # fetch the evaluation results lagged by one inner loop!\n",
    "    rewards.append(next(test_it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0633c36c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# close the generators\n",
    "batchit.close()\n",
    "test_it.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05c0efb2",
   "metadata": {},
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce115d82",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(records):\n",
    "    \"\"\"collate identically keyed dicts\"\"\"\n",
    "    out, n_records = {}, 0\n",
    "    for record in records:\n",
    "        for k, v in record.items():\n",
    "            out.setdefault(k, []).append(v)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "data = {k: numpy.array(v) for k, v in collate(losses).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'value_loss' in data:\n",
    "    plt.semilogy(data['value_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d8f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'entropy' in data:\n",
    "    plt.plot(data['entropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2d49a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'policy_score' in data:\n",
    "    plt.plot(data['policy_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01581b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(data['grad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3128f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = numpy.stack(rewards, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ffd0b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd6ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = numpy.median(rewards, axis=-1), rewards.std(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5545163",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fi, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "\n",
    "ax.plot(numpy.mean(rewards, axis=-1))\n",
    "ax.plot(numpy.median(rewards, axis=-1))\n",
    "ax.plot(numpy.min(rewards, axis=-1))\n",
    "ax.plot(numpy.std(rewards, axis=-1))\n",
    "# ax.plot(m+s * 1.96)\n",
    "# ax.plot(m-s * 1.96)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9856d6a4",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace0ac8",
   "metadata": {},
   "source": [
    "The ultimate evaluation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d372a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from rlplay.engine import core\n",
    "\n",
    "with factory_eval() as env:\n",
    "    learner.eval()\n",
    "    eval_rewards, info = core.evaluate([\n",
    "        env\n",
    "    ], learner, render=True, n_steps=1e4, device=device_)\n",
    "\n",
    "print(sum(eval_rewards))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4716caad",
   "metadata": {},
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade2f90c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df22ff",
   "metadata": {},
   "source": [
    "Let's analyze the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d924c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.algo.returns import npy_returns, npy_deltas\n",
    "\n",
    "td_target = eval_rewards + gamma * info['value'][1:]\n",
    "td_error = td_target - info['value'][:-1]\n",
    "# td_error = npy_deltas(\n",
    "#     eval_rewards, numpy.zeros_like(eval_rewards, dtype=bool), info['value'][:-1],\n",
    "#     gamma=gamma, bootstrap=info['value'][-1])\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "ax.semilogy(abs(td_error) / abs(td_target))\n",
    "ax.set_title('relative td(1)-error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef93ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.algo.returns import npy_returns, npy_deltas\n",
    "\n",
    "# plt.plot(\n",
    "#     npy_returns(eval_rewards, numpy.zeros_like(eval_rewards, dtype=bool),\n",
    "#                 gamma=gamma, bootstrap=info['value'][-1]))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "ax.plot(info['value'])\n",
    "ax.axhline(1 / (1 - gamma), c='k', alpha=0.5, lw=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26837bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.special import softmax, expit, entr\n",
    "\n",
    "*head, n_actions = info['logits'].shape\n",
    "proba = softmax(info['logits'], axis=-1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "ax.plot(entr(proba).sum(-1)[:, 0])\n",
    "ax.axhline(math.log(n_actions), c='k', alpha=0.5, lw=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "ax.hist(info['logits'][..., 1] - info['logits'][..., 0], bins=51);  # log-ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4cdeca",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68eeff0",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa2581",
   "metadata": {},
   "source": [
    "### Other agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78ac23e",
   "metadata": {},
   "source": [
    "An agent that uses other inputs, beside `obs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleActor(BaseActorModule):\n",
    "    def __init__(self, epsilon=0.1, lstm='none'):\n",
    "        assert lstm in ('none', 'loop', 'cudnn')\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_lstm = (lstm != 'none')\n",
    "        self.use_cudnn = (lstm == 'cudnn')\n",
    "\n",
    "        # for updating the exploration epsilon in the actor clones\n",
    "        self.register_buffer('epsilon', torch.tensor(epsilon))\n",
    "\n",
    "        # the features\n",
    "        n_output_dim = dict(obs=64, act=8, stepno=0)\n",
    "        self.features = torch.nn.Sequential(\n",
    "            ModuleDict(dict(\n",
    "                obs=Linear(4, n_output_dim['obs']),\n",
    "\n",
    "                act=Embedding(2, n_output_dim['act']),\n",
    "\n",
    "                stepno=Sequential(\n",
    "                    OneHotBits(32),\n",
    "                    Linear(32, n_output_dim['stepno']),\n",
    "                ),\n",
    "            )),\n",
    "            ReLU(),\n",
    "        )\n",
    "\n",
    "        # the core\n",
    "        n_features = sum(n_output_dim.values())\n",
    "        if self.use_lstm:\n",
    "            self.core = LSTM(n_features, 64, 1)\n",
    "\n",
    "        else:\n",
    "            self.core = Sequential(\n",
    "                Linear(n_features, 64, bias=True),\n",
    "                ReLU(),\n",
    "            )\n",
    "    \n",
    "        # the rest of the actor's model\n",
    "        self.baseline = Linear(64, 1)\n",
    "        self.policy = Sequential(\n",
    "            Linear(64, 2),\n",
    "            LogSoftmax(dim=-1),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, act, rew, fin, *, hx=None, stepno=None, virtual=False):\n",
    "        # Everything is  [T x B x ...]\n",
    "        input = self.features(locals())\n",
    "\n",
    "        # `input` is T x B x F, `hx` is either `None`, or a proper recurrent state\n",
    "        n_steps, n_envs, *_ = fin.shape\n",
    "        if not self.use_lstm:\n",
    "            # update `hx` into an empty container\n",
    "            out, hx = self.core(input), ()\n",
    "\n",
    "        elif not self.use_cudnn:\n",
    "            outputs = []\n",
    "            for x, m in zip(input.unsqueeze(1), ~fin.unsqueeze(-1)):\n",
    "                # `m` indicates if NO reset took place, otherwise\n",
    "                #  multiply by zero to stop the grads\n",
    "                if hx is not None:\n",
    "                    hx = suply(m.mul, hx)\n",
    "        \n",
    "                # one LSTM step [1 x B x ...]\n",
    "                output, hx = self.core(x, hx)\n",
    "                outputs.append(output)\n",
    "\n",
    "            # compile the output\n",
    "            out = torch.cat(outputs, dim=0)\n",
    "\n",
    "        else:\n",
    "            # sequence padding (MUST have sampling with `sticky=True`)\n",
    "            if n_steps > 1:\n",
    "                lengths = 1 + (~fin[1:]).sum(0).cpu()\n",
    "                input = pack_padded_sequence(input, lengths, enforce_sorted=False)\n",
    "\n",
    "            out, hx = self.core(input, hx)\n",
    "            if n_steps > 1:\n",
    "                out, lens = pad_packed_sequence(\n",
    "                    out, batch_first=False, total_length=n_steps)\n",
    "\n",
    "        # apply relu after the core and get the policy\n",
    "        logits = self.policy(out)\n",
    "\n",
    "        # value must not have any trailing dims, i.e. T x B\n",
    "        value = self.baseline(out).squeeze(-1)\n",
    "\n",
    "        if not self.training:\n",
    "            actions = logits.argmax(dim=-1)\n",
    "\n",
    "        else:\n",
    "            # blend the policy with a uniform distribution\n",
    "            prob = logits.detach().exp().mul_(1 - self.epsilon)\n",
    "            prob.add_(self.epsilon / logits.shape[-1])\n",
    "\n",
    "            actions = multinomial(prob)\n",
    "\n",
    "        return actions, hx, dict(value=value, logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4589b5f2",
   "metadata": {},
   "source": [
    "A non-recurrent actor with features shared between the policy and the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b035b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleActor(BaseActorModule):\n",
    "    def __init__(self, epsilon=0.1, lstm='none'):\n",
    "        assert lstm in ('none', 'loop', 'cudnn')\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_lstm = self.use_cudnn = False\n",
    "\n",
    "        # for updating the exploration epsilon in the actor clones\n",
    "        self.register_buffer('epsilon', torch.tensor(epsilon))\n",
    "\n",
    "        # the features\n",
    "        self.features = Sequential(\n",
    "            Linear(4, 20),\n",
    "            ReLU(),\n",
    "        )\n",
    "\n",
    "        self.baseline = Linear(20, 1)\n",
    "        self.policy = Sequential(\n",
    "            Linear(20, 2),\n",
    "            LogSoftmax(dim=-1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, obs, act, rew, fin, *, hx=None, stepno=None, virtual=False):\n",
    "        x = self.features(obs)\n",
    "\n",
    "        # value must not have any trailing dims, i.e. T x B\n",
    "        logits = self.policy(x)\n",
    "        value = self.baseline(x).squeeze(-1)\n",
    "\n",
    "        if not self.training:\n",
    "            actions = logits.argmax(dim=-1)\n",
    "\n",
    "        else:\n",
    "            # blend the policy with a uniform distribution\n",
    "            prob = logits.detach().exp().mul_(1 - self.epsilon)\n",
    "            prob.add_(self.epsilon / logits.shape[-1])\n",
    "\n",
    "            actions = multinomial(prob)\n",
    "\n",
    "        return actions, (), dict(value=value, logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2e371c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d55f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stepno = batch.state.stepno\n",
    "stepno = torch.arange(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36db6856",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = learner.features[0]['stepno'](stepno)\n",
    "\n",
    "    out = F.linear(F.relu(out), learner.core[1].weight[:, -8:],\n",
    "                       bias=learner.core[1].bias)\n",
    "#     out = F.linear(F.relu(out), learner.core.weight_ih_l0[:, -8:],\n",
    "#                        bias=learner.core.bias_ih_l0)\n",
    "#     out = F.relu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df99b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(8, 8), dpi=200,\n",
    "                         sharex=True, sharey=True)\n",
    "\n",
    "for j, ax in zip(range(out.shape[1]), axes.flat):\n",
    "    ax.plot(out[:, j], lw=1)\n",
    "\n",
    "fig.tight_layout(pad=0, h_pad=0, w_pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f9e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    plt.imshow(abs(learner.core[1].weight[:, -8:]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff1362",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = learner.features.stepno[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33725897",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    plt.imshow(abs(lin.weight))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
