{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "830df33b",
   "metadata": {},
   "source": [
    "# `rlplay`-ing around with the Double-Dee QN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718fe01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f2682",
   "metadata": {},
   "source": [
    "See example.ipynb for the overview of `rlplay`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90b222",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de9187",
   "metadata": {},
   "source": [
    "## Tabular CartPole with Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f92f1c9",
   "metadata": {},
   "source": [
    "### The environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390dfcb3",
   "metadata": {},
   "source": [
    "A version of the Taxi Environment that disassembles the observation into pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Dict, Discrete\n",
    "\n",
    "\n",
    "class StructuredTaxiEnv(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Dict(dict(\n",
    "            row=Discrete(5),\n",
    "            col=Discrete(5),\n",
    "            location=Discrete(5),\n",
    "            destination=Discrete(4)\n",
    "        ))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # observation = r x c x p x d \\in 5 x 5 x 5 x 4\n",
    "        n, d = divmod(observation, 4)\n",
    "        n, p = divmod(n, 5)\n",
    "        r, c = divmod(n, 5)\n",
    "        return dict(r=r, c=c, p=p, d=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c43da2",
   "metadata": {},
   "source": [
    "A wrapper that makes Taxi's `.rander` compatible with `core_evaluate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a232508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenderPatch(gym.Wrapper):\n",
    "    def render(self, mode='human'):\n",
    "        result = self.env.render(mode)\n",
    "        if result is None:\n",
    "            return True\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3716f023",
   "metadata": {},
   "source": [
    "The environment factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d3464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.toy_text import TaxiEnv\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def factory(struct=False):\n",
    "    env = gym.make(\"Taxi-v3\")\n",
    "    if struct:\n",
    "        env = StructuredTaxiEnv(env)\n",
    "    return RenderPatch(env)\n",
    "\n",
    "\n",
    "factory_eval = partial(factory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8b9ac",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8bf14",
   "metadata": {},
   "source": [
    "### the Actor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442a90a2",
   "metadata": {},
   "source": [
    "A procedure and a layer, which converts the input integer data into its\n",
    "little endian binary representation as float $\\{0, 1\\}^m$ vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotbits(input, n_bits=63, dtype=torch.float):\n",
    "    \"\"\"Encode integers to fixed-width binary floating point vectors\"\"\"\n",
    "    assert not input.dtype.is_floating_point\n",
    "    assert 0 < n_bits < 64  # torch.int64 is signed, so 64-1 bits max\n",
    "\n",
    "    # n_bits = {torch.int64: 63, torch.int32: 31, torch.int16: 15, torch.int8 : 7}\n",
    "\n",
    "    # get mask of set bits\n",
    "    pow2 = torch.tensor([1 << j for j in range(n_bits)]).to(input.device)\n",
    "    x = input.unsqueeze(-1).bitwise_and(pow2).to(bool)\n",
    "\n",
    "    # upcast bool to float to get one-hot\n",
    "    return x.to(dtype)\n",
    "\n",
    "\n",
    "class OneHotBits(torch.nn.Module):\n",
    "    def __init__(self, n_bits=63, dtype=torch.float):\n",
    "        assert 1 <= n_bits < 64\n",
    "        super().__init__()\n",
    "        self.n_bits, self.dtype = n_bits, dtype\n",
    "\n",
    "    def forward(self, input):\n",
    "        return onehotbits(input, n_bits=self.n_bits, dtype=self.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984f850",
   "metadata": {},
   "source": [
    "A special module dictionary, which aplies itself to the input dict of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f3fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Mapping\n",
    "from torch.nn import Module, ModuleDict as BaseModuleDict\n",
    "\n",
    "\n",
    "class ModuleDict(BaseModuleDict):\n",
    "    \"\"\"The ModuleDict, that applies itself to hte indup dicts.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        modules: Optional[Mapping[str, Module]] = None,\n",
    "        dim: Optional[int]=-1\n",
    "    ) -> None:\n",
    "        super().__init__(modules)\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        # enforce concatenation in the order of the declaration in  __init__\n",
    "        return torch.cat([\n",
    "            m(input[k]) for k, m in self.items()\n",
    "        ], dim=self.dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b945a330",
   "metadata": {},
   "source": [
    "A simple tabular q-learner actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64527f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine import BaseActorModule\n",
    "\n",
    "\n",
    "class CartPoleActor(BaseActorModule):\n",
    "    def __init__(self, duelling=False, epsilon=0.1):\n",
    "        super().__init__()\n",
    "        self.duelling = duelling\n",
    "\n",
    "        # the table is actually an embedding of the state index\n",
    "        n_dim = 6 + (1 if duelling else 0)\n",
    "        self.table = torch.nn.Embedding(500, n_dim)\n",
    "\n",
    "        # for updating the exploration epsilon in the clones\n",
    "        self.register_buffer('epsilon', torch.tensor(epsilon))\n",
    "\n",
    "    def forward(self, obs, act, rew, fin, *, hx=None, stepno=None, virtual=False):\n",
    "        qv, hx = self.table(obs), ()\n",
    "        if self.duelling:\n",
    "            val, adv= torch.split_with_sizes(qv, [1, 6], dim=-1)\n",
    "            qv = adv + (val - adv.mean(dim=-1, keepdim=True))\n",
    "\n",
    "            val, actions = val.squeeze(-1), qv.max(dim=-1).indices\n",
    "\n",
    "        else:\n",
    "            val, actions = qv.max(dim=-1)\n",
    "\n",
    "        if self.training:\n",
    "            *head, n_actions = qv.shape\n",
    "            actions = actions.where(\n",
    "                torch.rand(head).gt(self.epsilon),\n",
    "                torch.randint(n_actions, size=head))\n",
    "\n",
    "        return actions, hx, dict(q=qv, value=val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870a94be",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30c29f8",
   "metadata": {},
   "source": [
    "#### D-DQN loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829b01a4",
   "metadata": {},
   "source": [
    "Service functions for the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0046223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.utils.plyr import apply, suply, xgetitem\n",
    "\n",
    "\n",
    "def timeshift(state, *, shift=1):\n",
    "    \"\"\"Get current and shfited slices of nested objects.\"\"\"\n",
    "    # use xgetitem to lett None through\n",
    "    # XXX `curr[t]` = (x_t, a_{t-1}, r_t, d_t), t=0..T-H\n",
    "    curr = suply(xgetitem, state, index=slice(None, -shift))\n",
    "\n",
    "    # XXX `next[t]` = (x_{t+H}, a_{t+H-1}, r_{t+H}, d_{t+H}), t=0..T-H\n",
    "    next = suply(xgetitem, state, index=slice(shift, None))\n",
    "\n",
    "    return curr, next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9af740",
   "metadata": {},
   "source": [
    "Double DQN loss for contiguous trajectory fragements. \n",
    "\n",
    "* `.state[t+1].rew` -- $r_{t+1}$\n",
    "* `.state[t+1].fin` -- $d_{t+1}$\n",
    "* `.state[t+1].act` -- $a_t$ which caused $\n",
    "    (s_t, z_t, a_t) \\longrightarrow (s_{t+1}, z_{t+1}, r_{t+1}, d_{t+1})\n",
    "$\n",
    "* `_, _, fragment.actor[t] = actor(.state[t])` -- $\n",
    "    q(z_t, h_t, \\cdot; \\theta_{\\text{old}})\n",
    "$ -- used for rollout collection\n",
    "* `_, _, info_module[t] = module(.state[t])` -- $\n",
    "    q(z_t, h_t, \\cdot; \\theta)\n",
    "$ -- the current q-function, producing $(h_t)_{t=0}^{T+1}$\n",
    "* `_, _, info_target[t] = target(.state[t])` -- $\n",
    "    q(z_t, h_t, \\cdot; \\theta_-)\n",
    "$ -- the q-target with $(h^-_t)_{t=0}^{T+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8bb258",
   "metadata": {},
   "source": [
    "The current q-network minimizes the $\\mathrm{TD}(0)$-error is\n",
    "\n",
    "$$\n",
    "\\delta_t(\\theta)\n",
    "    = \\bigl(\n",
    "        r_{t+1}\n",
    "        + \\gamma 1_{\\{\\neg d_{t+1}\\}} v^*(z_{t+1})\n",
    "    \\bigr) - q(z_t, h_t, a_t; \\theta)\n",
    "    \\,, $$\n",
    "\n",
    "where the approximate state value estimate $\n",
    "    v^*(z_{t+1})\n",
    "$ is one of\n",
    "* `Q-learning`: $\n",
    "    \\max_a q(z_{t+1}, h_{t+1}, a; \\theta)\n",
    "$\n",
    "* `DQN`: $\n",
    "    \\max_a q(z_{t+1}, h_{t+1}, a; \\theta_-)\n",
    "$\n",
    "* `double DQN`: $\n",
    "    q(z_{t+1}, h_{t+1}, \\hat{a}_{t+1}; \\theta_-)\n",
    "$ for $\n",
    "    \\hat{a}_{t+1} = \\arg \\max_a Qq(z_{t+1}, h_{t+1}, a; \\theta)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbae1aa7",
   "metadata": {},
   "source": [
    "One of the works realted to DQN learning of recurrent agents is [Kapturowski et al. (2018)](https://openreview.net/forum?id=r1lyTjAqYX), who propose to use a burn-in period in\n",
    "the contiguous trajectory fragment in order to compenaste for the representation drift,\n",
    "due to differnet RNN parameters $\\theta_-$ and $\\theta$.\n",
    "\n",
    "There is no clear-cut evidence suggesting that the hidden recurrent sequences $h_t$\n",
    "and $h^-_t$ yield significantly different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf2134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# @torch.enable_grad()\n",
    "def ddq_learn(fragment, module, *, gamma=0.95, target=None, double=False):\n",
    "    r\"\"\"Compute the Double-DQN loss over a _contiguous_ fragment of a trajectory.\n",
    "\n",
    "    Details\n",
    "    -------\n",
    "    In Q-learning the action value function minimizes the TD-error\n",
    "\n",
    "    $$\n",
    "        r_{t+1}\n",
    "            + \\gamma 1_{\\neg d_{t+1}} v^*(z_{t+1})\n",
    "            - q(z_t, a_t; \\theta)\n",
    "        \\,, $$\n",
    "\n",
    "    w.r.t. Q-network parameters $\\theta$ where $z_t$ is the actionable state,\n",
    "    $r_{t+1}$ is the reward for $s_t \\to s_{t+1}$ transition. The value of\n",
    "    $z_t$ include the current observation $x_t$ and the recurrent state $h_t$,\n",
    "    the last action $a_{t-1}$, the last reward $r_t$, and termination flag\n",
    "    $d_t$.\n",
    "\n",
    "    In the classic Q-learning there is no target network and the next state\n",
    "    optimal state value function is bootstrapped using the current Q-network\n",
    "    (`module`):\n",
    "\n",
    "    $$\n",
    "        v^*(z_{t+1})\n",
    "            \\approx \\max_a q(z_{t+1}, a; \\theta)\n",
    "        \\,. $$\n",
    "\n",
    "    The DQN method, proposed by\n",
    "\n",
    "        [Minh et al. (2013)](https://arxiv.org/abs/1312.5602),\n",
    "\n",
    "    uses a secondary Q-network (`target`) to estimate the value of the next\n",
    "    state:\n",
    "\n",
    "    $$\n",
    "        v^*(z_{t+1})\n",
    "            \\approx \\max_a q(z_{t+1}, a; \\theta^-)\n",
    "        \\,, $$\n",
    "\n",
    "    where $\\theta^-$ are the frozen parameters of the target Q-network. The\n",
    "    Double DQN algorithm of\n",
    "\n",
    "        [van Hasselt et al. (2015)](https://arxiv.org/abs/1509.06461)\n",
    "\n",
    "    unravels the $\\max$ operator as\n",
    "    $\n",
    "        \\max_k u_k \\equiv u_{\\arg \\max_k u_k}\n",
    "    $\n",
    "    and replaces the outer $u$ with the Q-values of the target Q-network, while\n",
    "    computing the inner $u$ (inside the $\\arg\\max$) with the current Q-network.\n",
    "    Specifically, the Double DQN value estimate is\n",
    "\n",
    "    $$\n",
    "        v^*(z_{t+1})\n",
    "            \\approx q(z_{t+1}, \\hat{a}_{t+1}; \\theta^-)\n",
    "            \\,,\n",
    "            \\hat{a}_{t+1}\n",
    "                = \\arg \\max_a q(z_{t+1}, a; \\theta)\n",
    "        \\,, $$\n",
    "\n",
    "    for $\n",
    "        \\hat{a}_{t+1}\n",
    "            = \\arg \\max_a q(s_{t+1}, a; \\theta)\n",
    "    $ being the action taken by the current Q-network $\\theta$ at $z_{t+1}$.\n",
    "\n",
    "    Recurrent DQN\n",
    "    -------------\n",
    "    The key problem with the recurrent state $h_t$ in $z_t$ is its representaion\n",
    "    drift: the endogenous states used for collecting trajectory data during the\n",
    "    rollout are produced by an actor with stale perameters $\\theta_{\\text{old}}$,\n",
    "    and thus might have high discrepancy with the recurrent state produced by\n",
    "    the current Q-network $\\theta$ or the target $\\theta-_$. To mitigate this\n",
    "        \n",
    "        [Kapturowski et al. (2018)](https://openreview.net/forum?id=r1lyTjAqYX)\n",
    "    \n",
    "    proposed to spend a slice `burnin` of the recorded trajectory on\n",
    "    aligning the recurrent representation. Specifically, starting with $h_0$\n",
    "    (contained in `fragment.hx`) they propose to launch two sequences $h_t$\n",
    "    and $h^-_t$ from the same $h^-_0 = h_0$ using $q(\\cdot; \\theta)$ and\n",
    "    $q(\\cdot; \\theta^-)$, respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    trajectory, hx = fragment.state, fragment.hx\n",
    "    obs, act, rew, fin = trajectory.obs, trajectory.act, trajectory.rew, trajectory.fin\n",
    "\n",
    "    # get $Q(z_t, h_t, \\cdot; \\theta)$ for all t=0..T\n",
    "    _, _, info_module = module(\n",
    "        obs, act, rew, fin, hx=hx, stepno=trajectory.stepno)\n",
    "\n",
    "    # get the next state `state[t+1]` $z_{t+1}$ to access $a_t$\n",
    "    state_next = suply(xgetitem, trajectory, index=slice(1, None))\n",
    "\n",
    "    # $\\hat{A}_t$, the module's response to current and next state,\n",
    "    #  contains the q-values. `curr` is $q(z_t, h_{t+1}, \\cdot; \\theta)$\n",
    "    #  and `next` is $q(z_{t+1}, h_{t+1}, \\cdot; \\theta)$ is `next`.\n",
    "    info_module_curr, info_module_next = timeshift(info_module)\n",
    "\n",
    "    # get $q(z_t, h_t, a_t; \\theta)$ for all t=0..T-1\n",
    "    q_replay = info_module_curr['q'].gather(-1, state_next.act.unsqueeze(-1))\n",
    "\n",
    "    # get $\\hat{v}_{t+1}(z_{t+1}) = ...$\n",
    "    with torch.no_grad():\n",
    "        if target is None:\n",
    "            # get $... = \\max_a Q(z_{t+1}, h_{t+1}, a; \\theta)$\n",
    "            q_value = info_module_next['q'].max(dim=-1, keepdim=True).values\n",
    "\n",
    "        else:\n",
    "            _, _, info_target = target(\n",
    "                obs, act, rew, fin, hx=hx, stepno=trajectory.stepno)\n",
    "\n",
    "            info_target_next = suply(xgetitem, info_target, index=slice(1, None))\n",
    "            if not double:\n",
    "                # get $... = \\max_a Q(z_{t+1}, h^-_{t+1}, a; \\theta^-)$\n",
    "                q_value = info_target_next['q'].max(dim=-1, keepdim=True).values\n",
    "\n",
    "            else:\n",
    "                # get $\\hat{a}_{t+1} = \\arg \\max_a Q(z_{t+1}, h_{t+1}, a; \\theta)$\n",
    "                hat_act = info_module_next['q'].max(dim=-1).indices.unsqueeze(-1)\n",
    "\n",
    "                # get $... = Q(z_{t+1}, h^-_{t+1}, \\hat{a}_{t+1}; \\theta^-)$\n",
    "                q_value = info_target_next['q'].gather(-1, hat_act)\n",
    "\n",
    "        # get $r_{t+1} + \\gamma 1_{d_{t+1}} \\hat{v}_{t+1}(z_{t+1})$ using inplace ops\n",
    "        q_value.masked_fill_(state_next.fin.unsqueeze(-1), 0.)\n",
    "        q_value.mul_(gamma).add_(state_next.rew.unsqueeze(-1))\n",
    "\n",
    "    # td-error ell-2 loss\n",
    "    return F.mse_loss(q_replay, q_value, reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab8352",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e336a5e",
   "metadata": {},
   "source": [
    "prepare the optimizer for the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad23b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.6\n",
    "use_target, use_double, use_duelling = False, False, False\n",
    "\n",
    "# `target` does not work for some reason\n",
    "# `duelling` fails for both target and double, and sorta for ordinary q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842dc177",
   "metadata": {},
   "source": [
    "Initialize the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f2f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = CartPoleActor(duelling=use_duelling, epsilon=1.)\n",
    "\n",
    "learner.train()\n",
    "device_ = torch.device('cpu')  # torch.device('cuda:0')\n",
    "learner.to(device=device_)\n",
    "\n",
    "optim = torch.optim.SGD(learner.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82730b1a",
   "metadata": {},
   "source": [
    "Initialize the sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991c289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "T, B = 25, 8"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f17902fe",
   "metadata": {},
   "source": [
    "from rlplay.engine.rollout import same\n",
    "\n",
    "batchit = same.rollout(\n",
    "    [factory() for _ in range(B)],\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    sticky=False,\n",
    "    device=device_,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c298875",
   "metadata": {},
   "source": [
    "from rlplay.engine.rollout import single\n",
    "\n",
    "batchit = single.rollout(\n",
    "    factory,\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    n_envs=B,\n",
    "    sticky=False,\n",
    "    clone=True,\n",
    "    close=False,\n",
    "    device=device_,\n",
    "    start_method='fork',  # fork in notebook for macos, spawn in linux\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60ab21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout import multi\n",
    "\n",
    "batchit = multi.rollout(\n",
    "    factory,\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    n_actors=8,\n",
    "    n_per_actor=B,\n",
    "    n_buffers=16,\n",
    "    n_per_batch=2,\n",
    "    sticky=False,\n",
    "    pinned=False,\n",
    "    clone=True,\n",
    "    close=False,\n",
    "    device=device_,\n",
    "    start_method='fork',  # fork in notebook for macos, spawn in linux\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9a0a4",
   "metadata": {},
   "source": [
    "A generator of evaluation rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f3229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout.evaluate import evaluate\n",
    "\n",
    "test_it = evaluate(factory_eval, learner, n_envs=4, n_steps=500,\n",
    "                   clone=False, device=device_, start_method='fork')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846d8c9",
   "metadata": {},
   "source": [
    "Implement your favourite training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ab50a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import copy\n",
    "from math import log, exp\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "# the training loop\n",
    "losses, rewards = [], []\n",
    "decay = -log(2) / 50  # exploration epsilon halflife\n",
    "for epoch in tqdm.tqdm(range(400)):\n",
    "    # freeze the target for q\n",
    "    target = copy.deepcopy(learner) if use_target else None\n",
    "\n",
    "    for j, batch in zip(range(100), batchit):\n",
    "        loss = ddq_learn(batch, learner, target=target,\n",
    "                         gamma=gamma, double=use_double)\n",
    "\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        grad = clip_grad_norm_(learner.parameters(), max_norm=1e3)\n",
    "        optim.step()\n",
    "\n",
    "        losses.append(dict(\n",
    "            loss=float(loss), grad=float(grad),\n",
    "        ))\n",
    "\n",
    "    learner.epsilon.mul_(exp(decay)).clip_(0.1, 1.0)\n",
    "\n",
    "    # fetch the evaluation results lagged by one inner loop!\n",
    "    rewards.append(next(test_it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0633c36c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# close the generators\n",
    "batchit.close()\n",
    "test_it.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c524c1b",
   "metadata": {},
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce115d82",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(records):\n",
    "    \"\"\"collate identically keyed dicts\"\"\"\n",
    "    out, n_records = {}, 0\n",
    "    for record in records:\n",
    "        for k, v in record.items():\n",
    "            out.setdefault(k, []).append(v)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "data = {k: numpy.array(v) for k, v in collate(losses).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(data['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01581b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(data['grad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3128f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = numpy.stack(rewards, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ffd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd6ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = numpy.median(rewards, axis=-1), rewards.std(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5545163",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fi, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "\n",
    "ax.plot(numpy.mean(rewards, axis=-1))\n",
    "ax.plot(numpy.median(rewards, axis=-1))\n",
    "ax.plot(numpy.min(rewards, axis=-1))\n",
    "ax.plot(numpy.std(rewards, axis=-1))\n",
    "# ax.plot(m+s * 1.96)\n",
    "# ax.plot(m-s * 1.96)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9856d6a4",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace0ac8",
   "metadata": {},
   "source": [
    "The ultimate evaluation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d372a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from rlplay.engine import core\n",
    "\n",
    "with factory_eval() as env:\n",
    "    learner.eval()\n",
    "    eval_rewards, info = core.evaluate([\n",
    "        env\n",
    "    ], learner, render=True, n_steps=1e2, device=device_)\n",
    "\n",
    "print(sum(eval_rewards))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4716caad",
   "metadata": {},
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade2f90c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df22ff",
   "metadata": {},
   "source": [
    "Let's analyze the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8257eb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((info['q'] - numpy.expand_dims(info['value'], -1))[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d924c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.algo.returns import npy_returns, npy_deltas\n",
    "\n",
    "td_target = eval_rewards + gamma * info['value'][1:]\n",
    "td_error = td_target - info['value'][:-1]\n",
    "# td_error = npy_deltas(\n",
    "#     eval_rewards, numpy.zeros_like(eval_rewards, dtype=bool), info['value'][:-1],\n",
    "#     gamma=gamma, bootstrap=info['value'][-1])\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "ax.semilogy(abs(td_error) / abs(td_target))\n",
    "ax.set_title('relative td(1)-error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef93ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.algo.returns import npy_returns, npy_deltas\n",
    "\n",
    "# plt.plot(\n",
    "#     npy_returns(eval_rewards, numpy.zeros_like(eval_rewards, dtype=bool),\n",
    "#                 gamma=gamma, bootstrap=info['value'][-1]))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "ax.plot(info['value']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4cdeca",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09142f04",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stepno = batch.state.stepno\n",
    "stepno = torch.arange(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91906ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = learner.features['stepno'](stepno)\n",
    "\n",
    "    out = F.linear(F.relu(out), learner.core[1].weight[:, -8:],\n",
    "                       bias=learner.core[1].bias)\n",
    "#     out = F.linear(F.relu(out), learner.core.weight_ih_l0[:, -8:],\n",
    "#                        bias=learner.core.bias_ih_l0)\n",
    "#     out = F.relu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4953372",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8, 8, figsize=(8, 8), dpi=200,\n",
    "                         sharex=True, sharey=True)\n",
    "\n",
    "for j, ax in zip(range(out.shape[1]), axes.flat):\n",
    "    ax.plot(out[:, j], lw=1)\n",
    "\n",
    "fig.tight_layout(pad=0, h_pad=0, w_pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    plt.imshow(abs(learner.core[1].weight[:, -8:]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5a0491",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = learner.features.stepno[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f353e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    plt.imshow(abs(lin.weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68eeff0",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
