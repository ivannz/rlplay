{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "830df33b",
   "metadata": {},
   "source": [
    "# `rlplay`-ing around with the Double-Dee QN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718fe01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f2682",
   "metadata": {},
   "source": [
    "See example.ipynb for the overview of `rlplay`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90b222",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de9187",
   "metadata": {},
   "source": [
    "## Tabular CartPole with Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f92f1c9",
   "metadata": {},
   "source": [
    "### The environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390dfcb3",
   "metadata": {},
   "source": [
    "A version of the Taxi Environment that disassembles the observation into pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Dict, Discrete\n",
    "\n",
    "\n",
    "class StructuredTaxiEnv(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Dict(dict(\n",
    "            row=Discrete(5),\n",
    "            col=Discrete(5),\n",
    "            location=Discrete(5),\n",
    "            destination=Discrete(4)\n",
    "        ))\n",
    "\n",
    "    @staticmethod\n",
    "    def observation(n):\n",
    "        # observation = r x c x p x d \\in 5 x 5 x 5 x 4\n",
    "        # n, d = divmod(observation, 4)\n",
    "        # n, p = divmod(n, 5)\n",
    "        # r, c = divmod(n, 5)\n",
    "        d = n % 4\n",
    "        n = n // 4\n",
    "        p = n % 5\n",
    "        n = n // 5\n",
    "        c = n % 5\n",
    "        r = n // 5\n",
    "        return dict(row=r, col=c, location=p, destination=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c43da2",
   "metadata": {},
   "source": [
    "A wrapper that makes Taxi's `.render` compatible with `core_evaluate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a232508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenderPatch(gym.Wrapper):\n",
    "    def render(self, mode='human'):\n",
    "        result = self.env.render(mode)\n",
    "        if result is None:\n",
    "            return True\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3716f023",
   "metadata": {},
   "source": [
    "The environment factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d3464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.toy_text import TaxiEnv\n",
    "\n",
    "\n",
    "def base_factory(struct=False, seed=None):\n",
    "    env = gym.make(\"Taxi-v3\")\n",
    "    if struct:\n",
    "        env = StructuredTaxiEnv(env)\n",
    "    return RenderPatch(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8b9ac",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8bf14",
   "metadata": {},
   "source": [
    "### the Actor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442a90a2",
   "metadata": {},
   "source": [
    "A procedure and a layer, which converts the input integer data into its\n",
    "little-endian binary representation as float $\\{0, 1\\}^m$ vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotbits(input, n_bits=63, dtype=torch.float):\n",
    "    \"\"\"Encode integers to fixed-width binary floating point vectors\"\"\"\n",
    "    assert not input.dtype.is_floating_point\n",
    "    assert 0 < n_bits < 64  # torch.int64 is signed, so 64-1 bits max\n",
    "\n",
    "    # n_bits = {torch.int64: 63, torch.int32: 31, torch.int16: 15, torch.int8 : 7}\n",
    "\n",
    "    # get mask of set bits\n",
    "    pow2 = torch.tensor([1 << j for j in range(n_bits)]).to(input.device)\n",
    "    x = input.unsqueeze(-1).bitwise_and(pow2).to(bool)\n",
    "\n",
    "    # upcast bool to float to get one-hot\n",
    "    return x.to(dtype)\n",
    "\n",
    "\n",
    "class OneHotBits(torch.nn.Module):\n",
    "    def __init__(self, n_bits=63, dtype=torch.float):\n",
    "        assert 1 <= n_bits < 64\n",
    "        super().__init__()\n",
    "        self.n_bits, self.dtype = n_bits, dtype\n",
    "\n",
    "    def forward(self, input):\n",
    "        return onehotbits(input, n_bits=self.n_bits, dtype=self.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984f850",
   "metadata": {},
   "source": [
    "A special module dictionary, which applies itself to the input dict of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f3fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Mapping\n",
    "from torch.nn import Module, ModuleDict as BaseModuleDict\n",
    "\n",
    "\n",
    "class ModuleDict(BaseModuleDict):\n",
    "    \"\"\"The ModuleDict, that applies itself to the input dicts.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        modules: Optional[Mapping[str, Module]] = None,\n",
    "        dim: Optional[int]=-1\n",
    "    ) -> None:\n",
    "        super().__init__(modules)\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        # enforce concatenation in the order of the declaration in  __init__\n",
    "        return torch.cat([\n",
    "            m(input[k]) for k, m in self.items()\n",
    "        ], dim=self.dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b945a330",
   "metadata": {},
   "source": [
    "A simple tabular q-learner actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdd1adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine import BaseActorModule\n",
    "\n",
    "\n",
    "class TaxiActor(BaseActorModule):\n",
    "    def __init__(self, duelling=False, struct=False, epsilon=0.1):\n",
    "        super().__init__()\n",
    "        self.duelling = duelling\n",
    "\n",
    "        # the table is actually an embedding of the state index\n",
    "        n_dim = 6 + (1 if duelling else 0)\n",
    "        if not struct:\n",
    "            self.table = torch.nn.Embedding(500, n_dim)\n",
    "\n",
    "        else:\n",
    "            self.table = torch.nn.Sequential(\n",
    "                ModuleDict(dict(\n",
    "                    row=torch.nn.Embedding(5, 4),\n",
    "                    col=torch.nn.Embedding(5, 4),\n",
    "                    location=torch.nn.Embedding(5, 4),\n",
    "                    destination=torch.nn.Embedding(4, 4),\n",
    "                )),\n",
    "                torch.nn.Linear(16, n_dim)\n",
    "            )\n",
    "\n",
    "        # for updating the exploration epsilon in the clones\n",
    "        self.register_buffer('epsilon', torch.tensor(epsilon))\n",
    "\n",
    "    def forward(self, obs, act, rew, fin, *, hx=None, stepno=None, virtual=False):\n",
    "        qv, hx = self.table(obs), ()\n",
    "        if self.duelling:\n",
    "            # dueling net [Wang et al. (2016)](https://arxiv.org/abs/1511.06581)\n",
    "            val, adv= torch.split_with_sizes(qv, [1, 6], dim=-1)\n",
    "            qv = adv + (val - adv.mean(dim=-1, keepdim=True))\n",
    "\n",
    "            val, actions = val.squeeze(-1), qv.max(dim=-1).indices\n",
    "\n",
    "        else:\n",
    "            val, actions = qv.max(dim=-1)\n",
    "\n",
    "        if self.training:\n",
    "            *head, n_actions = qv.shape\n",
    "            actions = actions.where(\n",
    "                torch.rand(head).gt(self.epsilon),\n",
    "                torch.randint(n_actions, size=head))\n",
    "\n",
    "        return actions, hx, dict(q=qv, value=val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870a94be",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30c29f8",
   "metadata": {},
   "source": [
    "### D-DQN loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829b01a4",
   "metadata": {},
   "source": [
    "Service functions for the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0046223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plyr import apply, suply, xgetitem\n",
    "\n",
    "\n",
    "def timeshift(state, *, shift=1):\n",
    "    \"\"\"Get current and shifted slices of nested objects.\"\"\"\n",
    "    # use `xgetitem` to let None through\n",
    "    # XXX `curr[t]` = (x_t, a_{t-1}, r_t, d_t), t=0..T-H\n",
    "    curr = suply(xgetitem, state, index=slice(None, -shift))\n",
    "\n",
    "    # XXX `next[t]` = (x_{t+H}, a_{t+H-1}, r_{t+H}, d_{t+H}), t=0..T-H\n",
    "    next = suply(xgetitem, state, index=slice(shift, None))\n",
    "\n",
    "    return curr, next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9af740",
   "metadata": {},
   "source": [
    "Double DQN loss for contiguous trajectory fragments. \n",
    "\n",
    "* `.state[t+1].rew` -- $r_{t+1}$\n",
    "* `.state[t+1].fin` -- $d_{t+1}$\n",
    "* `.state[t+1].act` -- $a_t$ which caused $\n",
    "    (s_t, z_t, a_t) \\longrightarrow (s_{t+1}, z_{t+1}, r_{t+1}, d_{t+1})\n",
    "$\n",
    "* `_, _, fragment.actor[t] = actor(.state[t])` -- $\n",
    "    q(z_t, h_t, \\cdot; \\theta_{\\text{old}})\n",
    "$ -- used for rollout collection\n",
    "* `_, _, info_module[t] = module(.state[t])` -- $\n",
    "    q(z_t, h_t, \\cdot; \\theta)\n",
    "$ -- the current q-function, producing $(h_t)_{t=0}^{T+1}$\n",
    "* `_, _, info_target[t] = target(.state[t])` -- $\n",
    "    q(z_t, h_t, \\cdot; \\theta_-)\n",
    "$ -- the q-target with $(h^-_t)_{t=0}^{T+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8bb258",
   "metadata": {},
   "source": [
    "The current q-network minimizes the $\\mathrm{TD}(0)$-error is\n",
    "\n",
    "$$\n",
    "\\delta_t(\\theta)\n",
    "    = \\bigl(\n",
    "        r_{t+1}\n",
    "        + \\gamma 1_{\\{\\neg d_{t+1}\\}} v^*(z_{t+1})\n",
    "    \\bigr) - q(z_t, h_t, a_t; \\theta)\n",
    "    \\,, $$\n",
    "\n",
    "where the approximate state value estimate $\n",
    "    v^*(z_{t+1})\n",
    "$ is one of\n",
    "* `Q-learning`: $\n",
    "    \\max_a q(z_{t+1}, h_{t+1}, a; \\theta)\n",
    "$\n",
    "* `DQN`: $\n",
    "    \\max_a q(z_{t+1}, h_{t+1}, a; \\theta_-)\n",
    "$\n",
    "* `double DQN`: $\n",
    "    q(z_{t+1}, h_{t+1}, \\hat{a}_{t+1}; \\theta_-)\n",
    "$ for $\n",
    "    \\hat{a}_{t+1} = \\arg \\max_a Qq(z_{t+1}, h_{t+1}, a; \\theta)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbae1aa7",
   "metadata": {},
   "source": [
    "One of the works related to DQN learning of recurrent agents is [Kapturowski et al. (2018)](https://openreview.net/forum?id=r1lyTjAqYX), who propose to use a burn-in period in\n",
    "the contiguous trajectory fragment in order to compensate for the representation drift,\n",
    "due to different RNN parameters $\\theta_-$ and $\\theta$.\n",
    "\n",
    "There is no clear-cut evidence suggesting that the hidden recurrent sequences $h_t$\n",
    "and $h^-_t$ yield significantly different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf2134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# @torch.enable_grad()\n",
    "def ddq_learn(fragment, module, *, gamma=0.95, target=None, double=False):\n",
    "    r\"\"\"Compute the Double-DQN loss over a _contiguous_ fragment of a trajectory.\n",
    "\n",
    "    Details\n",
    "    -------\n",
    "    In Q-learning the action value function minimizes the TD-error\n",
    "\n",
    "    $$\n",
    "        r_{t+1}\n",
    "            + \\gamma 1_{\\neg d_{t+1}} v^*(z_{t+1})\n",
    "            - q(z_t, a_t; \\theta)\n",
    "        \\,, $$\n",
    "\n",
    "    w.r.t. Q-network parameters $\\theta$ where $z_t$ is the actionable state,\n",
    "    $r_{t+1}$ is the reward for $s_t \\to s_{t+1}$ transition. The value of\n",
    "    $z_t$ include the current observation $x_t$ and the recurrent state $h_t$,\n",
    "    the last action $a_{t-1}$, the last reward $r_t$, and termination flag\n",
    "    $d_t$.\n",
    "\n",
    "    In the classic Q-learning there is no target network and the next state\n",
    "    optimal state value function is bootstrapped using the current Q-network\n",
    "    (`module`):\n",
    "\n",
    "    $$\n",
    "        v^*(z_{t+1})\n",
    "            \\approx \\max_a q(z_{t+1}, a; \\theta)\n",
    "        \\,. $$\n",
    "\n",
    "    The DQN method, proposed by\n",
    "\n",
    "        [Minh et al. (2013)](https://arxiv.org/abs/1312.5602),\n",
    "\n",
    "    uses a secondary Q-network (`target`) to estimate the value of the next\n",
    "    state:\n",
    "\n",
    "    $$\n",
    "        v^*(z_{t+1})\n",
    "            \\approx \\max_a q(z_{t+1}, a; \\theta^-)\n",
    "        \\,, $$\n",
    "\n",
    "    where $\\theta^-$ are the frozen parameters of the target Q-network. The\n",
    "    Double DQN algorithm of\n",
    "\n",
    "        [van Hasselt et al. (2015)](https://arxiv.org/abs/1509.06461)\n",
    "\n",
    "    unravels the $\\max$ operator as\n",
    "    $\n",
    "        \\max_k u_k \\equiv u_{\\arg \\max_k u_k}\n",
    "    $\n",
    "    and replaces the outer $u$ with the Q-values of the target Q-network, while\n",
    "    computing the inner $u$ (inside the $\\arg\\max$) with the current Q-network.\n",
    "    Specifically, the Double DQN value estimate is\n",
    "\n",
    "    $$\n",
    "        v^*(z_{t+1})\n",
    "            \\approx q(z_{t+1}, \\hat{a}_{t+1}; \\theta^-)\n",
    "            \\,,\n",
    "            \\hat{a}_{t+1}\n",
    "                = \\arg \\max_a q(z_{t+1}, a; \\theta)\n",
    "        \\,, $$\n",
    "\n",
    "    for $\n",
    "        \\hat{a}_{t+1}\n",
    "            = \\arg \\max_a q(s_{t+1}, a; \\theta)\n",
    "    $ being the action taken by the current Q-network $\\theta$ at $z_{t+1}$.\n",
    "\n",
    "    Recurrent DQN\n",
    "    -------------\n",
    "    The key problem with the recurrent state $h_t$ in $z_t$ is its representation\n",
    "    drift: the endogenous states used for collecting trajectory data during the\n",
    "    rollout are produced by an actor with stale parameters $\\theta_{\\text{old}}$,\n",
    "    and thus might have high discrepancy with the recurrent state produced by\n",
    "    the current Q-network $\\theta$ or the target $\\theta-_$. To mitigate this\n",
    "        \n",
    "        [Kapturowski et al. (2018)](https://openreview.net/forum?id=r1lyTjAqYX)\n",
    "    \n",
    "    proposed to spend a slice `burn-in` of the recorded trajectory on\n",
    "    aligning the recurrent representation. Specifically, starting with $h_0$\n",
    "    (contained in `fragment.hx`) they propose to launch two sequences $h_t$\n",
    "    and $h^-_t$ from the same $h^-_0 = h_0$ using $q(\\cdot; \\theta)$ and\n",
    "    $q(\\cdot; \\theta^-)$, respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    trajectory, hx = fragment.state, fragment.hx\n",
    "    obs, act, rew, fin = trajectory.obs, trajectory.act, trajectory.rew, trajectory.fin\n",
    "\n",
    "    # get $Q(z_t, h_t, \\cdot; \\theta)$ for all t=0..T\n",
    "    _, _, info_module = module(\n",
    "        obs, act, rew, fin, hx=hx, stepno=trajectory.stepno)\n",
    "\n",
    "    # get the next state `state[t+1]` $z_{t+1}$ to access $a_t$\n",
    "    state_next = suply(xgetitem, trajectory, index=slice(1, None))\n",
    "\n",
    "    # $\\hat{A}_t$, the module's response to current and next state,\n",
    "    #  contains the q-values. `curr` is $q(z_t, h_{t+1}, \\cdot; \\theta)$\n",
    "    #  and `next` is $q(z_{t+1}, h_{t+1}, \\cdot; \\theta)$ is `next`.\n",
    "    info_module_curr, info_module_next = timeshift(info_module)\n",
    "\n",
    "    # get $q(z_t, h_t, a_t; \\theta)$ for all t=0..T-1\n",
    "    q_replay = info_module_curr['q'].gather(-1, state_next.act.unsqueeze(-1))\n",
    "\n",
    "    # get $\\hat{v}_{t+1}(z_{t+1}) = ...$\n",
    "    with torch.no_grad():\n",
    "        if target is None:\n",
    "            # get $... = \\max_a Q(z_{t+1}, h_{t+1}, a; \\theta)$\n",
    "            q_value = info_module_next['q'].max(dim=-1, keepdim=True).values\n",
    "\n",
    "        else:\n",
    "            _, _, info_target = target(\n",
    "                obs, act, rew, fin, hx=hx, stepno=trajectory.stepno)\n",
    "\n",
    "            info_target_next = suply(xgetitem, info_target, index=slice(1, None))\n",
    "            if not double:\n",
    "                # get $... = \\max_a Q(z_{t+1}, h^-_{t+1}, a; \\theta^-)$\n",
    "                q_value = info_target_next['q'].max(dim=-1, keepdim=True).values\n",
    "\n",
    "            else:\n",
    "                # get $\\hat{a}_{t+1} = \\arg \\max_a Q(z_{t+1}, h_{t+1}, a; \\theta)$\n",
    "                hat_act = info_module_next['q'].max(dim=-1).indices.unsqueeze(-1)\n",
    "\n",
    "                # get $... = Q(z_{t+1}, h^-_{t+1}, \\hat{a}_{t+1}; \\theta^-)$\n",
    "                q_value = info_target_next['q'].gather(-1, hat_act)\n",
    "\n",
    "        # get $r_{t+1} + \\gamma 1_{d_{t+1}} \\hat{v}_{t+1}(z_{t+1})$ using inplace ops\n",
    "        q_value.masked_fill_(state_next.fin.unsqueeze(-1), 0.)\n",
    "        q_value.mul_(gamma).add_(state_next.rew.unsqueeze(-1))\n",
    "\n",
    "    # td-error ell-2 loss\n",
    "    return F.mse_loss(q_replay, q_value, reduction='sum'), {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab8352",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22378cff",
   "metadata": {},
   "source": [
    "The following is my incomplete take on [Bellemare et al. (2017)](http://proceedings.mlr.press/v70/bellemare17a.html)\n",
    "\n",
    "Then the distributional Bellman operator on a policy $\\pi_\\theta$ at $(s, a)$ is\n",
    "defined as a random variable on $\\mathbb{R}$ with the following law\n",
    "\n",
    "$$\n",
    "(T Q_\\theta)(s, a)\n",
    "    \\overset{D}{=}\n",
    "        r + \\gamma Q_\\theta(s', a')\n",
    "    \\,, \\text{ for }\n",
    "    (r, s') \\sim p(r, s'\\mid s, a)\n",
    "    \\,, \\text{ and }\n",
    "    a' \\sim \\pi_\\theta(a \\mid s')\n",
    "    \\,.\n",
    "$$\n",
    "\n",
    "Formally, this means the following (omitting the dependency of on $(s, a)$):\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbb{P}\\bigl(\n",
    "    T Q_\\theta \\in U\n",
    "  \\bigr)\n",
    "    &= \\mathbb{P}\\bigl(\n",
    "      R + \\gamma Q_\\theta(S', A') \\in U\n",
    "    \\bigr)\n",
    "    \\\\\n",
    "    &= \\mathbb{E}_{r, s'\\sim p(r, s'\\mid s, a)}\n",
    "        \\mathbb{E}_{a'\\sim \\pi_\\theta(a \\mid s')}\n",
    "          \\mathbb{P}\\bigl(\n",
    "            R + \\gamma Q_\\theta(S', A') \\in U\n",
    "            \\,\\big \\vert\\, S'=s, R=r, A'=a'\n",
    "          \\bigr)\n",
    "    \\\\\n",
    "    &= \\mathbb{E}_{r, s'\\sim p(r, s'\\mid s, a)}\n",
    "        \\mathbb{E}_{a'\\sim \\pi_\\theta(a \\mid s')}\n",
    "          \\mathbb{P}\\bigl(\n",
    "            Q_\\theta(s', a') \\in \\frac{U - r} \\gamma\n",
    "          \\bigr)\n",
    "    \\,,\n",
    "\\end{align}\n",
    "\n",
    "or, in other words, $T$ acts on the conditional distribution $\n",
    "    Q_\\theta(\\cdot \\mid s, a)\n",
    "$ thus: for any bounded Borel measurable $f$\n",
    "\n",
    "\\begin{align}\n",
    "  \\int (T Q_\\theta)(dv \\mid s, a) f(v)\n",
    "    &= \\int\n",
    "        p(ds', dr\\mid s, a)  % clump p and R into one cond-distrib\n",
    "        \\pi_\\theta(da' \\mid s')\n",
    "        Q_\\theta(dv\\mid s', a')\n",
    "        f(r + \\gamma v)\n",
    "    \\\\\n",
    "    &=  % Fubini\n",
    "        \\mathbb{E}_{r, s'\\sim p(r, s'\\mid s, a)}\n",
    "        \\mathbb{E}_{a'\\sim \\pi_\\theta(a \\mid s')}\n",
    "        \\mathbb{E}_{v\\sim Q_\\theta(v\\mid s', a')}\n",
    "            f(r + \\gamma v)\n",
    "    \\,.\n",
    "\\end{align}\n",
    "\n",
    "Thus suggests several approximations, depending on the tractability of the inner expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79bb76e",
   "metadata": {},
   "source": [
    "Ideally we would like to find such distribution, that\n",
    "$$\n",
    "(T Q_\\theta)(s, a)\n",
    "    \\overset{D}{=}\n",
    "    Q_\\theta(s, a)\n",
    "    \\,, $$\n",
    "for all or most $s$ and $a$.\n",
    "\n",
    "If $f$ is $L$-Lipschitz, then\n",
    "\\begin{align}\n",
    "\\biggl\\lvert\n",
    "    \\int \\bigl(\n",
    "        (T H)(dv \\mid s, a) - (T G)(dv \\mid s, a)\n",
    "        \\bigr) f(v)\n",
    "    \\biggr\\rvert\n",
    "    &\\leq \\int\n",
    "        p(ds', dr\\mid s, a)\n",
    "        \\pi_\\theta(da' \\mid s')\n",
    "        H(dh\\mid s', a')\n",
    "        G(dg \\mid s', a')\n",
    "        \\bigl\\lvert\n",
    "            f(r + \\gamma h) - f(r + \\gamma g)\n",
    "        \\bigr\\rvert\n",
    "    \\\\\n",
    "    &\\leq L \\gamma \\int\n",
    "        p(ds', dr\\mid s, a)\n",
    "        \\pi_\\theta(da' \\mid s')\n",
    "        H(dh \\mid s', a')\n",
    "        G(dg \\mid s', a')\n",
    "        \\bigl\\lvert h - g \\bigr\\rvert\n",
    "    \\\\\n",
    "    &\\leq L \\gamma \\int\n",
    "        p(ds', dr\\mid s, a)\n",
    "        \\pi_\\theta(da' \\mid s')\n",
    "        \\int\n",
    "            H(dh \\mid s', a')\n",
    "            G(dg \\mid s', a')\n",
    "            \\bigl\\lvert h - g \\bigr\\rvert\n",
    "    \\\\\n",
    "    &\\leq L \\gamma \\sup_{s, a}\n",
    "        \\int\n",
    "            H(dh \\mid s, a)\n",
    "            G(dg \\mid s, a)\n",
    "            \\bigl\\lvert h - g \\bigr\\rvert\n",
    "    \\,,\n",
    "\\end{align}\n",
    "\n",
    "so there is hope for contraction or even convergence, however, it is better to\n",
    "consult [Bellemare et al. (2017)](http://proceedings.mlr.press/v70/bellemare17a.html)\n",
    "first.\n",
    "\n",
    "<!--\n",
    "$$\n",
    "\\biggl\\lvert\n",
    "    \\int \\bigl(\n",
    "        (T Q_\\theta)(dv \\mid s, a) - Q_\\theta(dv \\mid s, a)\n",
    "        \\bigr) f(v)\n",
    "    \\biggr\\rvert\n",
    "    \\leq \\int\n",
    "        p(ds', dr\\mid s, a)\n",
    "        \\pi_\\theta(da' \\mid s')\n",
    "        Q_\\theta(dv'\\mid s', a')\n",
    "        Q_\\theta(dv\\mid s, a)\n",
    "        \\bigl\\lvert\n",
    "            f(r + \\gamma v') - f(v)\n",
    "        \\bigr\\rvert\n",
    "    \\,. $$\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bba983",
   "metadata": {},
   "source": [
    "Below is the loss for a version of the distributional DQN, specific to Gaussian approximations\n",
    "and maxent regularization.\n",
    "\n",
    "We minimize (in expectation over transition $s, a \\to s'$) the Kullback-Leibler\n",
    "divergence between distributions $\n",
    "    (T Q_\\theta)(dv \\mid s, a)\n",
    "$ and $\n",
    "    Q_\\theta(dv \\mid s, a)\n",
    "$, while at the same time encouraging $\n",
    "    Q_\\theta(dv \\mid s, a)\n",
    "$ to exhibit high entropy. The actions $a' \\sim \\pi_\\theta(a\\mid s')$\n",
    "are sampled greedily as $\n",
    "    \\arg \\max_a \\int Q_\\theta(dv \\mid s', a) v\n",
    "$ (assuming finite action space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4eeba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdqn(fragment, module, *, gamma, C=1e-3):\n",
    "    trajectory, hx = fragment.state, fragment.hx\n",
    "    obs, act, rew, fin = trajectory.obs, trajectory.act, trajectory.rew, trajectory.fin\n",
    "\n",
    "    # get $Q(z_t, h_t, \\cdot; \\theta)$ for all t=0..T\n",
    "    act, _, info_module = module(\n",
    "        obs, act, rew, fin, hx=hx, stepno=trajectory.stepno)\n",
    "\n",
    "    # get the next state `state[t+1]` $z_{t+1}$ to access $a_t$\n",
    "    state_next, act = suply(xgetitem, (trajectory, act.unsqueeze(-1)), index=slice(1, None))\n",
    "\n",
    "    # $\\hat{A}_t$, the module's response to current and next state,\n",
    "    #  contains the q-values. `curr` is $q(z_t, h_{t+1}, \\cdot; \\theta)$\n",
    "    #  and `next` is $q(z_{t+1}, h_{t+1}, \\cdot; \\theta)$ is `next`.\n",
    "    info_module_curr, info_module_next = timeshift(info_module)\n",
    "\n",
    "    # get T Z_\\theta(\\cdot \\mid z_t, a_t)\n",
    "    #  \\overset{=}{D} N(\\cdot \\mid r_{t+1} + \\gamma 1_{d_{t+1}} \\hat{v}_{t+1}(z_{t+1}), ...)\n",
    "    # use action a' \\sim \\arg \\max_a z_\\theta(a \\mid z_{t+1}, a_{t+1})\n",
    "    factor = torch.where(state_next.fin, 1e-4, gamma)  # XXX some leakage from the next traj.\n",
    "    with torch.no_grad():\n",
    "        # get $r_{t+1} + \\gamma 1_{d_{t+1}} \\hat{v}_{t+1}(z_{t+1})$\n",
    "        zed = info_module_next['loc'].gather(-1, act).squeeze(-1)\n",
    "        loc = state_next.rew + factor * zed\n",
    "        # XXX should the termination does not affect the scale?\n",
    "        scl = factor * info_module_next['scl'].gather(-1, act).squeeze(-1)\n",
    "        q = Normal(loc, scl)\n",
    "\n",
    "    # get Z_\\theta(\\cdot \\mid s_t, a_t)\n",
    "    act = state_next.act.unsqueeze(-1)\n",
    "    p = Normal(\n",
    "        info_module_curr['loc'].gather(-1, act).squeeze(-1),\n",
    "        info_module_curr['scl'].gather(-1, act).squeeze(-1),\n",
    "    )\n",
    "    \n",
    "    # distance from T z_\\theta to z_\\theta on s-a-r-s' samples\n",
    "    dst = kl_divergence(q, p).mean()  # q is fixed target.\n",
    "    # XXX fwd kl means covering, not mode-seeking.\n",
    "\n",
    "    # use kl from N(0, 1) prior, but can we also use entropy?\n",
    "    # pi = Normal(*torch.tensor([0., 1.])).expand(loc.shape)\n",
    "    # kl_reg = kl_divergence(p, pi).mean()\n",
    "    reg = -p.entropy().mean()\n",
    "\n",
    "    return dst + C * reg, {\n",
    "        'dst': float(dst),\n",
    "        'reg': float(reg),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fbf7c6",
   "metadata": {},
   "source": [
    "Let's define an actor for the distributional Q-learning with the Gaussian approximation:\n",
    "\n",
    "$$\n",
    "Q_\\theta(\n",
    "    v \\mid z, a\n",
    "    ) = \\mathcal{N}\\bigl(\n",
    "        v \\,\\big\\vert \\,\n",
    "        \\mu(z, a),\n",
    "        \\sigma^2(z, a)\n",
    "    \\bigr)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c280c84",
   "metadata": {},
   "source": [
    "The greedy action sampling must take into account the distribution of $\n",
    "    j^* = \\arg\\max_i z_i\n",
    "$ for a collection of independent $\n",
    "    z_i \\sim \\mathcal{N}(Z_i \\mid \\mu_i, \\sigma^2_i)\n",
    "$, not just the means. Specifically, we must make sure that $j^*=j$ with probability\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(Z_j \\geq  \\max_{i\\neq j} Z_i)\n",
    "    = \\mathbb{E}\n",
    "        \\mathbb{P}(Z_j \\geq \\max_{i\\neq j} Z_i \\big \\vert Z_j)\n",
    "    = \\mathbb{E}_{\\xi \\sim \\mathcal{N}(0, 1)}\n",
    "        \\prod_{i\\neq j} \\Phi\\biggl(\n",
    "            \\frac{\\mu_j - \\mu_i}{\\sigma_i}\n",
    "            + \\frac{\\sigma_j}{\\sigma_i} \\xi\n",
    "        \\biggr)\n",
    "    \\,, $$\n",
    "    \n",
    "wherein we used independence and the fact that the max of independent Gaussian rvs satisfies\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\max_i Z_i \\leq c)\n",
    "    = \\prod_i \\mathbb{P}(Z_i \\leq c)\n",
    "    = \\prod_i \\Phi\\biggl(\n",
    "        \\frac{c - \\mu_i}{\\sigma_i}\n",
    "    \\biggr)\n",
    "    \\,, $$\n",
    "\n",
    "for $\\Phi$ -- the univariate standard Gaussian CDF.\n",
    "\n",
    "But this is hard to compute... So we use the largest mean heuristic.\n",
    "Maybe we can use extreme value asymptotics here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602be2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine import BaseActorModule\n",
    "from torch.distributions import Independent, Normal, kl_divergence\n",
    "\n",
    "\n",
    "class GaussTaxiActor(BaseActorModule):\n",
    "    def __init__(self, *, struct=False, epsilon=0.1):\n",
    "        super().__init__()\n",
    "        self.struct = struct\n",
    "\n",
    "        if not struct:\n",
    "            self.table = torch.nn.Embedding(500, 2 * 6)\n",
    "\n",
    "        else:\n",
    "            self.table = torch.nn.Sequential(\n",
    "                ModuleDict(dict(\n",
    "                    row=torch.nn.Embedding(5, 3),\n",
    "                    col=torch.nn.Embedding(5, 3),\n",
    "                    location=torch.nn.Embedding(5, 3),\n",
    "                    destination=torch.nn.Embedding(4, 3),\n",
    "                )),\n",
    "                torch.nn.Linear(12, 32),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(32, 2 * 6),\n",
    "            )\n",
    "\n",
    "        # for updating the exploration epsilon in the clones\n",
    "        self.register_buffer('epsilon', torch.tensor(epsilon))\n",
    "\n",
    "    def forward(self, obs, act, rew, fin, *, hx=None, stepno=None, virtual=False):\n",
    "        out, hx = self.table(obs), ()\n",
    "\n",
    "        loc, out = torch.chunk(out, 2, dim=-1)\n",
    "        scl = torch.clamp(F.softplus(out), min=1e-3)\n",
    "\n",
    "        # val, act = Normal(loc, scl).sample().max(dim=-1)\n",
    "        val, act = loc.max(dim=-1)  # unfounded heuristic :(\n",
    "\n",
    "        return act, hx, dict(loc=loc, scl=scl, q=loc, value=val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abfe639",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c84768c",
   "metadata": {},
   "source": [
    "### Run!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e336a5e",
   "metadata": {},
   "source": [
    "prepare the optimizer for the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad23b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "use_dist = True\n",
    "use_target = False\n",
    "use_double = False\n",
    "use_duelling = False\n",
    "\n",
    "# `target` does not work for some reason at all with taxi, maybe the freeze schedule is off?\n",
    "#  or contiguous fragments work to the detriment of learning\n",
    "\n",
    "# `duelling` also fails for both target and double, and sorta for ordinary q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842dc177",
   "metadata": {},
   "source": [
    "Initialize the learner and the environment factories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d06c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "structured = False\n",
    "\n",
    "factory_eval = partial(base_factory, struct=structured)\n",
    "factory = partial(base_factory, struct=structured)\n",
    "\n",
    "if use_dist:\n",
    "    learner = GaussTaxiActor(struct=structured)\n",
    "else:\n",
    "    learner = TaxiActor(duelling=use_duelling, epsilon=1., struct=structured)\n",
    "\n",
    "learner.train()\n",
    "device_ = torch.device('cpu')  # torch.device('cuda:0')\n",
    "learner.to(device=device_)\n",
    "\n",
    "optim = torch.optim.Adam(learner.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82730b1a",
   "metadata": {},
   "source": [
    "Initialize the sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991c289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "T, B = 25, 8"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f17902fe",
   "metadata": {},
   "source": [
    "from rlplay.engine.rollout import same\n",
    "\n",
    "batchit = same.rollout(\n",
    "    [factory() for _ in range(B)],\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    sticky=False,\n",
    "    device=device_,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c298875",
   "metadata": {},
   "source": [
    "from rlplay.engine.rollout import single\n",
    "\n",
    "batchit = single.rollout(\n",
    "    factory,\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    n_envs=B,\n",
    "    sticky=False,\n",
    "    clone=True,\n",
    "    close=False,\n",
    "    device=device_,\n",
    "    start_method='fork',  # fork in notebook for macos, spawn in linux\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60ab21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout import multi\n",
    "\n",
    "batchit = multi.rollout(\n",
    "    factory,\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    n_actors=8,\n",
    "    n_per_actor=B,\n",
    "    n_buffers=16,\n",
    "    n_per_batch=2,\n",
    "    sticky=False,\n",
    "    pinned=False,\n",
    "    clone=True,\n",
    "    close=False,\n",
    "    device=device_,\n",
    "    start_method='fork',  # fork in notebook for macos, spawn in linux\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9a0a4",
   "metadata": {},
   "source": [
    "A generator of evaluation rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f3229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout.evaluate import evaluate\n",
    "\n",
    "test_it = evaluate(factory_eval, learner, n_envs=4, n_steps=500,\n",
    "                   clone=False, device=device_, start_method='fork')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846d8c9",
   "metadata": {},
   "source": [
    "Implement your favourite training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ab50a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import copy\n",
    "from math import log, exp\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "# the training loop\n",
    "losses, rewards = [], []\n",
    "decay = -log(2) / 50  # exploration epsilon halflife\n",
    "for epoch in tqdm.tqdm(range(400)):\n",
    "    # freeze the target for q\n",
    "    target = copy.deepcopy(learner) if use_target else None\n",
    "\n",
    "    for j, batch in zip(range(100), batchit):\n",
    "        if use_dist:\n",
    "            loss, info = gdqn(batch, learner, gamma=gamma)\n",
    "\n",
    "        else:\n",
    "            loss, info = ddq_learn(batch, learner, target=target,\n",
    "                                   gamma=gamma, double=use_double)\n",
    "\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        grad = clip_grad_norm_(learner.parameters(), max_norm=1e3)\n",
    "        optim.step()\n",
    "\n",
    "        losses.append(dict(\n",
    "            loss=float(loss), grad=float(grad), **info\n",
    "        ))\n",
    "\n",
    "    learner.epsilon.mul_(exp(decay)).clip_(0.1, 1.0)\n",
    "\n",
    "    # fetch the evaluation results lagged by one inner loop!\n",
    "    rewards.append(next(test_it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0633c36c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# close the generators\n",
    "batchit.close()\n",
    "test_it.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48b2e835",
   "metadata": {},
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce115d82",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(records):\n",
    "    \"\"\"collate identically keyed dicts\"\"\"\n",
    "    out, n_records = {}, 0\n",
    "    for record in records:\n",
    "        for k, v in record.items():\n",
    "            out.setdefault(k, []).append(v)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "data = {k: numpy.array(v) for k, v in collate(losses).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(data['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01581b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(data['grad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22764b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'dst' in data:\n",
    "    plt.semilogy(data['dst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f9fe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'reg' in data:\n",
    "    plt.semilogy(data['reg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3128f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = numpy.stack(rewards, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ffd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd6ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = numpy.median(rewards, axis=-1), rewards.std(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5545163",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fi, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "\n",
    "ax.plot(numpy.mean(rewards, axis=-1))\n",
    "ax.plot(numpy.median(rewards, axis=-1))\n",
    "ax.plot(numpy.min(rewards, axis=-1))\n",
    "ax.plot(numpy.std(rewards, axis=-1))\n",
    "# ax.plot(m+s * 1.96)\n",
    "# ax.plot(m-s * 1.96)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9856d6a4",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace0ac8",
   "metadata": {},
   "source": [
    "The ultimate evaluation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d372a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from rlplay.engine import core\n",
    "\n",
    "with factory_eval() as env:\n",
    "    learner.eval()\n",
    "    eval_rewards, info = core.evaluate([\n",
    "        env\n",
    "    ], learner, render=True, n_steps=1e2, device=device_)\n",
    "\n",
    "print(sum(eval_rewards))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4716caad",
   "metadata": {},
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade2f90c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df22ff",
   "metadata": {},
   "source": [
    "Let's analyze the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91305f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "(info['q'] - numpy.expand_dims(info['value'], -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8257eb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((info['q'] - numpy.expand_dims(info['value'], -1))[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d924c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.algo.returns import npy_returns, npy_deltas\n",
    "\n",
    "td_target = eval_rewards + gamma * info['value'][1:]\n",
    "td_error = td_target - info['value'][:-1]\n",
    "# td_error = npy_deltas(\n",
    "#     eval_rewards, numpy.zeros_like(eval_rewards, dtype=bool), info['value'][:-1],\n",
    "#     gamma=gamma, bootstrap=info['value'][-1])\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "ax.semilogy(abs(td_error) / abs(td_target))\n",
    "ax.set_title('relative td(1)-error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef93ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.algo.returns import npy_returns, npy_deltas\n",
    "\n",
    "# plt.plot(\n",
    "#     npy_returns(eval_rewards, numpy.zeros_like(eval_rewards, dtype=bool),\n",
    "#                 gamma=gamma, bootstrap=info['value'][-1]))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "ax.plot(info['value']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580341aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    obs = torch.arange(500).unsqueeze(1)\n",
    "    if structured:\n",
    "        obs = StructuredTaxiEnv.observation(obs)\n",
    "\n",
    "    _, _, info = learner(obs, act=None, rew=None, fin=None)    \n",
    "    if 'loc' in info:\n",
    "        # shuffle dims to `a d p r c`\n",
    "        loc = info['loc'].reshape(5, 5, 5, 4, -1).permute(4, 3, 2, 0, 1)\n",
    "        scl = info['scl'].reshape(5, 5, 5, 4, -1).permute(4, 3, 2, 0, 1)\n",
    "        p = Normal(loc, scl)\n",
    "        ent = p.entropy()\n",
    "\n",
    "    else:\n",
    "        loc = info['q'].reshape(5, 5, 5, 4, -1).permute(4, 3, 2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399b83b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 'R'\n",
    "dst = {'R': 0, 'G': 1, 'Y': 2, 'B': 3}\n",
    "\n",
    "# +---------+\n",
    "# |R: | : :G|\n",
    "# | : | : : |\n",
    "# | : : : : |\n",
    "# | | : | : |\n",
    "# |Y| : |B: |\n",
    "# +---------+\n",
    "\n",
    "x = loc[:, dst[d]].permute(1, 2, 0, 3)  # .reshape(5 * 5, 6 * 5)\n",
    "plt.imshow(torch.cat([\n",
    "    x, x[:, :, :4].max(2, keepdim=True).values,\n",
    "], dim=2).reshape(5 * 5, 7 * 5))\n",
    "\n",
    "# cols: action snewpd, rows: location RGYBT\n",
    "plt.xlabel('action + pool')\n",
    "plt.xticks(list(range(2, 35, 5)), [*'snewpd', 'max'])\n",
    "plt.ylabel('location')\n",
    "plt.yticks(list(range(2, 25, 5)), list('RGYBT'))\n",
    "plt.title(f'destination: {d}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4cdeca",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
